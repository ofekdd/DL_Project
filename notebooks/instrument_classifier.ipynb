{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Instrument Classifier\n",
    "\n",
    "This notebook demonstrates a PyTorch Lightning project for multi-label musical instrument recognition from audio clips. It allows you to run the entire pipeline in Google Colab without any special modifications to the codebase.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's clone the repository and install the required dependencies.\n"
   ],
   "id": "49ef049691f0bc06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/your-username/instrument-classifier.git\n",
    "    %cd instrument-classifier\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -r requirements.txt\n"
   ],
   "id": "ef869f4e144bab3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import Libraries\n",
    "\n",
    "Let's import the necessary libraries for our project.\n"
   ],
   "id": "5e4282746ce6a56e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import pathlib\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import hashlib\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "pl.seed_everything(42)\n"
   ],
   "id": "8ff1bd280c609666"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "Let's define the configuration for our project.\n"
   ],
   "id": "fcfef9d9e27ab3eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create configs directory if it doesn't exist\n",
    "os.makedirs('configs', exist_ok=True)\n",
    "\n",
    "# Default configuration\n",
    "default_config = \"\"\"\n",
    "# Common hyper‑parameters\n",
    "sample_rate: 22050\n",
    "n_mels: 64\n",
    "hop_length: 512\n",
    "batch_size: 32\n",
    "num_epochs: 50\n",
    "learning_rate: 3e-4\n",
    "num_workers: 4\n",
    "\"\"\"\n",
    "\n",
    "# ResNet configuration\n",
    "resnet_config = \"\"\"\n",
    "# ResNet‑34 override\n",
    "model_name: resnet34\n",
    "learning_rate: 1e-4\n",
    "batch_size: 16\n",
    "\"\"\"\n",
    "\n",
    "# Write configurations to files\n",
    "with open('configs/default.yaml', 'w') as f:\n",
    "    f.write(default_config)\n",
    "    \n",
    "with open('configs/model_resnet.yaml', 'w') as f:\n",
    "    f.write(resnet_config)\n",
    "\n",
    "# Load configuration\n",
    "cfg = yaml.safe_load(default_config)\n",
    "resnet_cfg = {**cfg, **yaml.safe_load(resnet_config)}\n",
    "\n",
    "# Display configuration\n",
    "print(\"Default configuration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(\"\\nResNet configuration:\")\n",
    "for key, value in resnet_cfg.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ],
   "id": "bffff1e291852374"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download and Extract Data\n",
    "\n",
    "Let's download and extract the IRMAS dataset.\n"
   ],
   "id": "b293092929a565a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "IRMAS_URL = \"https://zenodo.org/record/1290750/files/IRMAS-TrainingData.zip?download=1\"\n",
    "MD5 = \"4fd9f5ed5a18d8e2687e6360b5f60afe\"  # expected archive checksum\n",
    "\n",
    "def md5(fname, chunk=2**20):\n",
    "    m = hashlib.md5()\n",
    "    with open(fname, 'rb') as fh:\n",
    "        while True:\n",
    "            data = fh.read(chunk)\n",
    "            if not data: break\n",
    "            m.update(data)\n",
    "    return m.hexdigest()\n",
    "\n",
    "def download_irmas(out_dir):\n",
    "    out_dir = pathlib.Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    archive_path = out_dir / \"IRMAS.zip\"\n",
    "    \n",
    "    if not archive_path.exists():\n",
    "        print(\"Downloading IRMAS ...\")\n",
    "        urllib.request.urlretrieve(IRMAS_URL, archive_path)\n",
    "    else:\n",
    "        print(\"Archive already exists, skipping download\")\n",
    "    \n",
    "    print(\"Verifying checksum ...\")\n",
    "    if md5(archive_path) != MD5:\n",
    "        print(\"Checksum mismatch!\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(\"Extracting ...\")\n",
    "    with zipfile.ZipFile(archive_path) as zf:\n",
    "        zf.extractall(out_dir)\n",
    "    print(\"Done. Data at\", out_dir)\n",
    "\n",
    "# Create data directories\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Download and extract IRMAS dataset\n",
    "download_irmas('data/raw')\n"
   ],
   "id": "20660c7a328a0958"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Let's preprocess the data by converting WAV files to log-mel spectrograms.\n"
   ],
   "id": "7f595db8ac67dd73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_file(wav_path, cfg):\n",
    "    y, sr = librosa.load(wav_path, sr=cfg['sample_rate'], mono=True)\n",
    "    mels = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_mels=cfg['n_mels'], hop_length=cfg['hop_length'], fmin=30\n",
    "    )\n",
    "    logmel = librosa.power_to_db(mels, ref=np.max).astype(np.float32)\n",
    "    return logmel\n",
    "\n",
    "def preprocess_data(in_dir, out_dir, cfg):\n",
    "    in_dir, out_dir = pathlib.Path(in_dir), pathlib.Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create train and validation directories\n",
    "    train_dir = out_dir / 'train'\n",
    "    val_dir = out_dir / 'val'\n",
    "    train_dir.mkdir(exist_ok=True)\n",
    "    val_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all WAV files\n",
    "    wav_files = list(in_dir.rglob(\"*.wav\"))\n",
    "    print(f\"Found {len(wav_files)} WAV files\")\n",
    "    \n",
    "    # Split into train and validation sets (90/10 split)\n",
    "    np.random.shuffle(wav_files)\n",
    "    split_idx = int(len(wav_files) * 0.9)\n",
    "    train_files = wav_files[:split_idx]\n",
    "    val_files = wav_files[split_idx:]\n",
    "    \n",
    "    # Process training files\n",
    "    print(\"Processing training files...\")\n",
    "    for wav in tqdm(train_files):\n",
    "        spec = process_file(wav, cfg)\n",
    "        rel = wav.relative_to(in_dir).with_suffix(\".npy\")\n",
    "        out_path = train_dir / rel\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(out_path, spec)\n",
    "    \n",
    "    # Process validation files\n",
    "    print(\"Processing validation files...\")\n",
    "    for wav in tqdm(val_files):\n",
    "        spec = process_file(wav, cfg)\n",
    "        rel = wav.relative_to(in_dir).with_suffix(\".npy\")\n",
    "        out_path = val_dir / rel\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(out_path, spec)\n",
    "    \n",
    "    print(f\"Processed {len(train_files)} training files and {len(val_files)} validation files\")\n",
    "\n",
    "# Preprocess data\n",
    "preprocess_data('data/raw/IRMAS', 'data/processed', cfg)\n"
   ],
   "id": "bf28a5e434ce5c10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Models\n",
    "\n",
    "Let's define the models for our project.\n"
   ],
   "id": "3164633cb076a802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "class CNNBaseline(nn.Module):\n",
    "    \"\"\"Simple CNN baseline for audio classification.\"\"\"\n",
    "    def __init__(self, n_classes=11):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # [B,16,32,32]\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # [B,32,16,16]\n",
    "        x = self.pool(self.relu(self.conv3(x)))  # [B,64,8,8]\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class ResNetSpec(nn.Module):\n",
    "    \"\"\"ResNet‑34 backbone adapted for single‑channel spectrogram input.\"\"\"\n",
    "    def __init__(self, n_classes=11):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet34(weights=None)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Replace FC\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(self.backbone.fc.in_features, n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Display model architectures\n",
    "print(\"CNN Baseline Architecture:\")\n",
    "print(CNNBaseline())\n",
    "print(\"\\nResNet Architecture:\")\n",
    "print(ResNetSpec())\n"
   ],
   "id": "5f526b3f279af863"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Dataset and DataLoader\n",
    "\n",
    "Let's define the dataset and dataloader for our project.\n"
   ],
   "id": "bfc36b7aebd23d37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LABELS = [\"cello\", \"clarinet\", \"flute\", \"acoustic_guitar\", \"organ\", \"piano\", \"saxophone\", \"trumpet\", \"violin\", \"voice\", \"other\"]\n",
    "\n",
    "class NpyDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.files = list(pathlib.Path(root).rglob(\"*.npy\"))\n",
    "        self.label_map = {label: i for i, label in enumerate(LABELS)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        spec = np.load(self.files[idx])\n",
    "        x = torch.tensor(spec).unsqueeze(0)  # [1,H,W]\n",
    "        \n",
    "        # Parse label from folder name\n",
    "        label_str = self.files[idx].parent.name.split(\"_\")[0]\n",
    "        y = torch.zeros(len(LABELS))\n",
    "        \n",
    "        # Map label string to index\n",
    "        if label_str in self.label_map:\n",
    "            y[self.label_map[label_str]] = 1.0\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_ds = NpyDataset(\"data/processed/train\")\n",
    "val_ds = NpyDataset(\"data/processed/val\")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_ds)}\")\n",
    "print(f\"Validation dataset size: {len(val_ds)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=resnet_cfg['batch_size'], shuffle=True, num_workers=resnet_cfg['num_workers'])\n",
    "val_loader = DataLoader(val_ds, batch_size=resnet_cfg['batch_size'], shuffle=False, num_workers=resnet_cfg['num_workers'])\n"
   ],
   "id": "2c010d737cdf946b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Lightning Module\n",
    "\n",
    "Let's define the PyTorch Lightning module for our project.\n"
   ],
   "id": "b864f29411e9a0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "\n",
    "class MetricCollection:\n",
    "    def __init__(self, n_classes):\n",
    "        self.accuracy = Accuracy(task=\"multilabel\", num_labels=n_classes)\n",
    "        self.precision = Precision(task=\"multilabel\", num_labels=n_classes)\n",
    "        self.recall = Recall(task=\"multilabel\", num_labels=n_classes)\n",
    "        self.f1 = F1Score(task=\"multilabel\", num_labels=n_classes)\n",
    "    \n",
    "    def __call__(self, preds, targets):\n",
    "        return {\n",
    "            \"accuracy\": self.accuracy(preds, targets),\n",
    "            \"precision\": self.precision(preds, targets),\n",
    "            \"recall\": self.recall(preds, targets),\n",
    "            \"f1\": self.f1(preds, targets)\n",
    "        }\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        n_classes = len(LABELS)\n",
    "        if cfg.get(\"model_name\", \"cnn\") == \"resnet34\":\n",
    "            self.model = ResNetSpec(n_classes)\n",
    "        else:\n",
    "            self.model = CNNBaseline(n_classes)\n",
    "        self.metrics = MetricCollection(n_classes)\n",
    "        self.lr = cfg[\"learning_rate\"]\n",
    "        self.save_hyperparameters(cfg)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def common_step(self, batch, stage):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = torch.nn.functional.binary_cross_entropy(preds, y)\n",
    "        metrics = self.metrics(preds, y)\n",
    "        self.log_dict({f\"{stage}/loss\": loss, **{f\"{stage}/{k}\": v for k, v in metrics.items()}},\n",
    "                      prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, _):\n",
    "        return self.common_step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, _):\n",
    "        return self.common_step(batch, \"val\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "# Initialize model\n",
    "model = LitModel(resnet_cfg)\n",
    "print(\"Model initialized with ResNet-34 backbone\")\n"
   ],
   "id": "4906192d967327a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train Model\n",
    "\n",
    "Let's train the model using PyTorch Lightning.\n"
   ],
   "id": "4c8b9cc9983f5c4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def default_callbacks():\n",
    "    return [\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val/f1\",\n",
    "            mode=\"max\",\n",
    "            save_top_k=1,\n",
    "            filename=\"best-{epoch:02d}-{val/f1:.2f}\",\n",
    "            verbose=True\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val/f1\",\n",
    "            mode=\"max\",\n",
    "            patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=resnet_cfg['num_epochs'],\n",
    "    callbacks=default_callbacks(),\n",
    "    accelerator=\"auto\"\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ],
   "id": "4e3db1b1a866d8ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "Let's perform inference on a sample audio file.\n"
   ],
   "id": "8c5357858a954b81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_features(path, cfg):\n",
    "    y, sr = librosa.load(path, sr=cfg['sample_rate'], mono=True)\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=cfg['n_mels'], hop_length=cfg['hop_length'])\n",
    "    logmel = librosa.power_to_db(mels, ref=np.max).astype(np.float32)\n",
    "    return torch.tensor(logmel).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def predict(model, wav_path, cfg):\n",
    "    model.eval()\n",
    "    x = extract_features(wav_path, cfg)\n",
    "    with torch.no_grad():\n",
    "        preds = model(x).squeeze().numpy()\n",
    "    return {label: float(preds[i]) for i, label in enumerate(LABELS)}\n",
    "\n",
    "# Get a sample WAV file for inference\n",
    "sample_wav = list(pathlib.Path('data/raw/IRMAS').rglob(\"*.wav\"))[0]\n",
    "print(f\"Sample WAV file: {sample_wav}\")\n",
    "\n",
    "# Perform inference\n",
    "results = predict(model, sample_wav, resnet_cfg)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPrediction results:\")\n",
    "for instrument, confidence in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{instrument}: {confidence:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results.keys(), results.values())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Instrument Detection Confidence')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "945e018895ca083d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualize Audio and Spectrogram\n",
    "\n",
    "Let's visualize the audio waveform and spectrogram of the sample file.\n"
   ],
   "id": "fe0e094f940775e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_audio(wav_path, cfg):\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(wav_path, sr=cfg['sample_rate'], mono=True)\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=cfg['n_mels'], hop_length=cfg['hop_length'])\n",
    "    logmel = librosa.power_to_db(mels, ref=np.max)\n",
    "    \n",
    "    # Plot waveform and spectrogram\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title('Waveform')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    librosa.display.specshow(logmel, sr=sr, x_axis='time', y_axis='mel', hop_length=cfg['hop_length'])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel Spectrogram')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample audio\n",
    "visualize_audio(sample_wav, resnet_cfg)\n"
   ],
   "id": "981d5529d1d82c7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the complete pipeline for multi-label musical instrument recognition:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Downloading and preprocessing the IRMAS dataset\n",
    "3. Defining and training a ResNet-34 model\n",
    "4. Performing inference on audio files\n",
    "5. Visualizing the results\n",
    "\n",
    "This notebook can be run in Google Colab without any special modifications to the codebase."
   ],
   "id": "7f5aafac61f9db62"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
