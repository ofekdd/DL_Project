{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Instrument Classifier\n",
    "\n",
    "This notebook demonstrates a PyTorch project for multi-label musical instrument recognition from audio clips. It allows you to run the entire pipeline in Google Colab without any special modifications to the codebase.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's clone the repository and install the required dependencies.\n"
   ],
   "id": "72f151f9e587ca61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:53:12.301574Z",
     "start_time": "2025-05-31T08:53:12.293326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/your-username/instrument-classifier.git\n",
    "    %cd instrument-classifier\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install -r requirements.txt\n"
   ],
   "id": "e41cf2ee959bd67c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import Libraries\n",
    "\n",
    "Let's import the necessary libraries for our project.\n"
   ],
   "id": "26c0112ad361303c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:53:15.258667Z",
     "start_time": "2025-05-31T08:53:12.343636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import pathlib\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import hashlib\n",
    "import sys\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n"
   ],
   "id": "347e90cad1a1ade4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "Let's define the configuration for our project.\n"
   ],
   "id": "47340b13e48e8ee6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:53:15.413846Z",
     "start_time": "2025-05-31T08:53:15.282212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create configs directory if it doesn't exist\n",
    "os.makedirs('configs', exist_ok=True)\n",
    "\n",
    "# Default configuration\n",
    "default_config = \"\"\"\n",
    "# Common hyper‑parameters\n",
    "sample_rate: 22050\n",
    "n_mels: 64\n",
    "hop_length: 512\n",
    "batch_size: 32\n",
    "num_epochs: 50\n",
    "learning_rate: 3e-4\n",
    "num_workers: 4\n",
    "\"\"\"\n",
    "\n",
    "# ResNet configuration\n",
    "resnet_config = \"\"\"\n",
    "# ResNet‑34 override\n",
    "model_name: resnet34\n",
    "learning_rate: 1e-4\n",
    "batch_size: 16\n",
    "\"\"\"\n",
    "\n",
    "# Write configurations to files\n",
    "with open('configs/default.yaml', 'w') as f:\n",
    "    f.write(default_config)\n",
    "\n",
    "with open('configs/model_resnet.yaml', 'w') as f:\n",
    "    f.write(resnet_config)\n",
    "\n",
    "# Load configuration\n",
    "cfg = yaml.safe_load(default_config)\n",
    "resnet_cfg = {**cfg, **yaml.safe_load(resnet_config)}\n",
    "\n",
    "# Display configuration\n",
    "print(\"Default configuration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nResNet configuration:\")\n",
    "for key, value in resnet_cfg.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ],
   "id": "18931ecd85bfbbf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default configuration:\n",
      "  sample_rate: 22050\n",
      "  n_mels: 64\n",
      "  hop_length: 512\n",
      "  batch_size: 32\n",
      "  num_epochs: 50\n",
      "  learning_rate: 3e-4\n",
      "  num_workers: 4\n",
      "\n",
      "ResNet configuration:\n",
      "  sample_rate: 22050\n",
      "  n_mels: 64\n",
      "  hop_length: 512\n",
      "  batch_size: 16\n",
      "  num_epochs: 50\n",
      "  learning_rate: 1e-4\n",
      "  num_workers: 4\n",
      "  model_name: resnet34\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download and Extract Data\n",
    "\n",
    "Let's download and extract the IRMAS dataset.\n"
   ],
   "id": "537b6591baeeddc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:55:16.294769Z",
     "start_time": "2025-05-31T08:53:15.461851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Download the IRMAS dataset (≈2 GB) and extract it.\n",
    "\n",
    "Example:\n",
    "    python data/download_irmas.py --out_dir data/raw\n",
    "\"\"\"\n",
    "import argparse, hashlib, os, sys, pathlib\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "IRMAS_URL = \"https://zenodo.org/record/1290750/files/IRMAS-TrainingData.zip?download=1\"\n",
    "MD5       = \"4fd9f5ed5a18d8e2687e6360b5f60afe\"  # expected archive checksum\n",
    "\n",
    "def md5(fname, chunk=2**20):\n",
    "    m = hashlib.md5()\n",
    "    with open(fname, 'rb') as fh:\n",
    "        while True:\n",
    "            data = fh.read(chunk)\n",
    "            if not data: break\n",
    "            m.update(data)\n",
    "    return m.hexdigest()\n",
    "\n",
    "def main(out_dir: str):\n",
    "    out_dir = pathlib.Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    archive_path = out_dir / \"IRMAS.zip\"\n",
    "\n",
    "    if not archive_path.exists():\n",
    "        print(\"Downloading IRMAS ...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(IRMAS_URL, archive_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed: {e}\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"Archive already exists, skipping download\")\n",
    "\n",
    "    print(\"Verifying checksum ...\")\n",
    "    if md5(archive_path) != MD5:\n",
    "        print(\"Checksum mismatch! The downloaded file may be corrupted.\", file=sys.stderr)\n",
    "        print(\"Try deleting the file and running the script again.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Extracting ...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(archive_path) as zf:\n",
    "            zf.extractall(out_dir)\n",
    "        print(\"Done. Data at\", out_dir)\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Extraction failed: The file is not a valid zip archive.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction failed: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--out_dir\", default=\"data/raw\", help=\"Destination directory\")\n",
    "\n",
    "    # Handle Jupyter notebook execution by ignoring unknown arguments\n",
    "    import sys\n",
    "    if 'ipykernel_launcher.py' in sys.argv[0]:\n",
    "        args, unknown = p.parse_known_args()\n",
    "    else:\n",
    "        args = p.parse_args()\n",
    "\n",
    "    main(args.out_dir)\n",
    "\n",
    "# When running in notebook, explicitly call main() with the output directory\n",
    "main(\"data/raw\")"
   ],
   "id": "d9910cb2ccc61bfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive already exists, skipping download\n",
      "Verifying checksum ...\n",
      "Extracting ...\n",
      "Done. Data at data/raw\n",
      "Archive already exists, skipping download\n",
      "Verifying checksum ...\n",
      "Extracting ...\n",
      "Done. Data at data/raw\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Let's preprocess the data by converting WAV files to log-mel spectrograms.\n"
   ],
   "id": "9e2eb58b1fe3ab60"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-31T08:55:16.379226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_multi_stft(\n",
    "    y: np.ndarray,\n",
    "    sr: int,\n",
    "    n_ffts=(256, 512, 1024),\n",
    "    band_ranges=((0, 1000), (1000, 4000), (4000, 11025))\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates 9 spectrograms: 3 window sizes × 3 frequency bands.\n",
    "\n",
    "    Parameters:\n",
    "        y (np.ndarray): Audio waveform\n",
    "        sr (int): Sampling rate\n",
    "        n_ffts (tuple): FFT window sizes\n",
    "        band_ranges (tuple): Frequency band ranges (Hz)\n",
    "\n",
    "    Returns:\n",
    "        dict: { (band_label, n_fft): spectrogram }\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for n_fft in n_ffts:\n",
    "        hop_length = n_fft // 4\n",
    "        stft = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "        mag = np.abs(stft)\n",
    "\n",
    "        freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "\n",
    "        for (f_low, f_high) in band_ranges:\n",
    "            band_label = f\"{f_low}-{f_high}Hz\"\n",
    "            # Get frequency indices within this band\n",
    "            band_mask = (freqs >= f_low) & (freqs < f_high)\n",
    "            band_spec = mag[band_mask, :]\n",
    "            # Convert to log scale\n",
    "            log_spec = librosa.power_to_db(band_spec, ref=np.max).astype(np.float32)\n",
    "            result[(band_label, n_fft)] = log_spec\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_file(wav_path, cfg):\n",
    "    y, sr = librosa.load(wav_path, sr=cfg['sample_rate'], mono=True)\n",
    "    return generate_multi_stft(y, sr)\n",
    "\n",
    "def preprocess_data(in_dir, out_dir, cfg):\n",
    "    in_dir, out_dir = pathlib.Path(in_dir), pathlib.Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create train and validation directories\n",
    "    train_dir = out_dir / 'train'\n",
    "    val_dir = out_dir / 'val'\n",
    "    train_dir.mkdir(exist_ok=True)\n",
    "    val_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Get all WAV files\n",
    "    wav_files = list(in_dir.rglob(\"*.wav\"))\n",
    "    print(f\"Found {len(wav_files)} WAV files\")\n",
    "\n",
    "    # Split into train and validation sets (90/10 split)\n",
    "    np.random.shuffle(wav_files)\n",
    "    split_idx = int(len(wav_files) * 0.9)\n",
    "    train_files = wav_files[:split_idx]\n",
    "    val_files = wav_files[split_idx:]\n",
    "\n",
    "    # Process training files\n",
    "    print(\"Processing training files...\")\n",
    "    for wav in tqdm(train_files):\n",
    "        specs_dict = process_file(wav, cfg)\n",
    "\n",
    "        # Create a directory for this audio file\n",
    "        rel_dir = wav.relative_to(in_dir).with_suffix(\"\")\n",
    "        file_out_dir = train_dir / rel_dir\n",
    "        file_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save each spectrogram with band and FFT size information in the filename\n",
    "        for (band_label, n_fft), spec in specs_dict.items():\n",
    "            spec_filename = f\"{band_label}_fft{n_fft}.npy\"\n",
    "            np.save(file_out_dir / spec_filename, spec)\n",
    "\n",
    "    # Process validation files\n",
    "    print(\"Processing validation files...\")\n",
    "    for wav in tqdm(val_files):\n",
    "        specs_dict = process_file(wav, cfg)\n",
    "\n",
    "        # Create a directory for this audio file\n",
    "        rel_dir = wav.relative_to(in_dir).with_suffix(\"\")\n",
    "        file_out_dir = val_dir / rel_dir\n",
    "        file_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save each spectrogram with band and FFT size information in the filename\n",
    "        for (band_label, n_fft), spec in specs_dict.items():\n",
    "            spec_filename = f\"{band_label}_fft{n_fft}.npy\"\n",
    "            np.save(file_out_dir / spec_filename, spec)\n",
    "\n",
    "    print(f\"Processed {len(train_files)} training files and {len(val_files)} validation files\")\n",
    "\n",
    "# Check if WAV files exist in data/raw/IRMAS\n",
    "irmas_path = pathlib.Path('data/raw/IRMAS')\n",
    "if irmas_path.exists() and any(irmas_path.rglob(\"*.wav\")):\n",
    "    print(\"Found WAV files in data/raw/IRMAS\")\n",
    "    preprocess_data('data/raw/IRMAS', 'data/processed', cfg)\n",
    "else:\n",
    "    # Check if WAV files exist in data/raw/IRMAS-TrainingData\n",
    "    training_data_path = pathlib.Path('data/raw/IRMAS-TrainingData')\n",
    "    if training_data_path.exists() and any(training_data_path.rglob(\"*.wav\")):\n",
    "        print(\"Found WAV files in data/raw/IRMAS-TrainingData\")\n",
    "        preprocess_data('data/raw/IRMAS-TrainingData', 'data/processed', cfg)\n",
    "    else:\n",
    "        # Check if WAV files exist directly in data/raw\n",
    "        raw_path = pathlib.Path('data/raw')\n",
    "        if raw_path.exists() and any(raw_path.rglob(\"*.wav\")):\n",
    "            print(\"Found WAV files in data/raw\")\n",
    "            preprocess_data('data/raw', 'data/processed', cfg)\n",
    "        else:\n",
    "            print(\"No WAV files found in data/raw or its subdirectories. Please check the extraction path.\")\n"
   ],
   "id": "5cf7e3889bce09b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found WAV files in data/raw/IRMAS-TrainingData\n",
      "Found 6705 WAV files\n",
      "Processing training files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1534/6034 [00:49<02:27, 30.42it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Models\n",
    "\n",
    "Let's define the models for our project.\n"
   ],
   "id": "9ee7c34e386b14cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "class CNNBaseline(nn.Module):\n",
    "    \"\"\"Simple CNN baseline for audio classification.\"\"\"\n",
    "    def __init__(self, n_classes=11):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # [B,16,32,32]\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # [B,32,16,16]\n",
    "        x = self.pool(self.relu(self.conv3(x)))  # [B,64,8,8]\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class ResNetSpec(nn.Module):\n",
    "    \"\"\"ResNet‑34 backbone adapted for single‑channel spectrogram input.\"\"\"\n",
    "    def __init__(self, n_classes=11):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet34(weights=None)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Replace FC\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(self.backbone.fc.in_features, n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Display model architectures\n",
    "print(\"CNN Baseline Architecture:\")\n",
    "print(CNNBaseline())\n",
    "print(\"\\nResNet Architecture:\")\n",
    "print(ResNetSpec())\n"
   ],
   "id": "50f99bc06b222371"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Dataset and DataLoader\n",
    "\n",
    "Let's define the dataset and dataloader for our project.\n"
   ],
   "id": "efe9d9d380194a32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LABELS = [\"cello\", \"clarinet\", \"flute\", \"acoustic_guitar\", \"organ\", \"piano\", \"saxophone\", \"trumpet\", \"violin\", \"voice\", \"other\"]\n",
    "\n",
    "class NpyDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.files = list(pathlib.Path(root).rglob(\"*.npy\"))\n",
    "        self.label_map = {label: i for i, label in enumerate(LABELS)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spec = np.load(self.files[idx])\n",
    "        x = torch.tensor(spec).unsqueeze(0)  # [1,H,W]\n",
    "\n",
    "        # Parse label from folder name\n",
    "        label_str = self.files[idx].parent.name.split(\"_\")[0]\n",
    "        y = torch.zeros(len(LABELS))\n",
    "\n",
    "        # Map label string to index\n",
    "        if label_str in self.label_map:\n",
    "            y[self.label_map[label_str]] = 1.0\n",
    "\n",
    "        return x, y\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_ds = NpyDataset(\"data/processed/train\")\n",
    "val_ds = NpyDataset(\"data/processed/val\")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_ds)}\")\n",
    "print(f\"Validation dataset size: {len(val_ds)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=resnet_cfg['batch_size'], shuffle=True, num_workers=resnet_cfg['num_workers'])\n",
    "val_loader = DataLoader(val_ds, batch_size=resnet_cfg['batch_size'], shuffle=False, num_workers=resnet_cfg['num_workers'])\n"
   ],
   "id": "c013489269d1c880"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Model\n",
    "\n",
    "Let's define the model for our project.\n"
   ],
   "id": "204355d3c004116b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "\n",
    "class MetricCollection:\n",
    "    def __init__(self, n_classes):\n",
    "        self.accuracy = Accuracy(task=\"multilabel\", num_labels=n_classes)\n",
    "        self.precision = Precision(task=\"multilabel\", num_labels=n_classes)\n",
    "        self.recall = Recall(task=\"multilabel\", num_labels=n_classes)\n",
    "        self.f1 = F1Score(task=\"multilabel\", num_labels=n_classes)\n",
    "\n",
    "    def __call__(self, preds, targets):\n",
    "        return {\n",
    "            \"accuracy\": self.accuracy(preds, targets),\n",
    "            \"precision\": self.precision(preds, targets),\n",
    "            \"recall\": self.recall(preds, targets),\n",
    "            \"f1\": self.f1(preds, targets)\n",
    "        }\n",
    "\n",
    "class InstrumentModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        n_classes = len(LABELS)\n",
    "        if cfg.get(\"model_name\", \"cnn\") == \"resnet34\":\n",
    "            self.model = ResNetSpec(n_classes)\n",
    "        else:\n",
    "            self.model = CNNBaseline(n_classes)\n",
    "        self.metrics = MetricCollection(n_classes)\n",
    "        self.lr = cfg[\"learning_rate\"]\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss_and_metrics(self, batch, stage=\"train\"):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = torch.nn.functional.binary_cross_entropy(preds, y)\n",
    "        metrics = self.metrics(preds, y)\n",
    "        return loss, metrics, preds\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "# Initialize model\n",
    "model = InstrumentModel(resnet_cfg)\n",
    "print(\"Model initialized with ResNet-34 backbone\")\n"
   ],
   "id": "a451c98dd4ecca90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train Model\n",
    "\n",
    "Let's train the model using PyTorch.\n"
   ],
   "id": "ccbb4da8c8dd0561"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Create directories for saving models\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Custom implementation of early stopping\"\"\"\n",
    "    def __init__(self, patience=5, mode='max', min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "\n",
    "        if self.mode == 'max':\n",
    "            if score > self.best_score + self.min_delta:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        else:  # mode == 'min'\n",
    "            if score < self.best_score - self.min_delta:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Train the model using standard PyTorch training loop\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = model.get_optimizer()\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, mode='max')\n",
    "\n",
    "    # Initialize best model tracking\n",
    "    best_f1 = 0.0\n",
    "    best_model_path = None\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_metrics = {}\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = [x.to(device) for x in batch]\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass, compute loss and metrics\n",
    "            loss, metrics, _ = model.compute_loss_and_metrics(batch, stage='train')\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            for k, v in metrics.items():\n",
    "                if k not in train_metrics:\n",
    "                    train_metrics[k] = 0.0\n",
    "                train_metrics[k] += v.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'f1': metrics['f1'].item()\n",
    "            })\n",
    "\n",
    "        # Compute average metrics for the epoch\n",
    "        train_loss /= len(train_loader)\n",
    "        for k in train_metrics:\n",
    "            train_metrics[k] /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_metrics = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                batch = [x.to(device) for x in batch]\n",
    "\n",
    "                # Forward pass, compute loss and metrics\n",
    "                loss, metrics, _ = model.compute_loss_and_metrics(batch, stage='val')\n",
    "\n",
    "                # Update metrics\n",
    "                val_loss += loss.item()\n",
    "                for k, v in metrics.items():\n",
    "                    if k not in val_metrics:\n",
    "                        val_metrics[k] = 0.0\n",
    "                    val_metrics[k] += v.item()\n",
    "\n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': loss.item(),\n",
    "                    'f1': metrics['f1'].item()\n",
    "                })\n",
    "\n",
    "        # Compute average metrics for the epoch\n",
    "        val_loss /= len(val_loader)\n",
    "        for k in val_metrics:\n",
    "            val_metrics[k] /= len(val_loader)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, F1: {train_metrics[\"f1\"]:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, F1: {val_metrics[\"f1\"]:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_f1:\n",
    "            best_f1 = val_metrics['f1']\n",
    "            best_model_path = f'checkpoints/best-{epoch+1:02d}-{val_metrics[\"f1\"]:.2f}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': val_metrics['f1'],\n",
    "                'val_loss': val_loss\n",
    "            }, best_model_path)\n",
    "            print(f'  Saved best model to {best_model_path}')\n",
    "\n",
    "        # Check early stopping\n",
    "        if early_stopping(val_metrics['f1']):\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_path:\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f'Loaded best model from {best_model_path} with F1: {checkpoint[\"val_f1\"]:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "model = train_model(model, train_loader, val_loader, num_epochs=resnet_cfg['num_epochs'], device=device)\n"
   ],
   "id": "d124dd6c75aff7d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "Let's perform inference on a sample audio file.\n"
   ],
   "id": "9950773b72011efa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_features(path, cfg):\n",
    "    y, sr = librosa.load(path, sr=cfg['sample_rate'], mono=True)\n",
    "    specs_dict = generate_multi_stft(y, sr)\n",
    "\n",
    "    # For inference, we'll use the middle frequency band (1000-4000Hz) with the middle FFT size (512)\n",
    "    # This is a simplification - in a real application, you might want to use all bands and FFT sizes\n",
    "    key = (\"1000-4000Hz\", 512)\n",
    "    if key in specs_dict:\n",
    "        spec = specs_dict[key]\n",
    "        return torch.tensor(spec).unsqueeze(0).unsqueeze(0)\n",
    "    else:\n",
    "        # Fallback to the first available spectrogram if the preferred one is not available\n",
    "        first_key = list(specs_dict.keys())[0]\n",
    "        spec = specs_dict[first_key]\n",
    "        return torch.tensor(spec).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def predict(model, wav_path, cfg):\n",
    "    model.eval()\n",
    "    x = extract_features(wav_path, cfg)\n",
    "    with torch.no_grad():\n",
    "        preds = model(x).squeeze().numpy()\n",
    "    return {label: float(preds[i]) for i, label in enumerate(LABELS)}\n",
    "\n",
    "# Get a sample WAV file for inference\n",
    "sample_wav = list(pathlib.Path('data/raw/IRMAS').rglob(\"*.wav\"))[0]\n",
    "print(f\"Sample WAV file: {sample_wav}\")\n",
    "\n",
    "# Perform inference\n",
    "results = predict(model, sample_wav, resnet_cfg)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPrediction results:\")\n",
    "for instrument, confidence in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{instrument}: {confidence:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results.keys(), results.values())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Instrument Detection Confidence')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "e154ebe6b874c678"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualize Audio and Spectrogram\n",
    "\n",
    "Let's visualize the audio waveform and spectrogram of the sample file.\n"
   ],
   "id": "94150721b354b7f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_audio(wav_path, cfg):\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(wav_path, sr=cfg['sample_rate'], mono=True)\n",
    "\n",
    "    # Compute multi-band STFT spectrograms\n",
    "    specs_dict = generate_multi_stft(y, sr)\n",
    "\n",
    "    # Plot waveform and selected spectrograms\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    # Plot waveform\n",
    "    plt.subplot(4, 1, 1)\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title('Waveform')\n",
    "\n",
    "    # Select three spectrograms to visualize (one from each frequency band with the middle FFT size)\n",
    "    keys_to_plot = [\n",
    "        (\"0-1000Hz\", 512),\n",
    "        (\"1000-4000Hz\", 512),\n",
    "        (\"4000-11025Hz\", 512)\n",
    "    ]\n",
    "\n",
    "    for i, key in enumerate(keys_to_plot):\n",
    "        if key in specs_dict:\n",
    "            plt.subplot(4, 1, i+2)\n",
    "            spec = specs_dict[key]\n",
    "            hop_length = 512 // 4  # hop_length for FFT size 512\n",
    "            librosa.display.specshow(spec, sr=sr, x_axis='time', hop_length=hop_length)\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Spectrogram: {key[0]}, FFT size: {key[1]}')\n",
    "        else:\n",
    "            print(f\"Spectrogram for {key} not found\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample audio\n",
    "visualize_audio(sample_wav, resnet_cfg)\n"
   ],
   "id": "8b73a177476e76e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the complete pipeline for multi-label musical instrument recognition:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Downloading and preprocessing the IRMAS dataset\n",
    "3. Defining and training a ResNet-34 model\n",
    "4. Performing inference on audio files\n",
    "5. Visualizing the results\n",
    "\n",
    "This notebook can be run in Google Colab without any special modifications to the codebase.\n"
   ],
   "id": "25d708d4ce2c07fc"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
