{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Instrument Classifier\n",
    "\n",
    "This notebook demonstrates a PyTorch project for multi-label musical instrument recognition from audio clips. It allows you to run the entire pipeline in Google Colab without any special modifications to the codebase.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's clone the repository and install the required dependencies.\n"
   ],
   "id": "72f151f9e587ca61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:08:46.113179Z",
     "start_time": "2025-05-31T08:08:46.108660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/your-username/instrument-classifier.git\n",
    "    %cd instrument-classifier\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install -r requirements.txt\n"
   ],
   "id": "e41cf2ee959bd67c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import Libraries\n",
    "\n",
    "Let's import the necessary libraries for our project.\n"
   ],
   "id": "26c0112ad361303c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:08:46.135085Z",
     "start_time": "2025-05-31T08:08:46.132328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import pathlib\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import hashlib\n",
    "import sys\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n"
   ],
   "id": "347e90cad1a1ade4",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "Let's define the configuration for our project.\n"
   ],
   "id": "47340b13e48e8ee6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:08:46.192501Z",
     "start_time": "2025-05-31T08:08:46.184320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create configs directory if it doesn't exist\n",
    "os.makedirs('configs', exist_ok=True)\n",
    "\n",
    "# Default configuration\n",
    "default_config = \"\"\"\n",
    "# Common hyper‑parameters\n",
    "sample_rate: 22050\n",
    "n_mels: 64\n",
    "hop_length: 512\n",
    "batch_size: 32\n",
    "num_epochs: 50\n",
    "learning_rate: 3e-4\n",
    "num_workers: 4\n",
    "\"\"\"\n",
    "\n",
    "# ResNet configuration\n",
    "resnet_config = \"\"\"\n",
    "# ResNet‑34 override\n",
    "model_name: resnet34\n",
    "learning_rate: 1e-4\n",
    "batch_size: 16\n",
    "\"\"\"\n",
    "\n",
    "# Write configurations to files\n",
    "with open('configs/default.yaml', 'w') as f:\n",
    "    f.write(default_config)\n",
    "\n",
    "with open('configs/model_resnet.yaml', 'w') as f:\n",
    "    f.write(resnet_config)\n",
    "\n",
    "# Load configuration\n",
    "cfg = yaml.safe_load(default_config)\n",
    "resnet_cfg = {**cfg, **yaml.safe_load(resnet_config)}\n",
    "\n",
    "# Display configuration\n",
    "print(\"Default configuration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nResNet configuration:\")\n",
    "for key, value in resnet_cfg.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ],
   "id": "18931ecd85bfbbf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default configuration:\n",
      "  sample_rate: 22050\n",
      "  n_mels: 64\n",
      "  hop_length: 512\n",
      "  batch_size: 32\n",
      "  num_epochs: 50\n",
      "  learning_rate: 3e-4\n",
      "  num_workers: 4\n",
      "\n",
      "ResNet configuration:\n",
      "  sample_rate: 22050\n",
      "  n_mels: 64\n",
      "  hop_length: 512\n",
      "  batch_size: 16\n",
      "  num_epochs: 50\n",
      "  learning_rate: 1e-4\n",
      "  num_workers: 4\n",
      "  model_name: resnet34\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download and Extract Data\n",
    "\n",
    "Let's download and extract the IRMAS dataset.\n"
   ],
   "id": "537b6591baeeddc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:27:18.144134Z",
     "start_time": "2025-05-31T08:18:09.082962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Download the IRMAS dataset (≈2 GB) and extract it.\n",
    "\n",
    "Example:\n",
    "    python data/download_irmas.py --out_dir data/raw\n",
    "\"\"\"\n",
    "import argparse, hashlib, os, sys, pathlib\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "IRMAS_URL = \"https://zenodo.org/record/1290750/files/IRMAS-TrainingData.zip?download=1\"\n",
    "MD5       = \"4fd9f5ed5a18d8e2687e6360b5f60afe\"  # expected archive checksum\n",
    "\n",
    "def md5(fname, chunk=2**20):\n",
    "    m = hashlib.md5()\n",
    "    with open(fname, 'rb') as fh:\n",
    "        while True:\n",
    "            data = fh.read(chunk)\n",
    "            if not data: break\n",
    "            m.update(data)\n",
    "    return m.hexdigest()\n",
    "\n",
    "def main(out_dir: str):\n",
    "    out_dir = pathlib.Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    archive_path = out_dir / \"IRMAS.zip\"\n",
    "\n",
    "    if not archive_path.exists():\n",
    "        print(\"Downloading IRMAS ...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(IRMAS_URL, archive_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed: {e}\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"Archive already exists, skipping download\")\n",
    "\n",
    "    print(\"Verifying checksum ...\")\n",
    "    if md5(archive_path) != MD5:\n",
    "        print(\"Checksum mismatch! The downloaded file may be corrupted.\", file=sys.stderr)\n",
    "        print(\"Try deleting the file and running the script again.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Extracting ...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(archive_path) as zf:\n",
    "            zf.extractall(out_dir)\n",
    "        print(\"Done. Data at\", out_dir)\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Extraction failed: The file is not a valid zip archive.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction failed: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--out_dir\", default=\"data/raw\", help=\"Destination directory\")\n",
    "\n",
    "    # Handle Jupyter notebook execution by ignoring unknown arguments\n",
    "    import sys\n",
    "    if 'ipykernel_launcher.py' in sys.argv[0]:\n",
    "        args, unknown = p.parse_known_args()\n",
    "    else:\n",
    "        args = p.parse_args()\n",
    "\n",
    "    main(args.out_dir)\n",
    "\n",
    "# When running in notebook, explicitly call main() with the output directory\n",
    "main(\"data/raw\")"
   ],
   "id": "d9910cb2ccc61bfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading IRMAS ...\n",
      "Verifying checksum ...\n",
      "Extracting ...\n",
      "Done. Data at data/raw\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Let's preprocess the data by converting WAV files to log-mel spectrograms.\n"
   ],
   "id": "9e2eb58b1fe3ab60"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-31T08:33:44.586757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(wav_path, cfg):\n",
    "    y, sr = librosa.load(wav_path, sr=cfg['sample_rate'], mono=True)\n",
    "    mels = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_mels=cfg['n_mels'], hop_length=cfg['hop_length'], fmin=30\n",
    "    )\n",
    "    logmel = librosa.power_to_db(mels, ref=np.max).astype(np.float32)\n",
    "    return logmel\n",
    "\n",
    "def preprocess_data(in_dir, out_dir, cfg):\n",
    "    in_dir, out_dir = pathlib.Path(in_dir), pathlib.Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create train and validation directories\n",
    "    train_dir = out_dir / 'train'\n",
    "    val_dir = out_dir / 'val'\n",
    "    train_dir.mkdir(exist_ok=True)\n",
    "    val_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Get all WAV files\n",
    "    wav_files = list(in_dir.rglob(\"*.wav\"))\n",
    "    print(f\"Found {len(wav_files)} WAV files\")\n",
    "\n",
    "    # Split into train and validation sets (90/10 split)\n",
    "    np.random.shuffle(wav_files)\n",
    "    split_idx = int(len(wav_files) * 0.9)\n",
    "    train_files = wav_files[:split_idx]\n",
    "    val_files = wav_files[split_idx:]\n",
    "\n",
    "    # Process training files\n",
    "    print(\"Processing training files...\")\n",
    "    for wav in tqdm(train_files):\n",
    "        spec = process_file(wav, cfg)\n",
    "        rel = wav.relative_to(in_dir).with_suffix(\".npy\")\n",
    "        out_path = train_dir / rel\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(out_path, spec)\n",
    "\n",
    "    # Process validation files\n",
    "    print(\"Processing validation files...\")\n",
    "    for wav in tqdm(val_files):\n",
    "        spec = process_file(wav, cfg)\n",
    "        rel = wav.relative_to(in_dir).with_suffix(\".npy\")\n",
    "        out_path = val_dir / rel\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(out_path, spec)\n",
    "\n",
    "    print(f\"Processed {len(train_files)} training files and {len(val_files)} validation files\")\n",
    "\n",
    "# Check if WAV files exist in data/raw/IRMAS\n",
    "irmas_path = pathlib.Path('data/raw/IRMAS')\n",
    "if irmas_path.exists() and any(irmas_path.rglob(\"*.wav\")):\n",
    "    print(\"Found WAV files in data/raw/IRMAS\")\n",
    "    preprocess_data('data/raw/IRMAS', 'data/processed', cfg)\n",
    "else:\n",
    "    # Check if WAV files exist in data/raw/IRMAS-TrainingData\n",
    "    training_data_path = pathlib.Path('data/raw/IRMAS-TrainingData')\n",
    "    if training_data_path.exists() and any(training_data_path.rglob(\"*.wav\")):\n",
    "        print(\"Found WAV files in data/raw/IRMAS-TrainingData\")\n",
    "        preprocess_data('data/raw/IRMAS-TrainingData', 'data/processed', cfg)\n",
    "    else:\n",
    "        # Check if WAV files exist directly in data/raw\n",
    "        raw_path = pathlib.Path('data/raw')\n",
    "        if raw_path.exists() and any(raw_path.rglob(\"*.wav\")):\n",
    "            print(\"Found WAV files in data/raw\")\n",
    "            preprocess_data('data/raw', 'data/processed', cfg)\n",
    "        else:\n",
    "            print(\"No WAV files found in data/raw or its subdirectories. Please check the extraction path.\")\n"
   ],
   "id": "5cf7e3889bce09b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found WAV files in data/raw/IRMAS-TrainingData\n",
      "Found 6705 WAV files\n",
      "Processing training files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/6034 [00:00<?, ?it/s]\u001B[AException ignored in: <function tqdm.__del__ at 0x7fbd752588b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/odahan/Technion/Semester_8/Deep_Learning/Project/.venv/lib/python3.9/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/home/odahan/Technion/Semester_8/Deep_Learning/Project/.venv/lib/python3.9/site-packages/tqdm/notebook.py\", line 282, in close\n",
      "    self.disp(bar_style='success', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "\n",
      "  0%|          | 1/6034 [00:11<18:58:41, 11.32s/it]\u001B[A\n",
      "  0%|          | 5/6034 [00:11<2:52:20,  1.72s/it] \u001B[A\n",
      "  0%|          | 9/6034 [00:11<1:19:15,  1.27it/s]\u001B[A\n",
      "  0%|          | 13/6034 [00:11<45:50,  2.19it/s] \u001B[A\n",
      "  0%|          | 17/6034 [00:11<29:47,  3.37it/s]\u001B[A\n",
      "  0%|          | 20/6034 [00:12<22:29,  4.46it/s]\u001B[A\n",
      "  0%|          | 23/6034 [00:12<17:32,  5.71it/s]\u001B[A\n",
      "  0%|          | 26/6034 [00:12<14:03,  7.13it/s]\u001B[A\n",
      "  0%|          | 30/6034 [00:12<10:17,  9.72it/s]\u001B[A\n",
      "  1%|          | 33/6034 [00:12<09:15, 10.81it/s]\u001B[A\n",
      "  1%|          | 36/6034 [00:13<09:30, 10.51it/s]\u001B[A\n",
      "  1%|          | 38/6034 [00:13<09:44, 10.25it/s]\u001B[A\n",
      "  1%|          | 40/6034 [00:13<09:24, 10.62it/s]\u001B[A\n",
      "  1%|          | 42/6034 [00:13<09:09, 10.90it/s]\u001B[A\n",
      "  1%|          | 44/6034 [00:13<10:35,  9.43it/s]\u001B[A\n",
      "  1%|          | 46/6034 [00:14<09:59,  9.98it/s]\u001B[A\n",
      "  1%|          | 48/6034 [00:14<09:05, 10.98it/s]\u001B[A\n",
      "  1%|          | 50/6034 [00:14<09:53, 10.09it/s]\u001B[A\n",
      "  1%|          | 53/6034 [00:14<07:41, 12.97it/s]\u001B[A\n",
      "  1%|          | 55/6034 [00:14<07:50, 12.72it/s]\u001B[A\n",
      "  1%|          | 57/6034 [00:14<07:15, 13.74it/s]\u001B[A\n",
      "  1%|          | 59/6034 [00:14<07:14, 13.74it/s]\u001B[A\n",
      "  1%|          | 61/6034 [00:15<07:11, 13.86it/s]\u001B[A\n",
      "  1%|          | 63/6034 [00:15<07:08, 13.95it/s]\u001B[A\n",
      "  1%|          | 65/6034 [00:15<07:24, 13.44it/s]\u001B[A\n",
      "  1%|          | 67/6034 [00:15<07:12, 13.81it/s]\u001B[A\n",
      "  1%|          | 69/6034 [00:15<06:47, 14.65it/s]\u001B[A\n",
      "  1%|          | 72/6034 [00:15<05:55, 16.75it/s]\u001B[A\n",
      "  1%|          | 75/6034 [00:15<05:31, 17.97it/s]\u001B[A\n",
      "  1%|▏         | 78/6034 [00:16<05:02, 19.67it/s]\u001B[A\n",
      "  1%|▏         | 81/6034 [00:16<04:57, 20.04it/s]\u001B[A\n",
      "  1%|▏         | 84/6034 [00:16<05:08, 19.27it/s]\u001B[A\n",
      "  1%|▏         | 86/6034 [00:16<05:27, 18.17it/s]\u001B[A\n",
      "  1%|▏         | 88/6034 [00:16<05:28, 18.09it/s]\u001B[A\n",
      "  1%|▏         | 90/6034 [00:16<05:49, 16.99it/s]\u001B[A\n",
      "  2%|▏         | 92/6034 [00:16<06:13, 15.91it/s]\u001B[A\n",
      "  2%|▏         | 94/6034 [00:16<05:55, 16.69it/s]\u001B[A\n",
      "  2%|▏         | 96/6034 [00:17<06:17, 15.74it/s]\u001B[A\n",
      "  2%|▏         | 98/6034 [00:17<06:38, 14.89it/s]\u001B[A\n",
      "  2%|▏         | 100/6034 [00:17<06:53, 14.34it/s]\u001B[A\n",
      "  2%|▏         | 102/6034 [00:17<08:16, 11.95it/s]\u001B[A\n",
      "  2%|▏         | 104/6034 [00:17<07:46, 12.71it/s]\u001B[A\n",
      "  2%|▏         | 106/6034 [00:17<07:06, 13.89it/s]\u001B[A\n",
      "  2%|▏         | 108/6034 [00:18<07:34, 13.05it/s]\u001B[A\n",
      "  2%|▏         | 110/6034 [00:18<06:48, 14.51it/s]\u001B[A\n",
      "  2%|▏         | 113/6034 [00:18<05:55, 16.65it/s]\u001B[A\n",
      "  2%|▏         | 116/6034 [00:18<05:15, 18.78it/s]\u001B[A\n",
      "  2%|▏         | 118/6034 [00:18<05:13, 18.87it/s]\u001B[A\n",
      "  2%|▏         | 121/6034 [00:18<05:02, 19.55it/s]\u001B[A\n",
      "  2%|▏         | 124/6034 [00:18<05:28, 17.98it/s]\u001B[A\n",
      "  2%|▏         | 126/6034 [00:19<05:52, 16.77it/s]\u001B[A\n",
      "  2%|▏         | 128/6034 [00:19<06:31, 15.07it/s]\u001B[A\n",
      "  2%|▏         | 130/6034 [00:19<07:23, 13.32it/s]\u001B[A\n",
      "  2%|▏         | 132/6034 [00:19<08:01, 12.25it/s]\u001B[A\n",
      "  2%|▏         | 134/6034 [00:19<08:12, 11.98it/s]\u001B[A\n",
      "  2%|▏         | 136/6034 [00:19<08:19, 11.82it/s]\u001B[A\n",
      "  2%|▏         | 138/6034 [00:20<08:53, 11.06it/s]\u001B[A\n",
      "  2%|▏         | 140/6034 [00:20<08:44, 11.23it/s]\u001B[A\n",
      "  2%|▏         | 142/6034 [00:20<09:05, 10.80it/s]\u001B[A\n",
      "  2%|▏         | 144/6034 [00:20<08:16, 11.86it/s]\u001B[A\n",
      "  2%|▏         | 146/6034 [00:20<08:02, 12.19it/s]\u001B[A\n",
      "  2%|▏         | 149/6034 [00:21<07:18, 13.42it/s]\u001B[A\n",
      "  3%|▎         | 152/6034 [00:21<06:01, 16.28it/s]\u001B[A\n",
      "  3%|▎         | 155/6034 [00:21<05:44, 17.05it/s]\u001B[A\n",
      "  3%|▎         | 157/6034 [00:21<05:41, 17.19it/s]\u001B[A\n",
      "  3%|▎         | 159/6034 [00:21<06:06, 16.04it/s]\u001B[A\n",
      "  3%|▎         | 161/6034 [00:21<07:27, 13.13it/s]\u001B[A\n",
      "  3%|▎         | 163/6034 [00:22<09:27, 10.34it/s]\u001B[A\n",
      "  3%|▎         | 165/6034 [00:22<10:13,  9.56it/s]\u001B[A\n",
      "  3%|▎         | 167/6034 [00:22<10:36,  9.22it/s]\u001B[A\n",
      "  3%|▎         | 169/6034 [00:22<09:52,  9.89it/s]\u001B[A\n",
      "  3%|▎         | 172/6034 [00:22<07:46, 12.55it/s]\u001B[A\n",
      "  3%|▎         | 174/6034 [00:23<07:38, 12.77it/s]\u001B[A\n",
      "  3%|▎         | 177/6034 [00:23<06:29, 15.04it/s]\u001B[A\n",
      "  3%|▎         | 179/6034 [00:23<06:40, 14.63it/s]\u001B[A\n",
      "  3%|▎         | 181/6034 [00:23<06:13, 15.67it/s]\u001B[A\n",
      "  3%|▎         | 183/6034 [00:23<06:21, 15.32it/s]\u001B[A\n",
      "  3%|▎         | 185/6034 [00:23<06:11, 15.73it/s]\u001B[A\n",
      "  3%|▎         | 187/6034 [00:23<06:13, 15.64it/s]\u001B[A\n",
      "  3%|▎         | 189/6034 [00:23<06:54, 14.11it/s]\u001B[A\n",
      "  3%|▎         | 191/6034 [00:24<06:24, 15.21it/s]\u001B[A\n",
      "  3%|▎         | 193/6034 [00:24<06:07, 15.92it/s]\u001B[A\n",
      "  3%|▎         | 196/6034 [00:24<05:18, 18.34it/s]\u001B[A\n",
      "  3%|▎         | 198/6034 [00:24<05:24, 17.99it/s]\u001B[A\n",
      "  3%|▎         | 200/6034 [00:24<05:24, 17.98it/s]\u001B[A\n",
      "  3%|▎         | 203/6034 [00:24<04:53, 19.84it/s]\u001B[A\n",
      "  3%|▎         | 205/6034 [00:24<05:03, 19.23it/s]\u001B[A\n",
      "  3%|▎         | 207/6034 [00:25<06:36, 14.71it/s]\u001B[A\n",
      "  3%|▎         | 209/6034 [00:25<06:31, 14.86it/s]\u001B[A\n",
      "  4%|▎         | 212/6034 [00:25<05:41, 17.06it/s]\u001B[A\n",
      "  4%|▎         | 215/6034 [00:25<05:16, 18.38it/s]\u001B[A\n",
      "  4%|▎         | 217/6034 [00:25<05:35, 17.34it/s]\u001B[A\n",
      "  4%|▎         | 219/6034 [00:25<05:57, 16.26it/s]\u001B[A\n",
      "  4%|▎         | 221/6034 [00:25<05:52, 16.48it/s]\u001B[A\n",
      "  4%|▎         | 223/6034 [00:25<06:08, 15.79it/s]\u001B[A\n",
      "  4%|▎         | 225/6034 [00:26<06:38, 14.57it/s]\u001B[A\n",
      "  4%|▍         | 227/6034 [00:26<06:46, 14.28it/s]\u001B[A\n",
      "  4%|▍         | 229/6034 [00:26<06:45, 14.31it/s]\u001B[A\n",
      "  4%|▍         | 231/6034 [00:26<06:20, 15.23it/s]\u001B[A\n",
      "  4%|▍         | 233/6034 [00:26<06:16, 15.40it/s]\u001B[A\n",
      "  4%|▍         | 236/6034 [00:26<05:51, 16.51it/s]\u001B[A\n",
      "  4%|▍         | 239/6034 [00:26<05:16, 18.29it/s]\u001B[A\n",
      "  4%|▍         | 241/6034 [00:27<05:53, 16.37it/s]\u001B[A\n",
      "  4%|▍         | 243/6034 [00:27<06:16, 15.38it/s]\u001B[A\n",
      "  4%|▍         | 245/6034 [00:27<07:24, 13.03it/s]\u001B[A\n",
      "  4%|▍         | 247/6034 [00:27<09:06, 10.59it/s]\u001B[A\n",
      "  4%|▍         | 249/6034 [00:27<08:30, 11.33it/s]\u001B[A\n",
      "  4%|▍         | 251/6034 [00:28<07:51, 12.26it/s]\u001B[A\n",
      "  4%|▍         | 254/6034 [00:28<06:15, 15.40it/s]\u001B[A\n",
      "  4%|▍         | 256/6034 [00:28<06:18, 15.28it/s]\u001B[A\n",
      "  4%|▍         | 260/6034 [00:28<04:51, 19.83it/s]\u001B[A\n",
      "  4%|▍         | 263/6034 [00:28<05:18, 18.12it/s]\u001B[A\n",
      "  4%|▍         | 265/6034 [00:28<05:17, 18.17it/s]\u001B[A\n",
      "  4%|▍         | 267/6034 [00:28<05:30, 17.44it/s]\u001B[A\n",
      "  4%|▍         | 270/6034 [00:28<05:03, 18.99it/s]\u001B[A\n",
      "  5%|▍         | 272/6034 [00:29<05:00, 19.20it/s]\u001B[A\n",
      "  5%|▍         | 274/6034 [00:29<05:17, 18.16it/s]\u001B[A\n",
      "  5%|▍         | 277/6034 [00:29<04:41, 20.44it/s]\u001B[A\n",
      "  5%|▍         | 280/6034 [00:29<05:38, 16.98it/s]\u001B[A\n",
      "  5%|▍         | 282/6034 [00:29<05:57, 16.10it/s]\u001B[A\n",
      "  5%|▍         | 284/6034 [00:29<06:07, 15.67it/s]\u001B[A\n",
      "  5%|▍         | 286/6034 [00:29<06:19, 15.16it/s]\u001B[A\n",
      "  5%|▍         | 288/6034 [00:30<06:26, 14.86it/s]\u001B[A\n",
      "  5%|▍         | 290/6034 [00:30<06:59, 13.68it/s]\u001B[A\n",
      "  5%|▍         | 292/6034 [00:30<08:08, 11.75it/s]\u001B[A\n",
      "  5%|▍         | 294/6034 [00:30<08:10, 11.70it/s]\u001B[A\n",
      "  5%|▍         | 297/6034 [00:30<06:58, 13.70it/s]\u001B[A"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Models\n",
    "\n",
    "Let's define the models for our project.\n"
   ],
   "id": "9ee7c34e386b14cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "class CNNBaseline(nn.Module):\n",
    "    \"\"\"Simple CNN baseline for audio classification.\"\"\"\n",
    "    def __init__(self, n_classes=11):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # [B,16,32,32]\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # [B,32,16,16]\n",
    "        x = self.pool(self.relu(self.conv3(x)))  # [B,64,8,8]\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class ResNetSpec(nn.Module):\n",
    "    \"\"\"ResNet‑34 backbone adapted for single‑channel spectrogram input.\"\"\"\n",
    "    def __init__(self, n_classes=11):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet34(weights=None)\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Replace FC\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(self.backbone.fc.in_features, n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Display model architectures\n",
    "print(\"CNN Baseline Architecture:\")\n",
    "print(CNNBaseline())\n",
    "print(\"\\nResNet Architecture:\")\n",
    "print(ResNetSpec())\n"
   ],
   "id": "50f99bc06b222371"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Dataset and DataLoader\n",
    "\n",
    "Let's define the dataset and dataloader for our project.\n"
   ],
   "id": "efe9d9d380194a32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LABELS = [\"cello\", \"clarinet\", \"flute\", \"acoustic_guitar\", \"organ\", \"piano\", \"saxophone\", \"trumpet\", \"violin\", \"voice\", \"other\"]\n",
    "\n",
    "class NpyDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.files = list(pathlib.Path(root).rglob(\"*.npy\"))\n",
    "        self.label_map = {label: i for i, label in enumerate(LABELS)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spec = np.load(self.files[idx])\n",
    "        x = torch.tensor(spec).unsqueeze(0)  # [1,H,W]\n",
    "\n",
    "        # Parse label from folder name\n",
    "        label_str = self.files[idx].parent.name.split(\"_\")[0]\n",
    "        y = torch.zeros(len(LABELS))\n",
    "\n",
    "        # Map label string to index\n",
    "        if label_str in self.label_map:\n",
    "            y[self.label_map[label_str]] = 1.0\n",
    "\n",
    "        return x, y\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_ds = NpyDataset(\"data/processed/train\")\n",
    "val_ds = NpyDataset(\"data/processed/val\")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_ds)}\")\n",
    "print(f\"Validation dataset size: {len(val_ds)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=resnet_cfg['batch_size'], shuffle=True, num_workers=resnet_cfg['num_workers'])\n",
    "val_loader = DataLoader(val_ds, batch_size=resnet_cfg['batch_size'], shuffle=False, num_workers=resnet_cfg['num_workers'])\n"
   ],
   "id": "c013489269d1c880"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Model\n",
    "\n",
    "Let's define the model for our project.\n"
   ],
   "id": "204355d3c004116b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "\n",
    "class MetricCollection:\n",
    "    def __init__(self, n_classes):\n",
    "        self.accuracy = Accuracy(task=\"multilabel\", num_labels=n_classes)\n",
    "        self.precision = Precision(task=\"multilabel\", num_labels=n_classes)\n",
    "        self.recall = Recall(task=\"multilabel\", num_labels=n_classes)\n",
    "        self.f1 = F1Score(task=\"multilabel\", num_labels=n_classes)\n",
    "\n",
    "    def __call__(self, preds, targets):\n",
    "        return {\n",
    "            \"accuracy\": self.accuracy(preds, targets),\n",
    "            \"precision\": self.precision(preds, targets),\n",
    "            \"recall\": self.recall(preds, targets),\n",
    "            \"f1\": self.f1(preds, targets)\n",
    "        }\n",
    "\n",
    "class InstrumentModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        n_classes = len(LABELS)\n",
    "        if cfg.get(\"model_name\", \"cnn\") == \"resnet34\":\n",
    "            self.model = ResNetSpec(n_classes)\n",
    "        else:\n",
    "            self.model = CNNBaseline(n_classes)\n",
    "        self.metrics = MetricCollection(n_classes)\n",
    "        self.lr = cfg[\"learning_rate\"]\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss_and_metrics(self, batch, stage=\"train\"):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = torch.nn.functional.binary_cross_entropy(preds, y)\n",
    "        metrics = self.metrics(preds, y)\n",
    "        return loss, metrics, preds\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "# Initialize model\n",
    "model = InstrumentModel(resnet_cfg)\n",
    "print(\"Model initialized with ResNet-34 backbone\")\n"
   ],
   "id": "a451c98dd4ecca90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train Model\n",
    "\n",
    "Let's train the model using PyTorch.\n"
   ],
   "id": "ccbb4da8c8dd0561"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Create directories for saving models\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Custom implementation of early stopping\"\"\"\n",
    "    def __init__(self, patience=5, mode='max', min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "\n",
    "        if self.mode == 'max':\n",
    "            if score > self.best_score + self.min_delta:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        else:  # mode == 'min'\n",
    "            if score < self.best_score - self.min_delta:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Train the model using standard PyTorch training loop\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = model.get_optimizer()\n",
    "\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, mode='max')\n",
    "\n",
    "    # Initialize best model tracking\n",
    "    best_f1 = 0.0\n",
    "    best_model_path = None\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_metrics = {}\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = [x.to(device) for x in batch]\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass, compute loss and metrics\n",
    "            loss, metrics, _ = model.compute_loss_and_metrics(batch, stage='train')\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            for k, v in metrics.items():\n",
    "                if k not in train_metrics:\n",
    "                    train_metrics[k] = 0.0\n",
    "                train_metrics[k] += v.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': loss.item(),\n",
    "                'f1': metrics['f1'].item()\n",
    "            })\n",
    "\n",
    "        # Compute average metrics for the epoch\n",
    "        train_loss /= len(train_loader)\n",
    "        for k in train_metrics:\n",
    "            train_metrics[k] /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_metrics = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                batch = [x.to(device) for x in batch]\n",
    "\n",
    "                # Forward pass, compute loss and metrics\n",
    "                loss, metrics, _ = model.compute_loss_and_metrics(batch, stage='val')\n",
    "\n",
    "                # Update metrics\n",
    "                val_loss += loss.item()\n",
    "                for k, v in metrics.items():\n",
    "                    if k not in val_metrics:\n",
    "                        val_metrics[k] = 0.0\n",
    "                    val_metrics[k] += v.item()\n",
    "\n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': loss.item(),\n",
    "                    'f1': metrics['f1'].item()\n",
    "                })\n",
    "\n",
    "        # Compute average metrics for the epoch\n",
    "        val_loss /= len(val_loader)\n",
    "        for k in val_metrics:\n",
    "            val_metrics[k] /= len(val_loader)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, F1: {train_metrics[\"f1\"]:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, F1: {val_metrics[\"f1\"]:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_f1:\n",
    "            best_f1 = val_metrics['f1']\n",
    "            best_model_path = f'checkpoints/best-{epoch+1:02d}-{val_metrics[\"f1\"]:.2f}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': val_metrics['f1'],\n",
    "                'val_loss': val_loss\n",
    "            }, best_model_path)\n",
    "            print(f'  Saved best model to {best_model_path}')\n",
    "\n",
    "        # Check early stopping\n",
    "        if early_stopping(val_metrics['f1']):\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_path:\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f'Loaded best model from {best_model_path} with F1: {checkpoint[\"val_f1\"]:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "model = train_model(model, train_loader, val_loader, num_epochs=resnet_cfg['num_epochs'], device=device)\n"
   ],
   "id": "d124dd6c75aff7d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "Let's perform inference on a sample audio file.\n"
   ],
   "id": "9950773b72011efa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_features(path, cfg):\n",
    "    y, sr = librosa.load(path, sr=cfg['sample_rate'], mono=True)\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=cfg['n_mels'], hop_length=cfg['hop_length'])\n",
    "    logmel = librosa.power_to_db(mels, ref=np.max).astype(np.float32)\n",
    "    return torch.tensor(logmel).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def predict(model, wav_path, cfg):\n",
    "    model.eval()\n",
    "    x = extract_features(wav_path, cfg)\n",
    "    with torch.no_grad():\n",
    "        preds = model(x).squeeze().numpy()\n",
    "    return {label: float(preds[i]) for i, label in enumerate(LABELS)}\n",
    "\n",
    "# Get a sample WAV file for inference\n",
    "sample_wav = list(pathlib.Path('data/raw/IRMAS').rglob(\"*.wav\"))[0]\n",
    "print(f\"Sample WAV file: {sample_wav}\")\n",
    "\n",
    "# Perform inference\n",
    "results = predict(model, sample_wav, resnet_cfg)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPrediction results:\")\n",
    "for instrument, confidence in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{instrument}: {confidence:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results.keys(), results.values())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Instrument Detection Confidence')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "e154ebe6b874c678"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualize Audio and Spectrogram\n",
    "\n",
    "Let's visualize the audio waveform and spectrogram of the sample file.\n"
   ],
   "id": "94150721b354b7f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_audio(wav_path, cfg):\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(wav_path, sr=cfg['sample_rate'], mono=True)\n",
    "\n",
    "    # Compute spectrogram\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=cfg['n_mels'], hop_length=cfg['hop_length'])\n",
    "    logmel = librosa.power_to_db(mels, ref=np.max)\n",
    "\n",
    "    # Plot waveform and spectrogram\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title('Waveform')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    librosa.display.specshow(logmel, sr=sr, x_axis='time', y_axis='mel', hop_length=cfg['hop_length'])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel Spectrogram')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample audio\n",
    "visualize_audio(sample_wav, resnet_cfg)\n"
   ],
   "id": "8b73a177476e76e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the complete pipeline for multi-label musical instrument recognition:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Downloading and preprocessing the IRMAS dataset\n",
    "3. Defining and training a ResNet-34 model\n",
    "4. Performing inference on audio files\n",
    "5. Visualizing the results\n",
    "\n",
    "This notebook can be run in Google Colab without any special modifications to the codebase.\n"
   ],
   "id": "25d708d4ce2c07fc"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
