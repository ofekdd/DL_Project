{
 "cells": [
  {
   "cell_type": "code",
   "id": "fae51c5c100772c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.171955Z",
     "start_time": "2025-06-07T07:18:27.160441Z"
    }
   },
   "source": "import sys\nimport warnings, tqdm\n\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmWarning)\nsys.modules['tqdm.notebook'] = tqdm\nsys.modules['tqdm.autonotebook'] = tqdm\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    import os\n    \n    # Mount Google Drive first\n    print(\"üìÅ Mounting Google Drive...\")\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Verify your dataset folder exists\n    drive_dataset_path = \"/content/drive/MyDrive/datasets/IRMAS\"\n    if os.path.exists(drive_dataset_path):\n        print(f\"‚úÖ Found dataset folder at: {drive_dataset_path}\")\n    else:\n        print(f\"‚ö†Ô∏è  Dataset folder not found at: {drive_dataset_path}\")\n        print(\"Creating the folder structure...\")\n        os.makedirs(drive_dataset_path, exist_ok=True)\n\n    # Always start fresh and clone the specific branch\n    print(\"üóëÔ∏è Cleaning up any existing project...\")\n    %cd /content\n    !rm -rf DL_Project\n\n    #TODO: Fix the branch according to the latest changes\n    print(\"üì• Cloning specific branch 'master'...\")\n    !git clone -b master https://github.com/ofekdd/DL_Project.git\n    %cd DL_Project\n\n    # Verify we're on the correct branch\n    print(\"üîç Verifying branch...\")\n    !git branch\n    !git log --oneline -n 3\n\n    # Install dependencies\n    print(\"üì¶ Installing dependencies...\")\n    !pip install -r requirements.txt\n\n    print(\"‚úÖ Setup complete with branch 'master'!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.238878Z",
     "start_time": "2025-06-07T07:18:27.232410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the current working directory and ensure it is the project root\n",
    "from pathlib import Path\n",
    "print(\"CWD :\", Path.cwd())                    # where the kernel is running\n",
    "print(\"Exists?\", Path('configs').is_dir())    # should be True if CWD is project root\n"
   ],
   "id": "3d6ed40822d8c61b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD : /home/odahan/Technion/Semester_8/Deep_Learning/Project/notebooks\n",
      "Exists? False\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.301113Z",
     "start_time": "2025-06-07T07:18:27.291167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Define the path to the YAML configuration file\n",
    "workspace = '/home/odahan/Technion/Semester_8/Deep_Learning/Project'\n",
    "yaml_path = 'configs/panns_enhanced.yaml' if IN_COLAB else f'{workspace}/configs/panns_enhanced.yaml'\n",
    "print(yaml_path)\n",
    "# Open and load the YAML file\n",
    "with open(yaml_path, 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "\n",
    "print(\"PANNs-enhanced configuration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "id": "4be9f8e58789698a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/odahan/Technion/Semester_8/Deep_Learning/Project/configs/multi_stft_cnn.yaml\n",
      "9cnn configuration:\n",
      "  model_name: multi_stft_cnn\n",
      "  sample_rate: 22050\n",
      "  n_mels: 64\n",
      "  hop_length: 512\n",
      "  batch_size: 8\n",
      "  num_epochs: 50\n",
      "  learning_rate: 2e-4\n",
      "  num_workers: 4\n",
      "  n_branches: 9\n",
      "  branch_output_dim: 128\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Download and setup the IRMAS dataset using Google Drive\nfrom data.download_irmas import main as download_irmas_main\nimport pathlib\nimport os\nimport zipfile\n\n# Set DATA_CACHE to your Google Drive folder\nif IN_COLAB:\n    DATA_CACHE = \"/content/drive/MyDrive/datasets/IRMAS\"\n    print(f\"üìÅ Using Google Drive dataset folder: {DATA_CACHE}\")\nelse:\n    # Check for dataset in home directory (for local development)\n    home_dataset_path = pathlib.Path.home() / \"datasets\" / \"irmas\"\n    DATA_CACHE = str(home_dataset_path if home_dataset_path.exists() else \"data/raw\")\n\n# Create the dataset directory if it doesn't exist\nos.makedirs(DATA_CACHE, exist_ok=True)\n\n# Check what we have in the Google Drive folder\nprint(\"üîç Current contents of your Google Drive dataset folder:\")\nif os.path.exists(DATA_CACHE):\n    items = os.listdir(DATA_CACHE)\n    for item in items:\n        item_path = os.path.join(DATA_CACHE, item)\n        if os.path.isdir(item_path):\n            print(f\"  üìÅ {item}/\")\n        else:\n            print(f\"  üìÑ {item}\")\n    if not items:\n        print(\"  (empty)\")\nelse:\n    print(\"  Folder does not exist, will be created\")\n\n# Define all IRMAS dataset parts\nirmas_datasets = {\n    \"IRMAS-TrainingData.zip\": \"IRMAS-TrainingData\",\n    \"IRMAS-TestingData-Part1.zip\": \"IRMAS-TestingData-Part1\", \n    \"IRMAS-TestingData-Part2.zip\": \"IRMAS-TestingData-Part2\",\n    \"IRMAS-TestingData-Part3.zip\": \"IRMAS-TestingData-Part3\"\n}\n\n# Check which parts are missing\nmissing_zips = []\nmissing_folders = []\n\nfor zip_name, folder_name in irmas_datasets.items():\n    zip_path = pathlib.Path(DATA_CACHE) / zip_name\n    folder_path = pathlib.Path(DATA_CACHE) / folder_name\n    \n    if not zip_path.exists():\n        missing_zips.append(zip_name)\n        print(f\"‚ùå Missing: {zip_name}\")\n    else:\n        print(f\"‚úÖ Found: {zip_name}\")\n    \n    if not folder_path.exists():\n        missing_folders.append(folder_name)\n        print(f\"‚ùå Missing extracted: {folder_name}\")\n    else:\n        print(f\"‚úÖ Found extracted: {folder_name}\")\n\n# Download missing zip files\nif missing_zips:\n    print(f\"\\nüì• Downloading {len(missing_zips)} missing dataset(s) to Google Drive...\")\n    download_irmas_main(pathlib.Path(DATA_CACHE))\nelse:\n    print(\"\\n‚úÖ All IRMAS zip files already present in Google Drive. Skipping download.\")\n\n# Extract missing folders\nfor zip_name, folder_name in irmas_datasets.items():\n    zip_path = pathlib.Path(DATA_CACHE) / zip_name\n    folder_path = pathlib.Path(DATA_CACHE) / folder_name\n    \n    if zip_path.exists() and not folder_path.exists():\n        print(f\"üì¶ Extracting {zip_name}...\")\n        try:\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(DATA_CACHE)\n            print(f\"‚úÖ Extracted {folder_name}\")\n        except Exception as e:\n            print(f\"‚ùå Failed to extract {zip_name}: {e}\")\n\n# Final verification\nprint(\"\\nüîç Final verification of dataset extraction:\")\nall_ready = True\nfor folder_name in irmas_datasets.values():\n    expected_path = pathlib.Path(DATA_CACHE) / folder_name\n    if expected_path.exists():\n        print(f\"‚úÖ {folder_name} found at {expected_path}\")\n    else:\n        print(f\"‚ùå {folder_name} missing at {expected_path}\")\n        all_ready = False\n\nif all_ready:\n    print(\"üéâ All IRMAS dataset parts are ready in Google Drive!\")\n    # Set IRMAS root to base path for compatibility with existing code\n    irmas_root = pathlib.Path(DATA_CACHE)\n    print(f\"üìÇ IRMAS base path set to: {irmas_root}\")\nelse:\n    print(\"\\n‚ö†Ô∏è Some dataset parts are missing. Please check for errors above.\")\n    irmas_root = None\n\n# Show final folder structure\nprint(f\"\\nüìä Final Google Drive dataset structure:\")\nif os.path.exists(DATA_CACHE):\n    for item in sorted(os.listdir(DATA_CACHE)):\n        item_path = os.path.join(DATA_CACHE, item)\n        if os.path.isdir(item_path):\n            # Count files in each subfolder\n            try:\n                file_count = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])\n                print(f\"  üìÅ {item}/ ({file_count} files)\")\n            except:\n                print(f\"  üìÅ {item}/\")\n        else:\n            # Show file size for zip files\n            try:\n                size_mb = os.path.getsize(item_path) / (1024*1024)\n                print(f\"  üìÑ {item} ({size_mb:.1f} MB)\")\n            except:\n                print(f\"  üìÑ {item}\")",
   "id": "6e04ee7069ac34ec",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fix NumPy compatibility issue\n",
    "import sys\n",
    "\n",
    "print(\"üîß Fixing NumPy compatibility...\")\n",
    "\n",
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Current NumPy version: {np.__version__}\")\n",
    "\n",
    "# If NumPy 2.0+, we need to downgrade or use a workaround\n",
    "if int(np.__version__.split('.')[0]) >= 2:\n",
    "    print(\"‚ö†Ô∏è  NumPy 2.0+ detected. Installing compatible version...\")\n",
    "    !pip install \"numpy<2.0\" --quiet\n",
    "\n",
    "    # Restart the kernel to load the new NumPy version\n",
    "    print(\"üîÑ Restarting kernel to load compatible NumPy...\")\n",
    "    import os\n",
    "\n",
    "    os.kill(os.getpid(), 9)  # This will restart the kernel in Colab\n",
    "else:\n",
    "    print(\"‚úÖ NumPy version is compatible\")"
   ],
   "id": "d9db882fdcb43259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from data.download_irmas import load_irmas_audio_dataset, load_irmas_testing_dataset\n",
    "import pathlib\n",
    "\n",
    "if irmas_root and irmas_root.exists():\n",
    "\n",
    "    print(\"üìÅ Dataset creation settings from config:\")\n",
    "    print(f\"   max_original_samples: {cfg.get('max_original_samples', 50)}\")\n",
    "    print(f\"   num_mixtures: {cfg.get('num_mixtures', 100)}\")\n",
    "    print(f\"   min_instruments: {cfg.get('min_instruments', 1)}\")\n",
    "    print(f\"   max_instruments: {cfg.get('max_instruments', 2)}\")\n",
    "\n",
    "    # Normalize root\n",
    "    base_root = irmas_root.parent if irmas_root.name == \"IRMAS-TrainingData\" else irmas_root\n",
    "\n",
    "    # Define separate paths\n",
    "    training_path = base_root / \"IRMAS-TrainingData\"\n",
    "    testing_paths = [base_root / f\"IRMAS-TestingData-Part{i}\" for i in range(1, 4)]\n",
    "\n",
    "    print(f\"üìÇ IRMAS paths:\")\n",
    "    print(f\"   ‚îú‚îÄ Training: {training_path}\")\n",
    "    for tp in testing_paths:\n",
    "        print(f\"   ‚îî‚îÄ Test Part: {tp}\")\n",
    "\n",
    "    # Load training data (single-label)\n",
    "    original_dataset = load_irmas_audio_dataset(base_root, cfg,\n",
    "                                            max_samples=cfg.get(\"max_original_samples\"))\n",
    "\n",
    "    # Load test data (multi-label)\n",
    "    test_datasets = []\n",
    "    for _ in testing_paths:        # paths 1-3 are *ignored* here\n",
    "        test_datasets.extend(load_irmas_testing_dataset(base_root, cfg))\n",
    "\n",
    "    # Merge datasets if needed\n",
    "    total_loaded = len(original_dataset) + len(test_datasets)\n",
    "    print(f\"\\nüìä Final dataset summary:\")\n",
    "    print(f\"   ‚úÖ Training samples loaded: {len(original_dataset)}\")\n",
    "    print(f\"   ‚úÖ Testing samples loaded: {len(test_datasets)}\")\n",
    "    print(f\"   ‚úÖ Total samples loaded:   {total_loaded}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå IRMAS root not found or invalid. Please run the download step first.\")"
   ],
   "id": "4277635d9e525b8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "if irmas_root:\n    print(f\"IRMAS dataset found at: {irmas_root}\")\n    \n    # Use Google Drive for processed features cache as well to avoid reprocessing\n    if IN_COLAB:\n        PROCESSED_DIR = \"/content/drive/MyDrive/datasets/IRMAS_processed_features\"\n        print(f\"üìÅ Using Google Drive for processed features: {PROCESSED_DIR}\")\n    else:\n        PROCESSED_DIR = \"data/processed\"\n\n    # Create processed directory if it doesn't exist\n    os.makedirs(PROCESSED_DIR, exist_ok=True)\n\n    # Use config value for original data percentage\n    original_data_percentage = cfg.get('original_data_percentage', 0.1)\n    print(f\"Using {original_data_percentage*100}% of original IRMAS data (from config)\")\n\n    # Check if processed features already exist\n    expected_processed_dirs = [\n        os.path.join(PROCESSED_DIR, \"train\"),\n        os.path.join(PROCESSED_DIR, \"val\"), \n        os.path.join(PROCESSED_DIR, \"test\")\n    ]\n    \n    processing_needed = not all(os.path.exists(d) and os.listdir(d) for d in expected_processed_dirs)\n    \n    if processing_needed:\n        print(\"üìä Processed features not found or incomplete. Starting preprocessing...\")\n        from data.preprocess import preprocess_data\n        \n        preprocess_data(\n            irmas_root=irmas_root,\n            out_dir=PROCESSED_DIR,\n            cfg=cfg,\n            original_data_percentage=original_data_percentage\n        )\n        print(f\"‚úÖ Preprocessing complete! Features saved to Google Drive: {PROCESSED_DIR}\")\n    else:\n        print(\"‚úÖ Processed features found in Google Drive. Skipping preprocessing.\")\n        \n    # Show processed features summary\n    print(f\"\\nüìä Processed features summary:\")\n    for split in [\"train\", \"val\", \"test\"]:\n        split_path = os.path.join(PROCESSED_DIR, split)\n        if os.path.exists(split_path):\n            file_count = len([f for f in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, f))])\n            print(f\"  üìÅ {split}/ ({file_count} sample folders)\")\n        else:\n            print(f\"  ‚ùå {split}/ (missing)\")\n\nelse:\n    print(\"Could not locate IRMAS dataset after download. Check paths and try again.\")",
   "id": "e96b51ebbb46946c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# üì¶  Configure paths & echo training settings\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"üîß Configuring data paths and training settings...\")\n\n# 1) show YAML-driven parameters\nbase_max_samples = cfg.get(\"max_samples\", None)\nprint(\"üìÅ Base configuration from YAML:\")\nprint(f\"   max_samples            : {base_max_samples}\")\nprint(f\"   max_original_samples   : {cfg.get('max_original_samples', 50)}\")\nprint(f\"   num_mixtures           : {cfg.get('num_mixtures', 100)}\")\nprint(f\"   min_instruments        : {cfg.get('min_instruments', 1)}\")\nprint(f\"   max_instruments        : {cfg.get('max_instruments', 2)}\")\nprint(f\"   original_data_percentage : {cfg.get('original_data_percentage', 0.1)}\")\n\n# 2) notebook override notice\nif base_max_samples != cfg.get(\"max_samples\"):\n    print(f\"‚ö†Ô∏è  Notebook override: max_samples changed to {cfg.get('max_samples')}\")\n\n# 3) define processed-data locations (use Google Drive path)\nif IN_COLAB:\n    PROCESSED_DIR = \"/content/drive/MyDrive/datasets/IRMAS_processed_features\"\nelse:\n    PROCESSED_DIR = \"data/processed\"\n    \ncfg.update(\n    {\n        \"data_dir\": PROCESSED_DIR,\n        \"train_dir\": f\"{PROCESSED_DIR}/train\",\n        \"val_dir\": f\"{PROCESSED_DIR}/val\",\n        \"test_dir\": f\"{PROCESSED_DIR}/test\",\n    }\n)\n\nprint(\"\\nüìÇ Data directories:\")\nprint(f\"   Processed data : {cfg['data_dir']}\")\nprint(f\"   Training       : {cfg['train_dir']}\")\nprint(f\"   Validation     : {cfg['val_dir']}\")\nprint(f\"   Test           : {cfg['test_dir']}\")\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# üîç  Verify directory existence & detailed sample counts\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nimport pathlib\n\ndef count_sample_folders(split_path: pathlib.Path):\n    counts = dict(original=0, mixed=0, irmasTest=0, other=0)\n    if not split_path.exists():\n        return counts  # all zeros\n    for d in split_path.iterdir():\n        if not d.is_dir():\n            continue\n        n = d.name\n        if n.startswith(\"original_\"):\n            counts[\"original\"] += 1\n        elif n.startswith(\"mixed_\"):\n            counts[\"mixed\"] += 1\n        elif n.startswith(\"irmasTest_\"):\n            counts[\"irmasTest\"] += 1\n        else:\n            counts[\"other\"] += 1\n    return counts\n\n\nprint(\"\\nüîç Verifying data directories & split composition:\")\nfor split in [\"train\", \"val\", \"test\"]:\n    path = pathlib.Path(cfg[f\"{split}_dir\"])\n    counts = count_sample_folders(path)\n    total = sum(counts.values())\n\n    status = \"‚úÖ\" if total else \"‚ùå\"\n    print(f\"\\n{status} {split.upper()} ({path}): {total} total sample folders\")\n    print(f\"      ‚Ä¢ original_:  {counts['original']}\")\n    print(f\"      ‚Ä¢ mixed_:     {counts['mixed']}\")\n    print(f\"      ‚Ä¢ irmasTest_: {counts['irmasTest']}\")\n    if counts[\"other\"]:\n        print(f\"      ‚Ä¢ other:      {counts['other']}  ‚Üê check if expected!\")\n\n    # sanity warnings\n    if split in (\"train\", \"val\") and counts[\"irmasTest\"]:\n        print(\"      ‚ö†Ô∏è  Unexpected irmasTest_ folders in this split!\")\n    if split == \"test\" and (counts[\"original\"] or counts[\"mixed\"]):\n        print(\"      ‚ö†Ô∏è  Test split should contain ONLY irmasTest_ folders.\")\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# üéõÔ∏è  Final training configuration summary\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"\\n‚úÖ Final training configuration:\")\nprint(f\"   Training samples limit : {cfg.get('max_samples', 'unlimited')}\")\nprint(f\"   Batch size             : {cfg.get('batch_size')}\")\nprint(\n    f\"   Validation limit       : {cfg.get('limit_val_batches', 1.0)} \"\n    f\"({'percentage' if cfg.get('limit_val_batches', 1.0) <= 1 else 'batches'})\"\n)\nprint(f\"   Learning rate          : {cfg.get('learning_rate')}\")\nprint(f\"   Epochs                 : {cfg.get('num_epochs')}\")",
   "id": "65e8d58f34a6b23d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import required modules for the model\n",
    "import torch\n",
    "from var import LABELS\n",
    "from models.panns_enhanced import MultiSTFTCNN_WithPANNs\n",
    "from data.download_pnn import download_panns_checkpoint\n",
    "\n",
    "n_classes = len(LABELS)\n",
    "\n",
    "# Download PANNs checkpoint if needed\n",
    "panns_path = download_panns_checkpoint()\n",
    "\n",
    "# Create the enhanced model with PANNs\n",
    "model = MultiSTFTCNN_WithPANNs(\n",
    "    n_classes=n_classes,\n",
    "    pretrained_path=panns_path,\n",
    "    freeze_backbone=False\n",
    ")\n",
    "\n",
    "print(\"PANNs-Enhanced Architecture:\")\n",
    "print(model)\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "\n",
    "    class ModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, x1, x2, x3):\n",
    "            return self.model([x1, x2, x3])\n",
    "\n",
    "    wrapped_model = ModelWrapper(model)\n",
    "\n",
    "    dummy_inputs = [\n",
    "        torch.zeros(1, 1, 128, 100),\n",
    "        torch.zeros(1, 1, 128, 100),\n",
    "        torch.zeros(1, 1, 128, 100)\n",
    "    ]\n",
    "\n",
    "    print(\"\\nModel Summary:\")\n",
    "    summary(wrapped_model, input_data=dummy_inputs, verbose=1)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nInstall torchinfo for detailed model summary: pip install torchinfo\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not generate model summary: {e}\")\n",
    "    print(\"This is normal - the model architecture is still correctly defined.\")\n",
    "\n",
    "print(\"\\nüîß Manual Model Summary:\")\n",
    "print(\"   üìä Input: 3 spectrograms (optimized window sizes for each frequency band)\")\n",
    "print(\"   üß† Architecture: 3 PANNs feature extractors + fusion layer + classifier\")\n",
    "print(f\"   üì§ Output: {n_classes} instrument classes\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"   üìà Total Parameters: {total_params:,}\")\n",
    "print(f\"   üéØ Trainable Parameters: {trainable_params:,}\")\n",
    "print(\"   üöÄ Using PANNs pretrained weights for enhanced feature extraction\")\n",
    "\n",
    "print(\"\\nüß™ Testing PANNs-enhanced model with dummy data...\")\n",
    "try:\n",
    "    dummy_input = [torch.zeros(2, 1, 20, 30) for _ in range(3)]\n",
    "    output = model(dummy_input)\n",
    "    print(\"   ‚úÖ Model test successful!\")\n",
    "    print(f\"   üìä Input: 3 tensors of shape {dummy_input[0].shape}\")\n",
    "    print(f\"   üì§ Output shape: {output.shape}\")\n",
    "    print(f\"   üéØ Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "    print(\"   ‚ÑπÔ∏è The model outputs raw logits that will be processed with softmax for classification\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Model test failed: {e}\")\n"
   ],
   "id": "bb56e96fc293e802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# üöÄ  TRAINING LAUNCH  ‚Äì  with split summary & extra sanity checks\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "print(\"üöÄ Starting training‚Ä¶\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1) Echo key configuration values\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"üìÅ Configuration:\")\n",
    "print(f\"   max_samples            : {cfg.get('max_samples', 'all')}\")\n",
    "print(f\"   train_dir              : {cfg.get('train_dir', 'not set')}\")\n",
    "print(f\"   val_dir                : {cfg.get('val_dir', 'not set')}\")\n",
    "print(f\"   test_dir               : {cfg.get('test_dir', 'not set')}\")\n",
    "print(f\"   batch_size             : {cfg.get('batch_size', 'not set')}\")\n",
    "print(\n",
    "    f\"   limit_val_batches      : {cfg.get('limit_val_batches', 1.0)} \"\n",
    "    f\"({'percentage' if cfg.get('limit_val_batches', 1.0) <= 1 else 'batches'})\"\n",
    ")\n",
    "print(f\"   num_sanity_val_steps   : {cfg.get('num_sanity_val_steps', 'default')}\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2) Split-directory summary helper\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def summarize_split(path: pathlib.Path):\n",
    "    if not path.exists():\n",
    "        return 0, \"missing dir\"\n",
    "    counts = Counter(\n",
    "        (p.name.split(\"_\")[0] for p in path.iterdir() if p.is_dir())\n",
    "    )  # original / mixed / irmasTest / ‚Ä¶\n",
    "    total = sum(counts.values())\n",
    "    detail = \", \".join(f\"{k}:{v}\" for k, v in counts.items()) if counts else \"empty\"\n",
    "    return total, detail\n",
    "\n",
    "\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    p = pathlib.Path(cfg.get(f\"{split}_dir\", \"\"))\n",
    "    total, detail = summarize_split(p)\n",
    "    status = \"‚úÖ\" if total else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {split.upper():5} ‚Üí {total:4} sample folders ({detail})\")\n",
    "\n",
    "print()  # spacer\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3) Guard against an empty validation set\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "val_total, _ = summarize_split(pathlib.Path(cfg[\"val_dir\"]))\n",
    "if val_total == 0:\n",
    "    raise RuntimeError(\n",
    "        \"Validation split is empty! \"\n",
    "        \"Increase `original_data_percentage` or check preprocessing.\"\n",
    "    )\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4) Training routine with robust import / fallback\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    from training.panns_train import main as train_main  # prefer PANNs variant\n",
    "    print(\"‚úÖ Imported training.panns_train.main\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  training.panns_train not available ‚Äì falling back to training.train\")\n",
    "    try:\n",
    "        from training.train import main as train_main\n",
    "    except ImportError as e:\n",
    "        sys.exit(f\"‚ùå Could not import training module: {e}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Launch training\n",
    "# --------------------------------------------------------------------\n",
    "try:\n",
    "    train_main(cfg)  # your Lightning entry point\n",
    "    print(\"üèÅ Training completed successfully!\")\n",
    "\n",
    "    # OPTIONAL: automatic test-set evaluation\n",
    "    # Comment out if your train_main already runs tests internally\n",
    "    if hasattr(train_main, \"__code__\") and \"run_test\" in train_main.__code__.co_varnames:\n",
    "        print(\"\\nüî¨ Running test-set evaluation‚Ä¶\")\n",
    "        train_main(cfg, run_test=True)\n",
    "        print(\"‚úÖ Test evaluation completed!\")\n",
    "\n",
    "except Exception as err:\n",
    "    print(f\"‚ùå Training error: {err}\")\n",
    "    traceback.print_exc(limit=2)\n",
    "    # Additional debugging for empty dataset issues\n",
    "    if \"num_samples=0\" in str(err).lower():\n",
    "        td = pathlib.Path(cfg[\"train_dir\"])\n",
    "        if td.exists():\n",
    "            items = list(td.iterdir())\n",
    "            print(f\"üîç Train dir contains {len(items)} items. First few:\")\n",
    "            for itm in items[:5]:\n",
    "                print(\"   ‚Ä¢\", itm.name)\n",
    "        else:\n",
    "            print(f\"‚ùå Train dir {td} does not exist!\")\n"
   ],
   "id": "730ba260b71c40b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Inference and visualization using the test set\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ],
   "id": "a54701eb69295b86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# üöÄ  Inference with single-label classification & accuracy calculation\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "import glob, re, yaml, sys, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Settings\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "LIGHTNING_LOGS_DIR = \"lightning_logs\"\n",
    "VISUALISE_FIRST     = True     # show waveform + spectrogram for first file?\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1)  Locate the best checkpoint.\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def find_best_checkpoint(log_dir=\"lightning_logs\"):\n",
    "    \"\"\"\n",
    "    Return the .ckpt with the highest val_mAP.\n",
    "    Works even for names like '‚Ä¶val_mAP=0.000.ckpt'.\n",
    "    \"\"\"\n",
    "    ckpts = glob.glob(f\"{log_dir}/*/checkpoints/*.ckpt\")\n",
    "    if not ckpts:\n",
    "        print(f\"‚ùå No checkpoints found under {log_dir}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üîç Found {len(ckpts)} candidate checkpoint(s)\")\n",
    "\n",
    "    best, best_map, best_epoch = None, -1.0, -1\n",
    "    pat_epoch = re.compile(r\"epoch=(\\d+)\")\n",
    "    # capture the number but STOP before a trailing dot (if any)\n",
    "    pat_map   = re.compile(r\"val_Accuracy=([0-9]+(?:\\.[0-9]*)?)\")\n",
    "\n",
    "    for c in ckpts:\n",
    "        m_ep  = pat_epoch.search(c)\n",
    "        m_map = pat_map.search(c)\n",
    "        if not (m_ep and m_map):\n",
    "            continue\n",
    "        epoch = int(m_ep.group(1))\n",
    "        vmap  = float(m_map.group(1).rstrip(\".\"))   # <- strips stray dots\n",
    "        if vmap > best_map or (vmap == best_map and epoch > best_epoch):\n",
    "            best, best_map, best_epoch = c, vmap, epoch\n",
    "\n",
    "    if best:\n",
    "        print(f\"‚úÖ Best checkpoint ‚Üí {best} | epoch {best_epoch} | val_Accuracy {best_map:.4f}\")\n",
    "    return best or ckpts[0]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2)  Load the model.\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ckpt_path = find_best_checkpoint()\n",
    "if ckpt_path is None:\n",
    "    sys.exit(\"üõë  Aborting ‚Äì no checkpoints found.\")\n",
    "\n",
    "# Re-load cfg (same YAML you used for training)\n",
    "with open(yaml_path) as fh:\n",
    "    cfg = yaml.safe_load(fh)\n",
    "\n",
    "try:\n",
    "    from inference.predict import predict_batch_with_accuracy\n",
    "    from utils.model_loader import load_model_from_checkpoint\n",
    "    from var import LABELS\n",
    "except ImportError as e:\n",
    "    sys.exit(f\"‚ùå Required inference modules missing: {e}\")\n",
    "\n",
    "model = load_model_from_checkpoint(ckpt_path, len(LABELS))\n",
    "model.eval()\n",
    "print(\"üéâ Model restored and set to eval mode\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3)  Gather test WAV files (preferring cfg['test_dir']).\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MAX_TEST_SAMPLES = cfg.get(\"max_test_samples\", 5)\n",
    "\n",
    "wav_candidates = []\n",
    "tdir = cfg.get(\"test_dir\")\n",
    "if tdir and Path(tdir).exists():\n",
    "    wav_candidates = list(Path(tdir).rglob(\"*.wav\"))\n",
    "if not wav_candidates and \"irmas_root\" in globals() and irmas_root:\n",
    "    wav_candidates = list(Path(irmas_root).rglob(\"*.wav\"))\n",
    "\n",
    "# ‚üµ add filter here\n",
    "wav_candidates = [\n",
    "    p for p in wav_candidates\n",
    "    if \"IRMAS-TestingData-Part\" in str(p)   # keep only true test WAVs\n",
    "]\n",
    "\n",
    "if not wav_candidates:\n",
    "    sys.exit(\"‚ùå No test WAV files found!\")\n",
    "\n",
    "# NEW ‚ñ∏ honour max_test_samples\n",
    "if MAX_TEST_SAMPLES is not None and len(wav_candidates) > MAX_TEST_SAMPLES:\n",
    "    wav_candidates = wav_candidates[:MAX_TEST_SAMPLES]\n",
    "    print(f\"‚öñÔ∏è  Using only the first {MAX_TEST_SAMPLES} test files (quick run)\")\n",
    "\n",
    "test_files = wav_candidates\n",
    "print(f\"üóÇÔ∏è  Running inference on {len(test_files)} file(s)\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4)  Run predictions with accuracy calculation on the fly.\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"‚ÑπÔ∏è Using single-label classification approach\")\n",
    "print(\"   The model uses softmax to pick the most dominant instrument\")\n",
    "print(\"   Accuracy is calculated on-the-fly during inference\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Run batch inference with accuracy calculation\n",
    "batch_result = predict_batch_with_accuracy(\n",
    "    model,\n",
    "    [str(f) for f in test_files],\n",
    "    cfg,\n",
    "    show_details=True\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 5)  Show final summary\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total files processed: {len(test_files)}\")\n",
    "print(f\"Files with ground truth: {batch_result['total_files']}\")\n",
    "print(f\"Correct predictions: {batch_result['total_correct']}\")\n",
    "print(f\"Overall accuracy: {batch_result['accuracy']:.1%}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 6)  Optional: Visualize first file\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if VISUALISE_FIRST and test_files:\n",
    "    try:\n",
    "        first_file = test_files[0]\n",
    "        y, sr = librosa.load(str(first_file), sr=cfg[\"sample_rate\"])\n",
    "\n",
    "        # Waveform\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.plot(np.linspace(0, len(y)/sr, len(y)), y)\n",
    "        plt.title(f\"Waveform ‚Äì {first_file.name}\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Spectrogram\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        librosa.display.specshow(D, sr=sr, y_axis=\"log\", x_axis=\"time\")\n",
    "        plt.colorbar(format=\"%+2.0f dB\")\n",
    "        plt.title(f\"Spectrogram ‚Äì {first_file.name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Visualisation failed: {e}\")\n",
    "\n",
    "print(\"‚úÖ Inference with accuracy calculation completed!\")"
   ],
   "id": "f7809bccc1e49b2e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}