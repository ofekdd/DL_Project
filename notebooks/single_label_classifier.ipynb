{
 "cells": [
  {
   "cell_type": "code",
   "id": "fae51c5c100772c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.171955Z",
     "start_time": "2025-06-07T07:18:27.160441Z"
    }
   },
   "source": [
    "\n",
    "import sys\n",
    "import warnings, tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmWarning)\n",
    "sys.modules['tqdm.notebook'] = tqdm\n",
    "sys.modules['tqdm.autonotebook'] = tqdm\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "\n",
    "    # Always start fresh and clone the specific branch\n",
    "    print(\"üóëÔ∏è Cleaning up any existing project...\")\n",
    "    %cd / content\n",
    "    !rm -rf DL_Project\n",
    "\n",
    "    #TODO: Fix the branch according to the latest changes\n",
    "    print(\"üì• Cloning specific branch 'master'...\")\n",
    "    !git clone -b master https://github.com/ofekdd/DL_Project.git\n",
    "    %cd DL_Project\n",
    "\n",
    "    # Verify we're on the correct branch\n",
    "    print(\"üîç Verifying branch...\")\n",
    "    !git branch\n",
    "    !git log --oneline -n 3\n",
    "\n",
    "    # Install dependencies\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    print(\"‚úÖ Setup complete with branch 'master'!\")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.238878Z",
     "start_time": "2025-06-07T07:18:27.232410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the current working directory and ensure it is the project root\n",
    "from pathlib import Path\n",
    "print(\"CWD :\", Path.cwd())                    # where the kernel is running\n",
    "print(\"Exists?\", Path('configs').is_dir())    # should be True if CWD is project root\n"
   ],
   "id": "3d6ed40822d8c61b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD : /home/odahan/Technion/Semester_8/Deep_Learning/Project/notebooks\n",
      "Exists? False\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.301113Z",
     "start_time": "2025-06-07T07:18:27.291167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Define the path to the YAML configuration file\n",
    "workspace = '/home/odahan/Technion/Semester_8/Deep_Learning/Project'\n",
    "yaml_path = 'configs/panns_enhanced.yaml' if IN_COLAB else f'{workspace}/configs/panns_enhanced.yaml'\n",
    "print(yaml_path)\n",
    "# Open and load the YAML file\n",
    "with open(yaml_path, 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "\n",
    "print(\"PANNs-enhanced configuration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "id": "4be9f8e58789698a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/odahan/Technion/Semester_8/Deep_Learning/Project/configs/multi_stft_cnn.yaml\n",
      "9cnn configuration:\n",
      "  model_name: multi_stft_cnn\n",
      "  sample_rate: 22050\n",
      "  n_mels: 64\n",
      "  hop_length: 512\n",
      "  batch_size: 8\n",
      "  num_epochs: 50\n",
      "  learning_rate: 2e-4\n",
      "  num_workers: 4\n",
      "  n_branches: 9\n",
      "  branch_output_dim: 128\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download the IRMAS dataset (training + testing parts) if needed\n",
    "from data.download_irmas import main as download_irmas_main\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Environment-aware cache path\n",
    "if IN_COLAB:\n",
    "    DATA_CACHE = \"/content/drive/MyDrive/datasets/IRMAS\"\n",
    "else:\n",
    "    # Check for dataset in home directory\n",
    "    home_dataset_path = pathlib.Path.home() / \"datasets\" / \"irmas\"\n",
    "    DATA_CACHE = str(home_dataset_path if home_dataset_path.exists() else \"data/raw\")\n",
    "\n",
    "# Create the dataset directory if it doesn't exist\n",
    "os.makedirs(DATA_CACHE, exist_ok=True)\n",
    "\n",
    "# Download the IRMAS datasets if not already downloaded\n",
    "irmas_zips = [\n",
    "    \"IRMAS.TrainingData.zip\",\n",
    "    \"IRMAS-TestingData-Part1.zip\",\n",
    "    \"IRMAS-TestingData-Part2.zip\",\n",
    "    \"IRMAS-TestingData-Part3.zip\"\n",
    "]\n",
    "download_required = any(not (pathlib.Path(DATA_CACHE) / zip_name).exists() for zip_name in irmas_zips)\n",
    "\n",
    "if download_required:\n",
    "    print(f\"üì• Downloading IRMAS datasets into: {DATA_CACHE}\")\n",
    "    download_irmas_main(pathlib.Path(DATA_CACHE))\n",
    "else:\n",
    "    print(\"‚úÖ All IRMAS zip files already present. Skipping download.\")\n",
    "\n",
    "# Confirm dataset availability\n",
    "expected_dirs = [\n",
    "    \"IRMAS-TrainingData\",\n",
    "    \"IRMAS-TestingData-Part1\",\n",
    "    \"IRMAS-TestingData-Part2\",\n",
    "    \"IRMAS-TestingData-Part3\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Verifying dataset extraction:\")\n",
    "missing = []\n",
    "for dir_name in expected_dirs:\n",
    "    expected_path = pathlib.Path(DATA_CACHE) / dir_name\n",
    "    if expected_path.exists():\n",
    "        print(f\"‚úÖ {dir_name} found at {expected_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dir_name} missing at {expected_path}\")\n",
    "        missing.append(dir_name)\n",
    "\n",
    "if missing:\n",
    "    print(\"\\n‚ö†Ô∏è Some dataset parts are missing. You may want to delete corrupted zip files and re-run the cell.\")\n",
    "else:\n",
    "    print(\"üéâ All expected IRMAS dataset parts are ready.\")\n",
    "\n",
    "# Set IRMAS root to base path (used in later cells to access all parts)\n",
    "irmas_root = pathlib.Path(DATA_CACHE)\n",
    "print(f\"\\nüìÇ IRMAS base path set to: {irmas_root}\")"
   ],
   "id": "6e04ee7069ac34ec",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fix NumPy compatibility issue\n",
    "import sys\n",
    "\n",
    "print(\"üîß Fixing NumPy compatibility...\")\n",
    "\n",
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Current NumPy version: {np.__version__}\")\n",
    "\n",
    "# If NumPy 2.0+, we need to downgrade or use a workaround\n",
    "if int(np.__version__.split('.')[0]) >= 2:\n",
    "    print(\"‚ö†Ô∏è  NumPy 2.0+ detected. Installing compatible version...\")\n",
    "    !pip install \"numpy<2.0\" --quiet\n",
    "\n",
    "    # Restart the kernel to load the new NumPy version\n",
    "    print(\"üîÑ Restarting kernel to load compatible NumPy...\")\n",
    "    import os\n",
    "\n",
    "    os.kill(os.getpid(), 9)  # This will restart the kernel in Colab\n",
    "else:\n",
    "    print(\"‚úÖ NumPy version is compatible\")"
   ],
   "id": "d9db882fdcb43259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from data.download_irmas import load_irmas_audio_dataset, load_irmas_testing_dataset\n",
    "import pathlib\n",
    "\n",
    "if irmas_root and irmas_root.exists():\n",
    "\n",
    "    print(\"üìÅ Dataset creation settings from config:\")\n",
    "    print(f\"   max_original_samples: {cfg.get('max_original_samples', 50)}\")\n",
    "    print(f\"   num_mixtures: {cfg.get('num_mixtures', 100)}\")\n",
    "    print(f\"   min_instruments: {cfg.get('min_instruments', 1)}\")\n",
    "    print(f\"   max_instruments: {cfg.get('max_instruments', 2)}\")\n",
    "\n",
    "    # Normalize root\n",
    "    base_root = irmas_root.parent if irmas_root.name == \"IRMAS-TrainingData\" else irmas_root\n",
    "\n",
    "    # Define separate paths\n",
    "    training_path = base_root / \"IRMAS-TrainingData\"\n",
    "    testing_paths = [base_root / f\"IRMAS-TestingData-Part{i}\" for i in range(1, 4)]\n",
    "\n",
    "    print(f\"üìÇ IRMAS paths:\")\n",
    "    print(f\"   ‚îú‚îÄ Training: {training_path}\")\n",
    "    for tp in testing_paths:\n",
    "        print(f\"   ‚îî‚îÄ Test Part: {tp}\")\n",
    "\n",
    "    # Load training data (single-label)\n",
    "    original_dataset = load_irmas_audio_dataset(base_root, cfg,\n",
    "                                            max_samples=cfg.get(\"max_original_samples\"))\n",
    "\n",
    "    # Load test data (multi-label)\n",
    "    test_datasets = []\n",
    "    for _ in testing_paths:        # paths 1-3 are *ignored* here\n",
    "        test_datasets.extend(load_irmas_testing_dataset(base_root, cfg))\n",
    "\n",
    "    # Merge datasets if needed\n",
    "    total_loaded = len(original_dataset) + len(test_datasets)\n",
    "    print(f\"\\nüìä Final dataset summary:\")\n",
    "    print(f\"   ‚úÖ Training samples loaded: {len(original_dataset)}\")\n",
    "    print(f\"   ‚úÖ Testing samples loaded: {len(test_datasets)}\")\n",
    "    print(f\"   ‚úÖ Total samples loaded:   {total_loaded}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå IRMAS root not found or invalid. Please run the download step first.\")"
   ],
   "id": "4277635d9e525b8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading IRMAS dataset to data/raw...\n",
      "Archive already exists, skipping download\n",
      "Verifying checksum ...\n",
      "Extracting ...\n",
      "Done. Data at data/raw\n",
      "IRMAS dataset found at: data/raw/IRMAS-TrainingData\n",
      "\n",
      "To preprocess the data, you can run:\n",
      "python data/preprocess.py --in_dir data/raw/IRMAS-TrainingData --out_dir data/processed\n",
      "\n",
      "Or execute this command in the next cell:\n",
      "!python data/preprocess.py --in_dir data/raw/IRMAS-TrainingData --out_dir data/processed\n"
     ]
    }
   ],
   "execution_count": 29,
   "source": [
    "if irmas_root:\n",
    "    print(f\"IRMAS dataset found at: {irmas_root}\")\n",
    "    PROCESSED_DIR = \"/content/IRMAS_features\" if IN_COLAB else \"data/processed\"\n",
    "\n",
    "    # Use config value for original data percentage\n",
    "    original_data_percentage = cfg.get('original_data_percentage', 0.1)\n",
    "    print(f\"Using {original_data_percentage*100}% of original IRMAS data (from config)\")\n",
    "\n",
    "    from data.preprocess import preprocess_data\n",
    "\n",
    "    preprocess_data(\n",
    "        irmas_root=irmas_root,\n",
    "        out_dir=PROCESSED_DIR,\n",
    "        cfg=cfg,\n",
    "        original_data_percentage=original_data_percentage\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Preprocessing complete with mixed labels. Features saved to {PROCESSED_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not locate IRMAS dataset after download. Check paths and try again.\")"
   ],
   "id": "e96b51ebbb46946c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# üì¶  Configure paths & echo training settings\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"üîß Configuring data paths and training settings...\")\n",
    "\n",
    "# 1) show YAML-driven parameters\n",
    "base_max_samples = cfg.get(\"max_samples\", None)\n",
    "print(\"üìÅ Base configuration from YAML:\")\n",
    "print(f\"   max_samples            : {base_max_samples}\")\n",
    "print(f\"   max_original_samples   : {cfg.get('max_original_samples', 50)}\")\n",
    "print(f\"   num_mixtures           : {cfg.get('num_mixtures', 100)}\")\n",
    "print(f\"   min_instruments        : {cfg.get('min_instruments', 1)}\")\n",
    "print(f\"   max_instruments        : {cfg.get('max_instruments', 2)}\")\n",
    "print(f\"   original_data_percentage : {cfg.get('original_data_percentage', 0.1)}\")\n",
    "\n",
    "# 2) notebook override notice\n",
    "if base_max_samples != cfg.get(\"max_samples\"):\n",
    "    print(f\"‚ö†Ô∏è  Notebook override: max_samples changed to {cfg.get('max_samples')}\")\n",
    "\n",
    "# 3) define processed-data locations\n",
    "PROCESSED_DIR = \"/content/IRMAS_features\" if IN_COLAB else \"data/processed\"\n",
    "cfg.update(\n",
    "    {\n",
    "        \"data_dir\": PROCESSED_DIR,\n",
    "        \"train_dir\": f\"{PROCESSED_DIR}/train\",\n",
    "        \"val_dir\": f\"{PROCESSED_DIR}/val\",\n",
    "        \"test_dir\": f\"{PROCESSED_DIR}/test\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nüìÇ Data directories:\")\n",
    "print(f\"   Processed data : {cfg['data_dir']}\")\n",
    "print(f\"   Training       : {cfg['train_dir']}\")\n",
    "print(f\"   Validation     : {cfg['val_dir']}\")\n",
    "print(f\"   Test           : {cfg['test_dir']}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# üîç  Verify directory existence & detailed sample counts\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import pathlib\n",
    "\n",
    "def count_sample_folders(split_path: pathlib.Path):\n",
    "    counts = dict(original=0, mixed=0, irmasTest=0, other=0)\n",
    "    if not split_path.exists():\n",
    "        return counts  # all zeros\n",
    "    for d in split_path.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        n = d.name\n",
    "        if n.startswith(\"original_\"):\n",
    "            counts[\"original\"] += 1\n",
    "        elif n.startswith(\"mixed_\"):\n",
    "            counts[\"mixed\"] += 1\n",
    "        elif n.startswith(\"irmasTest_\"):\n",
    "            counts[\"irmasTest\"] += 1\n",
    "        else:\n",
    "            counts[\"other\"] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "print(\"\\nüîç Verifying data directories & split composition:\")\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    path = pathlib.Path(cfg[f\"{split}_dir\"])\n",
    "    counts = count_sample_folders(path)\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    status = \"‚úÖ\" if total else \"‚ùå\"\n",
    "    print(f\"\\n{status} {split.upper()} ({path}): {total} total sample folders\")\n",
    "    print(f\"      ‚Ä¢ original_:  {counts['original']}\")\n",
    "    print(f\"      ‚Ä¢ mixed_:     {counts['mixed']}\")\n",
    "    print(f\"      ‚Ä¢ irmasTest_: {counts['irmasTest']}\")\n",
    "    if counts[\"other\"]:\n",
    "        print(f\"      ‚Ä¢ other:      {counts['other']}  ‚Üê check if expected!\")\n",
    "\n",
    "    # sanity warnings\n",
    "    if split in (\"train\", \"val\") and counts[\"irmasTest\"]:\n",
    "        print(\"      ‚ö†Ô∏è  Unexpected irmasTest_ folders in this split!\")\n",
    "    if split == \"test\" and (counts[\"original\"] or counts[\"mixed\"]):\n",
    "        print(\"      ‚ö†Ô∏è  Test split should contain ONLY irmasTest_ folders.\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# üéõÔ∏è  Final training configuration summary\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n‚úÖ Final training configuration:\")\n",
    "print(f\"   Training samples limit : {cfg.get('max_samples', 'unlimited')}\")\n",
    "print(f\"   Batch size             : {cfg.get('batch_size')}\")\n",
    "print(\n",
    "    f\"   Validation limit       : {cfg.get('limit_val_batches', 1.0)} \"\n",
    "    f\"({'percentage' if cfg.get('limit_val_batches', 1.0) <= 1 else 'batches'})\"\n",
    ")\n",
    "print(f\"   Learning rate          : {cfg.get('learning_rate')}\")\n",
    "print(f\"   Epochs                 : {cfg.get('num_epochs')}\")\n"
   ],
   "id": "65e8d58f34a6b23d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import required modules for the model\n",
    "import torch\n",
    "from var import LABELS\n",
    "from models.panns_enhanced import MultiSTFTCNN_WithPANNs\n",
    "from data.download_pnn import download_panns_checkpoint\n",
    "\n",
    "n_classes = len(LABELS)\n",
    "\n",
    "# Download PANNs checkpoint if needed\n",
    "panns_path = download_panns_checkpoint()\n",
    "\n",
    "# Create the enhanced model with PANNs\n",
    "model = MultiSTFTCNN_WithPANNs(\n",
    "    n_classes=n_classes,\n",
    "    pretrained_path=panns_path,\n",
    "    freeze_backbone=False\n",
    ")\n",
    "\n",
    "print(\"PANNs-Enhanced Architecture:\")\n",
    "print(model)\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "\n",
    "    class ModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, x1, x2, x3):\n",
    "            return self.model([x1, x2, x3])\n",
    "\n",
    "    wrapped_model = ModelWrapper(model)\n",
    "\n",
    "    dummy_inputs = [\n",
    "        torch.zeros(1, 1, 128, 100),\n",
    "        torch.zeros(1, 1, 128, 100),\n",
    "        torch.zeros(1, 1, 128, 100)\n",
    "    ]\n",
    "\n",
    "    print(\"\\nModel Summary:\")\n",
    "    summary(wrapped_model, input_data=dummy_inputs, verbose=1)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nInstall torchinfo for detailed model summary: pip install torchinfo\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not generate model summary: {e}\")\n",
    "    print(\"This is normal - the model architecture is still correctly defined.\")\n",
    "\n",
    "print(\"\\nüîß Manual Model Summary:\")\n",
    "print(\"   üìä Input: 3 spectrograms (optimized window sizes for each frequency band)\")\n",
    "print(\"   üß† Architecture: 3 PANNs feature extractors + fusion layer + classifier\")\n",
    "print(f\"   üì§ Output: {n_classes} instrument classes\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"   üìà Total Parameters: {total_params:,}\")\n",
    "print(f\"   üéØ Trainable Parameters: {trainable_params:,}\")\n",
    "print(\"   üöÄ Using PANNs pretrained weights for enhanced feature extraction\")\n",
    "\n",
    "print(\"\\nüß™ Testing PANNs-enhanced model with dummy data...\")\n",
    "try:\n",
    "    dummy_input = [torch.zeros(2, 1, 20, 30) for _ in range(3)]\n",
    "    output = model(dummy_input)\n",
    "    print(\"   ‚úÖ Model test successful!\")\n",
    "    print(f\"   üìä Input: 3 tensors of shape {dummy_input[0].shape}\")\n",
    "    print(f\"   üì§ Output shape: {output.shape}\")\n",
    "    print(f\"   üéØ Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "    print(\"   ‚ÑπÔ∏è The model outputs raw logits that will be processed with softmax for classification\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Model test failed: {e}\")\n"
   ],
   "id": "bb56e96fc293e802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# üöÄ  TRAINING LAUNCH  ‚Äì  with split summary & extra sanity checks\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "print(\"üöÄ Starting training‚Ä¶\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1) Echo key configuration values\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"üìÅ Configuration:\")\n",
    "print(f\"   max_samples            : {cfg.get('max_samples', 'all')}\")\n",
    "print(f\"   train_dir              : {cfg.get('train_dir', 'not set')}\")\n",
    "print(f\"   val_dir                : {cfg.get('val_dir', 'not set')}\")\n",
    "print(f\"   test_dir               : {cfg.get('test_dir', 'not set')}\")\n",
    "print(f\"   batch_size             : {cfg.get('batch_size', 'not set')}\")\n",
    "print(\n",
    "    f\"   limit_val_batches      : {cfg.get('limit_val_batches', 1.0)} \"\n",
    "    f\"({'percentage' if cfg.get('limit_val_batches', 1.0) <= 1 else 'batches'})\"\n",
    ")\n",
    "print(f\"   num_sanity_val_steps   : {cfg.get('num_sanity_val_steps', 'default')}\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2) Split-directory summary helper\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def summarize_split(path: pathlib.Path):\n",
    "    if not path.exists():\n",
    "        return 0, \"missing dir\"\n",
    "    counts = Counter(\n",
    "        (p.name.split(\"_\")[0] for p in path.iterdir() if p.is_dir())\n",
    "    )  # original / mixed / irmasTest / ‚Ä¶\n",
    "    total = sum(counts.values())\n",
    "    detail = \", \".join(f\"{k}:{v}\" for k, v in counts.items()) if counts else \"empty\"\n",
    "    return total, detail\n",
    "\n",
    "\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    p = pathlib.Path(cfg.get(f\"{split}_dir\", \"\"))\n",
    "    total, detail = summarize_split(p)\n",
    "    status = \"‚úÖ\" if total else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {split.upper():5} ‚Üí {total:4} sample folders ({detail})\")\n",
    "\n",
    "print()  # spacer\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3) Guard against an empty validation set\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "val_total, _ = summarize_split(pathlib.Path(cfg[\"val_dir\"]))\n",
    "if val_total == 0:\n",
    "    raise RuntimeError(\n",
    "        \"Validation split is empty! \"\n",
    "        \"Increase `original_data_percentage` or check preprocessing.\"\n",
    "    )\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4) Training routine with robust import / fallback\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    from training.panns_train import main as train_main  # prefer PANNs variant\n",
    "    print(\"‚úÖ Imported training.panns_train.main\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  training.panns_train not available ‚Äì falling back to training.train\")\n",
    "    try:\n",
    "        from training.train import main as train_main\n",
    "    except ImportError as e:\n",
    "        sys.exit(f\"‚ùå Could not import training module: {e}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Launch training\n",
    "# --------------------------------------------------------------------\n",
    "try:\n",
    "    train_main(cfg)  # your Lightning entry point\n",
    "    print(\"üèÅ Training completed successfully!\")\n",
    "\n",
    "    # OPTIONAL: automatic test-set evaluation\n",
    "    # Comment out if your train_main already runs tests internally\n",
    "    if hasattr(train_main, \"__code__\") and \"run_test\" in train_main.__code__.co_varnames:\n",
    "        print(\"\\nüî¨ Running test-set evaluation‚Ä¶\")\n",
    "        train_main(cfg, run_test=True)\n",
    "        print(\"‚úÖ Test evaluation completed!\")\n",
    "\n",
    "except Exception as err:\n",
    "    print(f\"‚ùå Training error: {err}\")\n",
    "    traceback.print_exc(limit=2)\n",
    "    # Additional debugging for empty dataset issues\n",
    "    if \"num_samples=0\" in str(err).lower():\n",
    "        td = pathlib.Path(cfg[\"train_dir\"])\n",
    "        if td.exists():\n",
    "            items = list(td.iterdir())\n",
    "            print(f\"üîç Train dir contains {len(items)} items. First few:\")\n",
    "            for itm in items[:5]:\n",
    "                print(\"   ‚Ä¢\", itm.name)\n",
    "        else:\n",
    "            print(f\"‚ùå Train dir {td} does not exist!\")\n"
   ],
   "id": "730ba260b71c40b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Inference and visualization using the test set\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ],
   "id": "a54701eb69295b86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# üöÄ  Inference with single-label classification\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "import glob, re, yaml, sys, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Settings\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "LIGHTNING_LOGS_DIR = \"lightning_logs\"\n",
    "N_TEST_FILES        = 5        # change if you want more/less demo files\n",
    "VISUALISE_FIRST     = True     # show waveform + spectrogram for first file?\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1)  Locate the best checkpoint.\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def find_best_checkpoint(log_dir=\"lightning_logs\"):\n",
    "    \"\"\"\n",
    "    Return the .ckpt with the highest val_mAP.\n",
    "    Works even for names like '‚Ä¶val_mAP=0.000.ckpt'.\n",
    "    \"\"\"\n",
    "    ckpts = glob.glob(f\"{log_dir}/*/checkpoints/*.ckpt\")\n",
    "    if not ckpts:\n",
    "        print(f\"‚ùå No checkpoints found under {log_dir}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üîç Found {len(ckpts)} candidate checkpoint(s)\")\n",
    "\n",
    "    best, best_map, best_epoch = None, -1.0, -1\n",
    "    pat_epoch = re.compile(r\"epoch=(\\d+)\")\n",
    "    # capture the number but STOP before a trailing dot (if any)\n",
    "    pat_map   = re.compile(r\"val_Accuracy=([0-9]+(?:\\.[0-9]*)?)\")\n",
    "\n",
    "    for c in ckpts:\n",
    "        m_ep  = pat_epoch.search(c)\n",
    "        m_map = pat_map.search(c)\n",
    "        if not (m_ep and m_map):\n",
    "            continue\n",
    "        epoch = int(m_ep.group(1))\n",
    "        vmap  = float(m_map.group(1).rstrip(\".\"))   # <- strips stray dots\n",
    "        if vmap > best_map or (vmap == best_map and epoch > best_epoch):\n",
    "            best, best_map, best_epoch = c, vmap, epoch\n",
    "\n",
    "    if best:\n",
    "        print(f\"‚úÖ Best checkpoint ‚Üí {best} | epoch {best_epoch} | val_Accuracy {best_map:.4f}\")\n",
    "    return best or ckpts[0]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2)  Note on single-label classification\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"‚ÑπÔ∏è Using single-label classification approach\")\n",
    "print(\"   The model now uses softmax to pick the most dominant instrument\")\n",
    "print(\"   No thresholds are needed in this approach\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3)  Load the model.\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ckpt_path = find_best_checkpoint()\n",
    "if ckpt_path is None:\n",
    "    sys.exit(\"üõë  Aborting ‚Äì no checkpoints found.\")\n",
    "\n",
    "# Re-load cfg (same YAML you used for training)\n",
    "with open(yaml_path) as fh:\n",
    "    cfg = yaml.safe_load(fh)\n",
    "\n",
    "try:\n",
    "    from inference.predict import predict_with_ground_truth\n",
    "    from utils.model_loader import load_model_from_checkpoint\n",
    "    from var import LABELS\n",
    "except ImportError as e:\n",
    "    sys.exit(f\"‚ùå Required inference modules missing: {e}\")\n",
    "\n",
    "model = load_model_from_checkpoint(ckpt_path, len(LABELS))\n",
    "model.eval()\n",
    "print(\"üéâ Model restored and set to eval mode\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4)  Gather test WAV files (preferring cfg['test_dir']).\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MAX_TEST_SAMPLES = cfg.get(\"max_test_samples\", 5)\n",
    "\n",
    "wav_candidates = []\n",
    "tdir = cfg.get(\"test_dir\")\n",
    "if tdir and Path(tdir).exists():\n",
    "    wav_candidates = list(Path(tdir).rglob(\"*.wav\"))\n",
    "if not wav_candidates and \"irmas_root\" in globals() and irmas_root:\n",
    "    wav_candidates = list(Path(irmas_root).rglob(\"*.wav\"))\n",
    "\n",
    "# ‚üµ add filter here\n",
    "wav_candidates = [\n",
    "    p for p in wav_candidates\n",
    "    if \"IRMAS-TestingData-Part\" in str(p)   # keep only true test WAVs\n",
    "]\n",
    "\n",
    "if not wav_candidates:\n",
    "    sys.exit(\"‚ùå No test WAV files found!\")\n",
    "\n",
    "# NEW ‚ñ∏ honour max_test_samples\n",
    "if MAX_TEST_SAMPLES is not None and len(wav_candidates) > MAX_TEST_SAMPLES:\n",
    "    wav_candidates = wav_candidates[:MAX_TEST_SAMPLES]\n",
    "    print(f\"‚öñÔ∏è  Using only the first {MAX_TEST_SAMPLES} test files (quick run)\")\n",
    "\n",
    "test_files = wav_candidates       # (no extra slice needed)\n",
    "print(f\"üóÇÔ∏è  Running inference on {len(test_files)} file(s)\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 5)  Run predictions with single-label classification.\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "for idx, wav in enumerate(test_files, 1):\n",
    "    print(f\"üéµ {idx}/{len(test_files)}  {wav.name}\")\n",
    "\n",
    "    # Run prediction with single-label approach\n",
    "    result = predict_with_ground_truth(\n",
    "        model,\n",
    "        str(wav),\n",
    "        cfg,\n",
    "        show_ground_truth=True,\n",
    "    )\n",
    "\n",
    "    # Ground-truth printout (if available)\n",
    "    gt = result.get(\"ground_truth\")\n",
    "    if gt:\n",
    "        print(\"   üéØ Ground truth :\", \", \".join(gt))\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Ground truth : Not available (filename doesn't match IRMAS pattern)\")\n",
    "\n",
    "    # Show top prediction\n",
    "    print(f\"   üé∫ Top prediction: {result['top_prediction']} ({result['top_score']:.4f})\")\n",
    "\n",
    "    # Show accuracy if available\n",
    "    if \"correct\" in result:\n",
    "        status = \"‚úÖ CORRECT\" if result[\"correct\"] else \"‚ùå INCORRECT\"\n",
    "        print(f\"   üìä Evaluation: {status}\")\n",
    "        if result[\"correct\"]:\n",
    "            print(f\"      (Top prediction matches one of the ground truth labels)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Evaluation not possible (no ground truth available)\")\n",
    "\n",
    "    # Show top 5 predictions\n",
    "    top = sorted(\n",
    "        result[\"predictions\"].items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )[:5]\n",
    "    print(\"   Top-5 scores:\")\n",
    "    for lab, sc in top:\n",
    "        flag = \"üî•\" if lab == result[\"top_prediction\"] else \"üî∏\"\n",
    "        print(f\"      {flag} {lab:<12} {sc:.3f}\")\n",
    "\n",
    "    # Visualise first file only\n",
    "    if idx == 1 and VISUALISE_FIRST:\n",
    "        try:\n",
    "            y, sr = librosa.load(str(wav), sr=cfg[\"sample_rate\"])\n",
    "            # Waveform\n",
    "            plt.figure(figsize=(12, 3))\n",
    "            plt.plot(np.linspace(0, len(y)/sr, len(y)), y)\n",
    "            plt.title(f\"Waveform ‚Äì {wav.name}\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            # Spectrogram\n",
    "            D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            librosa.display.specshow(D, sr=sr, y_axis=\"log\", x_axis=\"time\")\n",
    "            plt.colorbar(format=\"%+2.0f dB\")\n",
    "            plt.title(f\"Spectrogram ‚Äì {wav.name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Visualisation failed: {e}\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"‚úÖ Inference run finished.\")\n"
   ],
   "id": "f7809bccc1e49b2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the single-label classification model on the test set.\n"
   ],
   "id": "51c99b31c0173b4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "VAL_DIR  = \"/content/IRMAS_features/val\"           # spectrogram .npy\n",
    "TEST_DIR = \"/content/drive/MyDrive/datasets/IRMAS/IRMAS-TestingData-Part1\"  # raw WAV+TXT\n",
    "\n",
    "cfg[\"val_dir\"]  = VAL_DIR\n",
    "cfg[\"test_dir\"] = TEST_DIR\n",
    "test_dir        = TEST_DIR\n",
    "\n",
    "print(\"VAL exists:\",  Path(VAL_DIR).is_dir(),  \"n numpy:\", len(glob.glob(f'{VAL_DIR}/**/*.npy',  recursive=True)))\n",
    "print(\"TEST exists:\", Path(TEST_DIR).is_dir(), \"n wav  :\", len(glob.glob(f'{TEST_DIR}/**/*.wav', recursive=True)))\n"
   ],
   "id": "63d92ca555435a1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# üéØ  Model Evaluation  ‚ûú  Quick Inference & Comprehensive Testing\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "import pathlib, glob, re, yaml, os, sys, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from var import LABELS\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Utility: locate best checkpoint (highest val_mAP)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def find_best_checkpoint(lightning_logs_dir=\"lightning_logs\"):\n",
    "    ckpts = glob.glob(f\"{lightning_logs_dir}/*/checkpoints/*.ckpt\")\n",
    "    if not ckpts:\n",
    "        print(f\"‚ùå No checkpoints under {lightning_logs_dir}\")\n",
    "        return None\n",
    "    best, best_map, best_epoch = None, -1, -1\n",
    "    for c in ckpts:\n",
    "        m_epoch = re.search(r\"epoch=(\\d+)\", c)\n",
    "        m_map   = re.search(r\"val_Accuracy=([0-9.]+)\", c)\n",
    "        if m_epoch and m_map:\n",
    "            epoch = int(m_epoch.group(1)); vmap = float(m_map.group(1))\n",
    "            if vmap > best_map or (vmap == best_map and epoch > best_epoch):\n",
    "                best, best_map, best_epoch = c, vmap, epoch\n",
    "    if best:\n",
    "        print(f\"üèÜ Best checkpoint ‚Üí {Path(best).name} (epoch {best_epoch}, Accuracy {best_map:.4f})\")\n",
    "    return best or ckpts[0]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# PART A ¬∑ Model evaluation on validation set\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"üéØ Model Evaluation\")\n",
    "\n",
    "ckpt_path = globals().get(\"ckpt_path\") or find_best_checkpoint()\n",
    "if not ckpt_path:\n",
    "    sys.exit(\"üõë  No checkpoint available ‚Äì run training first!\")\n",
    "\n",
    "try:\n",
    "    from utils.model_loader import load_model_from_checkpoint\n",
    "    from data.dataset import create_dataloaders\n",
    "except ImportError as e:\n",
    "    sys.exit(f\"‚ùå Required modules missing: {e}\")\n",
    "\n",
    "# 1) Load model\n",
    "model = load_model_from_checkpoint(ckpt_path, len(LABELS))\n",
    "model.eval()\n",
    "print(\"üéâ Model loaded and set to eval mode\")\n",
    "\n",
    "# 2) Build val-only DataLoader (train_dir dummy = val_dir)\n",
    "val_dir = cfg.get(\"val_dir\")\n",
    "if not val_dir or not Path(val_dir).exists():\n",
    "    print(\"‚ö†Ô∏è  val_dir missing ‚Äì skipping validation-set metrics\")\n",
    "    val_loader = None\n",
    "else:\n",
    "    _, val_loader = create_dataloaders(\n",
    "        train_dir=val_dir, val_dir=val_dir,\n",
    "        batch_size=cfg.get(\"batch_size\", 32),\n",
    "        num_workers=min(2, os.cpu_count() or 1),\n",
    "        use_multi_stft=True,\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ Model and validation data loaded successfully!\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Full evaluation on the entire test set (optional)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if test_dir and Path(test_dir).exists():\n",
    "    try:\n",
    "        print(\"\\nüìà Running comprehensive evaluation on full test set‚Ä¶\")\n",
    "        from visualization.evaluation import run_comprehensive_evaluation\n",
    "\n",
    "        eval_res = run_comprehensive_evaluation(\n",
    "            checkpoint_path=ckpt_path,\n",
    "            test_dir=test_dir,\n",
    "            config_path=yaml_path,\n",
    "        )\n",
    "        print(\"‚úÖ Comprehensive evaluation finished!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Evaluation error: {e}\")\n",
    "        traceback.print_exc(limit=2)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Test directory missing ‚Äì skipping full evaluation.\")\n"
   ],
   "id": "9800c0a94a1197d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
