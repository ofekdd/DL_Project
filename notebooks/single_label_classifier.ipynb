{
 "cells": [
  {
   "cell_type": "code",
   "id": "fae51c5c100772c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.171955Z",
     "start_time": "2025-06-07T07:18:27.160441Z"
    }
   },
   "source": "import sys\nimport warnings, tqdm\n\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmWarning)\nsys.modules['tqdm.notebook'] = tqdm\nsys.modules['tqdm.autonotebook'] = tqdm\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    import os\n    \n    # Mount Google Drive first\n    print(\"üìÅ Mounting Google Drive...\")\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Verify your dataset folder exists\n    drive_dataset_path = \"/content/drive/MyDrive/datasets/IRMAS\"\n    if os.path.exists(drive_dataset_path):\n        print(f\"‚úÖ Found dataset folder at: {drive_dataset_path}\")\n    else:\n        print(f\"‚ö†Ô∏è  Dataset folder not found at: {drive_dataset_path}\")\n        print(\"Creating the folder structure...\")\n        os.makedirs(drive_dataset_path, exist_ok=True)\n\n    # Always start fresh and clone the specific branch\n    print(\"üóëÔ∏è Cleaning up any existing project...\")\n    %cd /content\n    !rm -rf DL_Project\n\n    #TODO: Fix the branch according to the latest changes\n    print(\"üì• Cloning specific branch 'master'...\")\n    !git clone -b master https://github.com/ofekdd/DL_Project.git\n    %cd DL_Project\n\n    # Verify we're on the correct branch\n    print(\"üîç Verifying branch...\")\n    !git branch\n    !git log --oneline -n 3\n\n    # Install dependencies\n    print(\"üì¶ Installing dependencies...\")\n    !pip install -r requirements.txt\n\n    print(\"‚úÖ Setup complete with branch 'master'!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.238878Z",
     "start_time": "2025-06-07T07:18:27.232410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the current working directory and ensure it is the project root\n",
    "from pathlib import Path\n",
    "print(\"CWD :\", Path.cwd())                    # where the kernel is running\n",
    "print(\"Exists?\", Path('configs').is_dir())    # should be True if CWD is project root\n"
   ],
   "id": "3d6ed40822d8c61b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD : /home/odahan/Technion/Semester_8/Deep_Learning/Project/notebooks\n",
      "Exists? False\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.301113Z",
     "start_time": "2025-06-07T07:18:27.291167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Define the path to the YAML configuration file\n",
    "workspace = '/home/odahan/Technion/Semester_8/Deep_Learning/Project'\n",
    "yaml_path = 'configs/panns_enhanced.yaml' if IN_COLAB else f'{workspace}/configs/panns_enhanced.yaml'\n",
    "print(yaml_path)\n",
    "# Open and load the YAML file\n",
    "with open(yaml_path, 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "\n",
    "print(\"PANNs-enhanced configuration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "id": "4be9f8e58789698a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/odahan/Technion/Semester_8/Deep_Learning/Project/configs/multi_stft_cnn.yaml\n",
      "9cnn configuration:\n",
      "  model_name: multi_stft_cnn\n",
      "  sample_rate: 22050\n",
      "  n_mels: 64\n",
      "  hop_length: 512\n",
      "  batch_size: 8\n",
      "  num_epochs: 50\n",
      "  learning_rate: 2e-4\n",
      "  num_workers: 4\n",
      "  n_branches: 9\n",
      "  branch_output_dim: 128\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Download and setup the IRMAS dataset using Google Drive\nfrom data.download_irmas import main as download_irmas_main\nimport pathlib\nimport os\nimport zipfile\n\n# Set DATA_CACHE to your Google Drive folder\nif IN_COLAB:\n    DATA_CACHE = \"/content/drive/MyDrive/datasets/IRMAS\"\n    print(f\"üìÅ Using Google Drive dataset folder: {DATA_CACHE}\")\nelse:\n    # Check for dataset in home directory (for local development)\n    home_dataset_path = pathlib.Path.home() / \"datasets\" / \"irmas\"\n    DATA_CACHE = str(home_dataset_path if home_dataset_path.exists() else \"data/raw\")\n\n# Create the dataset directory if it doesn't exist\nos.makedirs(DATA_CACHE, exist_ok=True)\n\n# Check what we have in the Google Drive folder\nprint(\"üîç Current contents of your Google Drive dataset folder:\")\nif os.path.exists(DATA_CACHE):\n    items = os.listdir(DATA_CACHE)\n    for item in items:\n        item_path = os.path.join(DATA_CACHE, item)\n        if os.path.isdir(item_path):\n            print(f\"  üìÅ {item}/\")\n        else:\n            print(f\"  üìÑ {item}\")\n    if not items:\n        print(\"  (empty)\")\nelse:\n    print(\"  Folder does not exist, will be created\")\n\n# Define all IRMAS dataset parts\nirmas_datasets = {\n    \"IRMAS-TrainingData.zip\": \"IRMAS-TrainingData\",\n    \"IRMAS-TestingData-Part1.zip\": \"IRMAS-TestingData-Part1\", \n    \"IRMAS-TestingData-Part2.zip\": \"IRMAS-TestingData-Part2\",\n    \"IRMAS-TestingData-Part3.zip\": \"IRMAS-TestingData-Part3\"\n}\n\n# Check which parts are missing\nmissing_zips = []\nmissing_folders = []\n\nfor zip_name, folder_name in irmas_datasets.items():\n    zip_path = pathlib.Path(DATA_CACHE) / zip_name\n    folder_path = pathlib.Path(DATA_CACHE) / folder_name\n    \n    if not zip_path.exists():\n        missing_zips.append(zip_name)\n        print(f\"‚ùå Missing: {zip_name}\")\n    else:\n        print(f\"‚úÖ Found: {zip_name}\")\n    \n    if not folder_path.exists():\n        missing_folders.append(folder_name)\n        print(f\"‚ùå Missing extracted: {folder_name}\")\n    else:\n        print(f\"‚úÖ Found extracted: {folder_name}\")\n\n# Download missing zip files\nif missing_zips:\n    print(f\"\\nüì• Downloading {len(missing_zips)} missing dataset(s) to Google Drive...\")\n    download_irmas_main(pathlib.Path(DATA_CACHE))\nelse:\n    print(\"\\n‚úÖ All IRMAS zip files already present in Google Drive. Skipping download.\")\n\n# Extract missing folders\nfor zip_name, folder_name in irmas_datasets.items():\n    zip_path = pathlib.Path(DATA_CACHE) / zip_name\n    folder_path = pathlib.Path(DATA_CACHE) / folder_name\n    \n    if zip_path.exists() and not folder_path.exists():\n        print(f\"üì¶ Extracting {zip_name}...\")\n        try:\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(DATA_CACHE)\n            print(f\"‚úÖ Extracted {folder_name}\")\n        except Exception as e:\n            print(f\"‚ùå Failed to extract {zip_name}: {e}\")\n\n# Final verification\nprint(\"\\nüîç Final verification of dataset extraction:\")\nall_ready = True\nfor folder_name in irmas_datasets.values():\n    expected_path = pathlib.Path(DATA_CACHE) / folder_name\n    if expected_path.exists():\n        print(f\"‚úÖ {folder_name} found at {expected_path}\")\n    else:\n        print(f\"‚ùå {folder_name} missing at {expected_path}\")\n        all_ready = False\n\nif all_ready:\n    print(\"üéâ All IRMAS dataset parts are ready in Google Drive!\")\n    # Set IRMAS root to base path for compatibility with existing code\n    irmas_root = pathlib.Path(DATA_CACHE)\n    print(f\"üìÇ IRMAS base path set to: {irmas_root}\")\nelse:\n    print(\"\\n‚ö†Ô∏è Some dataset parts are missing. Please check for errors above.\")\n    irmas_root = None\n\n# Show final folder structure\nprint(f\"\\nüìä Final Google Drive dataset structure:\")\nif os.path.exists(DATA_CACHE):\n    for item in sorted(os.listdir(DATA_CACHE)):\n        item_path = os.path.join(DATA_CACHE, item)\n        if os.path.isdir(item_path):\n            # Count files in each subfolder\n            try:\n                file_count = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])\n                print(f\"  üìÅ {item}/ ({file_count} files)\")\n            except:\n                print(f\"  üìÅ {item}/\")\n        else:\n            # Show file size for zip files\n            try:\n                size_mb = os.path.getsize(item_path) / (1024*1024)\n                print(f\"  üìÑ {item} ({size_mb:.1f} MB)\")\n            except:\n                print(f\"  üìÑ {item}\")",
   "id": "6e04ee7069ac34ec",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fix NumPy compatibility issue\n",
    "import sys\n",
    "\n",
    "print(\"üîß Fixing NumPy compatibility...\")\n",
    "\n",
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Current NumPy version: {np.__version__}\")\n",
    "\n",
    "# If NumPy 2.0+, we need to downgrade or use a workaround\n",
    "if int(np.__version__.split('.')[0]) >= 2:\n",
    "    print(\"‚ö†Ô∏è  NumPy 2.0+ detected. Installing compatible version...\")\n",
    "    !pip install \"numpy<2.0\" --quiet\n",
    "\n",
    "    # Restart the kernel to load the new NumPy version\n",
    "    print(\"üîÑ Restarting kernel to load compatible NumPy...\")\n",
    "    import os\n",
    "\n",
    "    os.kill(os.getpid(), 9)  # This will restart the kernel in Colab\n",
    "else:\n",
    "    print(\"‚úÖ NumPy version is compatible\")"
   ],
   "id": "d9db882fdcb43259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from data.download_irmas import load_irmas_audio_dataset, load_irmas_testing_dataset\n",
    "import pathlib\n",
    "\n",
    "if irmas_root and irmas_root.exists():\n",
    "\n",
    "    print(\"üìÅ Dataset creation settings from config:\")\n",
    "    print(f\"   max_original_samples: {cfg.get('max_original_samples', 50)}\")\n",
    "    print(f\"   num_mixtures: {cfg.get('num_mixtures', 100)}\")\n",
    "    print(f\"   min_instruments: {cfg.get('min_instruments', 1)}\")\n",
    "    print(f\"   max_instruments: {cfg.get('max_instruments', 2)}\")\n",
    "\n",
    "    # Normalize root\n",
    "    base_root = irmas_root.parent if irmas_root.name == \"IRMAS-TrainingData\" else irmas_root\n",
    "\n",
    "    # Define separate paths\n",
    "    training_path = base_root / \"IRMAS-TrainingData\"\n",
    "    testing_paths = [base_root / f\"IRMAS-TestingData-Part{i}\" for i in range(1, 4)]\n",
    "\n",
    "    print(f\"üìÇ IRMAS paths:\")\n",
    "    print(f\"   ‚îú‚îÄ Training: {training_path}\")\n",
    "    for tp in testing_paths:\n",
    "        print(f\"   ‚îî‚îÄ Test Part: {tp}\")\n",
    "\n",
    "    # Load training data (single-label)\n",
    "    original_dataset = load_irmas_audio_dataset(base_root, cfg,\n",
    "                                            max_samples=cfg.get(\"max_original_samples\"))\n",
    "\n",
    "    # Load test data (multi-label)\n",
    "    test_datasets = []\n",
    "    for _ in testing_paths:        # paths 1-3 are *ignored* here\n",
    "        test_datasets.extend(load_irmas_testing_dataset(base_root, cfg))\n",
    "\n",
    "    # Merge datasets if needed\n",
    "    total_loaded = len(original_dataset) + len(test_datasets)\n",
    "    print(f\"\\nüìä Final dataset summary:\")\n",
    "    print(f\"   ‚úÖ Training samples loaded: {len(original_dataset)}\")\n",
    "    print(f\"   ‚úÖ Testing samples loaded: {len(test_datasets)}\")\n",
    "    print(f\"   ‚úÖ Total samples loaded:   {total_loaded}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå IRMAS root not found or invalid. Please run the download step first.\")"
   ],
   "id": "4277635d9e525b8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "if irmas_root:\n    print(f\"IRMAS dataset found at: {irmas_root}\")\n    \n    # Use Google Drive for processed features cache as well to avoid reprocessing\n    if IN_COLAB:\n        PROCESSED_DIR = \"/content/drive/MyDrive/datasets/IRMAS_processed_features\"\n        print(f\"üìÅ Using Google Drive for processed features: {PROCESSED_DIR}\")\n    else:\n        PROCESSED_DIR = \"data/processed\"\n\n    # Create processed directory if it doesn't exist\n    os.makedirs(PROCESSED_DIR, exist_ok=True)\n\n    # Use config value for original data percentage\n    original_data_percentage = cfg.get('original_data_percentage', 0.1)\n    print(f\"Using {original_data_percentage*100}% of original IRMAS data (from config)\")\n\n    # Check if processed features already exist\n    expected_processed_dirs = [\n        os.path.join(PROCESSED_DIR, \"train\"),\n        os.path.join(PROCESSED_DIR, \"val\"), \n        os.path.join(PROCESSED_DIR, \"test\")\n    ]\n    \n    processing_needed = not all(os.path.exists(d) and os.listdir(d) for d in expected_processed_dirs)\n    \n    if processing_needed:\n        print(\"üìä Processed features not found or incomplete. Starting preprocessing...\")\n        from data.preprocess import preprocess_data\n        \n        preprocess_data(\n            irmas_root=irmas_root,\n            out_dir=PROCESSED_DIR,\n            cfg=cfg,\n            original_data_percentage=original_data_percentage\n        )\n        print(f\"‚úÖ Preprocessing complete! Features saved to Google Drive: {PROCESSED_DIR}\")\n    else:\n        print(\"‚úÖ Processed features found in Google Drive. Skipping preprocessing.\")\n        \n    # Show processed features summary\n    print(f\"\\nüìä Processed features summary:\")\n    for split in [\"train\", \"val\", \"test\"]:\n        split_path = os.path.join(PROCESSED_DIR, split)\n        if os.path.exists(split_path):\n            file_count = len([f for f in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, f))])\n            print(f\"  üìÅ {split}/ ({file_count} sample folders)\")\n        else:\n            print(f\"  ‚ùå {split}/ (missing)\")\n\nelse:\n    print(\"Could not locate IRMAS dataset after download. Check paths and try again.\")",
   "id": "e96b51ebbb46946c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# üì¶  Configure paths & echo training settings\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"üîß Configuring data paths and training settings...\")\n\n# 1) show YAML-driven parameters\nbase_max_samples = cfg.get(\"max_samples\", None)\nprint(\"üìÅ Base configuration from YAML:\")\nprint(f\"   max_samples            : {base_max_samples}\")\nprint(f\"   max_original_samples   : {cfg.get('max_original_samples', 50)}\")\nprint(f\"   num_mixtures           : {cfg.get('num_mixtures', 100)}\")\nprint(f\"   min_instruments        : {cfg.get('min_instruments', 1)}\")\nprint(f\"   max_instruments        : {cfg.get('max_instruments', 2)}\")\nprint(f\"   original_data_percentage : {cfg.get('original_data_percentage', 0.1)}\")\n\n# 2) notebook override notice\nif base_max_samples != cfg.get(\"max_samples\"):\n    print(f\"‚ö†Ô∏è  Notebook override: max_samples changed to {cfg.get('max_samples')}\")\n\n# 3) define processed-data locations (use Google Drive path)\nif IN_COLAB:\n    PROCESSED_DIR = \"/content/drive/MyDrive/datasets/IRMAS_processed_features\"\nelse:\n    PROCESSED_DIR = \"data/processed\"\n    \ncfg.update(\n    {\n        \"data_dir\": PROCESSED_DIR,\n        \"train_dir\": f\"{PROCESSED_DIR}/train\",\n        \"val_dir\": f\"{PROCESSED_DIR}/val\",\n        \"test_dir\": f\"{PROCESSED_DIR}/test\",\n    }\n)\n\nprint(\"\\nüìÇ Data directories:\")\nprint(f\"   Processed data : {cfg['data_dir']}\")\nprint(f\"   Training       : {cfg['train_dir']}\")\nprint(f\"   Validation     : {cfg['val_dir']}\")\nprint(f\"   Test           : {cfg['test_dir']}\")\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# üîç  Verify directory existence & detailed sample counts\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nimport pathlib\n\ndef count_sample_folders(split_path: pathlib.Path):\n    counts = dict(original=0, mixed=0, irmasTest=0, other=0)\n    if not split_path.exists():\n        return counts  # all zeros\n    for d in split_path.iterdir():\n        if not d.is_dir():\n            continue\n        n = d.name\n        if n.startswith(\"original_\"):\n            counts[\"original\"] += 1\n        elif n.startswith(\"mixed_\"):\n            counts[\"mixed\"] += 1\n        elif n.startswith(\"irmasTest_\"):\n            counts[\"irmasTest\"] += 1\n        else:\n            counts[\"other\"] += 1\n    return counts\n\n\nprint(\"\\nüîç Verifying data directories & split composition:\")\nfor split in [\"train\", \"val\", \"test\"]:\n    path = pathlib.Path(cfg[f\"{split}_dir\"])\n    counts = count_sample_folders(path)\n    total = sum(counts.values())\n\n    status = \"‚úÖ\" if total else \"‚ùå\"\n    print(f\"\\n{status} {split.upper()} ({path}): {total} total sample folders\")\n    print(f\"      ‚Ä¢ original_:  {counts['original']}\")\n    print(f\"      ‚Ä¢ mixed_:     {counts['mixed']}\")\n    print(f\"      ‚Ä¢ irmasTest_: {counts['irmasTest']}\")\n    if counts[\"other\"]:\n        print(f\"      ‚Ä¢ other:      {counts['other']}  ‚Üê check if expected!\")\n\n    # sanity warnings\n    if split in (\"train\", \"val\") and counts[\"irmasTest\"]:\n        print(\"      ‚ö†Ô∏è  Unexpected irmasTest_ folders in this split!\")\n    if split == \"test\" and (counts[\"original\"] or counts[\"mixed\"]):\n        print(\"      ‚ö†Ô∏è  Test split should contain ONLY irmasTest_ folders.\")\n\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# üéõÔ∏è  Final training configuration summary\n# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"\\n‚úÖ Final training configuration:\")\nprint(f\"   Training samples limit : {cfg.get('max_samples', 'unlimited')}\")\nprint(f\"   Batch size             : {cfg.get('batch_size')}\")\nprint(\n    f\"   Validation limit       : {cfg.get('limit_val_batches', 1.0)} \"\n    f\"({'percentage' if cfg.get('limit_val_batches', 1.0) <= 1 else 'batches'})\"\n)\nprint(f\"   Learning rate          : {cfg.get('learning_rate')}\")\nprint(f\"   Epochs                 : {cfg.get('num_epochs')}\")",
   "id": "65e8d58f34a6b23d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import required modules for the model\n",
    "import torch\n",
    "from var import LABELS\n",
    "from models.Conv_wavelet_cnn import WaveletCNN  # <- your new file\n",
    "\n",
    "n_classes = len(LABELS)\n",
    "sr = cfg[\"sample_rate\"]\n",
    "\n",
    "# Create the Wavelet-based model\n",
    "model = WaveletCNN(n_classes=n_classes, sr=sr)\n",
    "print(model)\n",
    "\n",
    "# Optional: torchinfo summary\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "    # pretend 2 seconds of audio\n",
    "    T = int(sr * 2.0)\n",
    "    summary(model, input_size=(1, T))\n",
    "except Exception as e:\n",
    "    print(f\"(torchinfo unavailable or failed: {e})\")\n",
    "\n",
    "# quick smoke test on CPU (or GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.zeros(2, int(sr * 2.0), device=device)  # [B, T]\n",
    "    y = model(x)\n",
    "print(\"Smoke test OK:\", y.shape)\n",
    "\n",
    "\n"
   ],
   "id": "bb56e96fc293e802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "# TRAINING LAUNCH (Wavelet version)\n",
    "from training.train import main as train_main  # make sure this uses WaveletCNN inside\n",
    "\n",
    "\n",
    "print(\"üì¶ Training config summary:\")\n",
    "print(f\"  train_dir: {cfg.get('train_dir')}\")\n",
    "print(f\"  val_dir  : {cfg.get('val_dir')}\")\n",
    "print(f\"  batch_size: {cfg.get('batch_size')}\")\n",
    "print(f\"  lr        : {cfg.get('learning_rate')}\")\n",
    "print(f\"  epochs    : {cfg.get('num_epochs')}\")\n",
    "\n",
    "try:\n",
    "    train_main(cfg)\n",
    "    print(\"üèÅ Training completed successfully!\")\n",
    "except Exception as err:\n",
    "    import traceback\n",
    "    print(f\"‚ùå Training error: {err}\")\n",
    "    traceback.print_exc(limit=2)\n"
   ],
   "id": "730ba260b71c40b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Inference and visualization using the test set\n",
   "id": "a54701eb69295b86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %%\n",
    "# Inference (wavelet model, raw waveforms)\n",
    "import glob, re, yaml, sys, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def find_best_checkpoint(log_dir=\"lightning_logs\"):\n",
    "    \"\"\"\n",
    "    Return the .ckpt with the highest val_mAP (matches your ModelCheckpoint filename).\n",
    "    \"\"\"\n",
    "    ckpts = glob.glob(f\"{log_dir}/*/checkpoints/*.ckpt\")\n",
    "    if not ckpts:\n",
    "        print(f\"‚ùå No checkpoints found under {log_dir}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üîç Found {len(ckpts)} candidate checkpoint(s)\")\n",
    "\n",
    "    best, best_map, best_epoch = None, -1.0, -1\n",
    "    pat_epoch = re.compile(r\"epoch=(\\d+)\")\n",
    "    pat_map   = re.compile(r\"val_mAP=([0-9]+(?:\\.[0-9]*)?)\")  # <-- NOTE: mAP key\n",
    "\n",
    "    for c in ckpts:\n",
    "        m_ep  = pat_epoch.search(c)\n",
    "        m_map = pat_map.search(c)\n",
    "        if not (m_ep and m_map):\n",
    "            continue\n",
    "        epoch = int(m_ep.group(1))\n",
    "        vmap  = float(m_map.group(1).rstrip(\".\"))\n",
    "        if vmap > best_map or (vmap == best_map and epoch > best_epoch):\n",
    "            best, best_map, best_epoch = c, vmap, epoch\n",
    "\n",
    "    if best:\n",
    "        print(f\"‚úÖ Best checkpoint ‚Üí {best} | epoch {best_epoch} | val_mAP {best_map:.4f}\")\n",
    "    return best or ckpts[0]\n",
    "\n",
    "ckpt_path = find_best_checkpoint()\n",
    "if ckpt_path is None:\n",
    "    sys.exit(\"üõë  Aborting ‚Äì no checkpoints found.\")\n",
    "\n",
    "# Reload YAML\n",
    "with open(yaml_path) as fh:\n",
    "    cfg = yaml.safe_load(fh)\n",
    "\n",
    "# Rebuild wavelet model and load weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WaveletCNN(n_classes=len(LABELS), sr=cfg[\"sample_rate\"]).to(device).eval()\n",
    "\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "state_dict = state.get(\"state_dict\", state)\n",
    "# strip 'model.' prefix if present\n",
    "cleaned = {}\n",
    "for k, v in state_dict.items():\n",
    "    cleaned[k[6:]] = v if k.startswith(\"model.\") else v\n",
    "\n",
    "missing, unexpected = model.load_state_dict(cleaned, strict=False)\n",
    "if missing:   print(\"Missing keys:\", missing[:5], \"‚Ä¶\")  # usually harmless if layer names changed slightly\n",
    "if unexpected: print(\"Unexpected keys:\", unexpected[:5], \"‚Ä¶\")\n",
    "\n",
    "def predict_waveform(model, wav_path, cfg, device):\n",
    "    y, sr = librosa.load(wav_path, sr=cfg[\"sample_rate\"], mono=True)\n",
    "    x = torch.from_numpy(y.astype(np.float32)).to(device).unsqueeze(0)  # [1, T]\n",
    "    with torch.no_grad():\n",
    "        probs = model(x).squeeze(0).cpu().numpy()  # already sigmoid in model\n",
    "    return {label: float(probs[i]) for i, label in enumerate(LABELS)}\n",
    "\n",
    "# Gather test wavs\n",
    "MAX_TEST_SAMPLES = cfg.get(\"max_test_samples\", 5)\n",
    "wav_candidates = []\n",
    "tdir = cfg.get(\"test_dir\")\n",
    "if tdir and Path(tdir).exists():\n",
    "    wav_candidates = list(Path(tdir).rglob(\"*.wav\"))\n",
    "if not wav_candidates and \"irmas_root\" in globals() and irmas_root:\n",
    "    wav_candidates = list(Path(irmas_root).rglob(\"*.wav\"))\n",
    "wav_candidates = [p for p in wav_candidates if \"IRMAS-TestingData-Part\" in str(p)]\n",
    "if MAX_TEST_SAMPLES and len(wav_candidates) > MAX_TEST_SAMPLES:\n",
    "    wav_candidates = wav_candidates[:MAX_TEST_SAMPLES]\n",
    "\n",
    "print(f\"üóÇÔ∏è  Running inference on {len(wav_candidates)} file(s)\")\n",
    "\n",
    "# Simple single-label accuracy (top-1) if GT is in file name\n",
    "import re\n",
    "irmas_to_label = {\n",
    "    'cel': 'cello','cla':'clarinet','flu':'flute',\n",
    "    'gac':'acoustic_guitar','gel':'acoustic_guitar',\n",
    "    'org':'organ','pia':'piano','sax':'saxophone',\n",
    "    'tru':'trumpet','vio':'violin','voi':'voice'\n",
    "}\n",
    "def parse_gt_from_name(name: str):\n",
    "    codes = re.findall(r\"\\[([a-z]{3})\\]\", name)\n",
    "    return [irmas_to_label[c] for c in codes if c in irmas_to_label]\n",
    "\n",
    "correct = 0\n",
    "total   = 0\n",
    "for p in wav_candidates:\n",
    "    preds = predict_waveform(model, str(p), cfg, device)\n",
    "    top = max(preds.items(), key=lambda kv: kv[1])[0]\n",
    "    gt = parse_gt_from_name(p.name)\n",
    "    if gt:\n",
    "        total += 1\n",
    "        if top in gt:\n",
    "            correct += 1\n",
    "    print(f\"{p.name:45s} ‚Üí top: {top:15s} | gt: {gt}\")\n",
    "\n",
    "if total:\n",
    "    print(f\"\\nüéØ Single-label top-1 accuracy: {correct}/{total} = {correct/total:.1%}\")\n",
    "else:\n",
    "    print(\"\\n(no ground-truth tags found in filenames; skipped accuracy)\")\n"
   ],
   "id": "f7809bccc1e49b2e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
