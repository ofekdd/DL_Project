{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Instrument Classification – Colab Runtime\n",
    "This notebook sets up the environment, downloads the IRMAS dataset **once** to your Google Drive (shared folder), regenerates features each session, and trains the model."
   ],
   "id": "e96dda9b1458f0ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Colab & Drive setup ----------------------------------------------------\n",
    "import sys, pathlib, os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Drive mounted.\")\n",
    "else:\n",
    "    print(\"Running outside Colab\")\n"
   ],
   "id": "1dab21d5b32e6be3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Project clone & dependency install ------------------------------------\n",
    "REPO_URL = \"https://github.com/ofekdd/DL_Project.git\"   # <- adjust if needed\n",
    "REPO_DIR = \"DL_Project\"\n",
    "\n",
    "if not pathlib.Path(REPO_DIR).is_dir():\n",
    "    !git clone $REPO_URL\n",
    "%cd $REPO_DIR\n",
    "\n",
    "!pip -q install -r requirements.txt\n"
   ],
   "id": "cf3d5314bf968326"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Dataset download (raw zip only on Drive) ------------------------------\n",
    "import subprocess, pathlib, os, sys\n",
    "\n",
    "DATA_CACHE = \"/content/drive/MyDrive/DL_Shared/IRMAS\" if IN_COLAB else \"data/raw/IRMAS\"\n",
    "!python data/download_irmas.py --out_dir $DATA_CACHE\n"
   ],
   "id": "60f2c7a8a0484623"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Feature preprocessing (done each session into /content) ---------------\n",
    "import pathlib, os, sys, subprocess, json, shutil\n",
    "\n",
    "FEATURE_DIR = \"/content/IRMAS_features\"\n",
    "if not pathlib.Path(FEATURE_DIR).is_dir():\n",
    "    print(\"Preprocessing train split ...\")\n",
    "    !python data/preprocess.py --in_dir $DATA_CACHE/IRMAS-TrainingData --out_dir $FEATURE_DIR/train\n",
    "    print(\"Preprocessing test split ...\")\n",
    "    !python data/preprocess.py --in_dir $DATA_CACHE/IRMAS-TestingData --out_dir $FEATURE_DIR/test\n",
    "else:\n",
    "    print(\"Features already exist in this runtime – skipping.\")\n"
   ],
   "id": "85a80653b6593263"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Model Selection --------------------------------------------------------\n",
    "# Choose which model to use: \"cnn\", \"9cnn\", or \"resnet34\"\n",
    "MODEL_TYPE = \"resnet34\"  # Change this to \"cnn\" or \"9cnn\" to use a different model\n",
    "\n",
    "# Create model-specific config file\n",
    "import yaml\n",
    "\n",
    "# Load default config\n",
    "with open(\"configs/default.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set model name in config\n",
    "config[\"model_name\"] = MODEL_TYPE\n",
    "\n",
    "# Adjust other parameters based on model type\n",
    "if MODEL_TYPE == \"resnet34\":\n",
    "    config[\"learning_rate\"] = 1e-4\n",
    "    config[\"batch_size\"] = 16\n",
    "elif MODEL_TYPE == \"9cnn\":\n",
    "    config[\"learning_rate\"] = 2e-4\n",
    "    config[\"batch_size\"] = 8\n",
    "else:  # Default CNN\n",
    "    config[\"learning_rate\"] = 3e-4\n",
    "    config[\"batch_size\"] = 32\n",
    "\n",
    "# Write to a temporary config file\n",
    "CONFIG = f\"configs/model_{MODEL_TYPE}.yaml\"\n",
    "with open(CONFIG, \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(f\"Using model: {MODEL_TYPE}\")\n",
    "print(f\"Config saved to: {CONFIG}\")\n",
    "\n",
    "# --- Training --------------------------------------------------------------\n",
    "import torch\n",
    "from training.train import main as train_main\n",
    "\n",
    "!python -m training.train --config $CONFIG\n"
   ],
   "id": "919f7d753f51380f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Inference demo --------------------------------------------------------\n",
    "CKPT_PATH = !ls lightning_logs/*/checkpoints/*.ckpt | tail -n 1\n",
    "TEST_WAV = f\"{DATA_CACHE}/IRMAS-TestingData/0001.wav\"\n",
    "!python inference/predict.py {CKPT_PATH[0]} $TEST_WAV\n"
   ],
   "id": "5c64b20e8a72fb22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Visualization ---------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from data.preprocess import generate_multi_stft\n",
    "from inference.predict import predict\n",
    "from models import ResNetSpec\n",
    "\n",
    "def visualize_audio(wav_path, cfg):\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(wav_path, sr=cfg['sample_rate'], mono=True)\n",
    "\n",
    "    # Compute multi-band STFT spectrograms\n",
    "    specs_dict = generate_multi_stft(y, sr)\n",
    "\n",
    "    # Plot waveform and selected spectrograms\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    # Plot waveform\n",
    "    plt.subplot(4, 1, 1)\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title('Waveform')\n",
    "\n",
    "    # Select three spectrograms to visualize (one from each frequency band with the middle FFT size)\n",
    "    keys_to_plot = [\n",
    "        (\"0-1000Hz\", 512),\n",
    "        (\"1000-4000Hz\", 512),\n",
    "        (\"4000-11025Hz\", 512)\n",
    "    ]\n",
    "\n",
    "    for i, key in enumerate(keys_to_plot):\n",
    "        if key in specs_dict:\n",
    "            plt.subplot(4, 1, i+2)\n",
    "            spec = specs_dict[key]\n",
    "            hop_length = 512 // 4  # hop_length for FFT size 512\n",
    "            librosa.display.specshow(spec, sr=sr, x_axis='time', hop_length=hop_length)\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Spectrogram: {key[0]}, FFT size: {key[1]}')\n",
    "        else:\n",
    "            print(f\"Spectrogram for {key} not found\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load configuration\n",
    "cfg = yaml.safe_load(open(CONFIG))  # Use the same config file created for training\n",
    "\n",
    "# Visualize a sample audio file\n",
    "if pathlib.Path(TEST_WAV).exists():\n",
    "    visualize_audio(TEST_WAV, cfg)\n",
    "\n",
    "    # Load model and make predictions\n",
    "    if CKPT_PATH and pathlib.Path(CKPT_PATH[0]).exists():\n",
    "        # Create the appropriate model based on MODEL_TYPE\n",
    "        if MODEL_TYPE == \"resnet34\":\n",
    "            from models import ResNetSpec\n",
    "            model = ResNetSpec(11)\n",
    "        elif MODEL_TYPE == \"9cnn\":\n",
    "            from models import MultiSTFTCNN\n",
    "            model = MultiSTFTCNN(11)\n",
    "        else:  # Default CNN\n",
    "            from models import CNNBaseline\n",
    "            model = CNNBaseline(11)\n",
    "\n",
    "        model.load_state_dict(torch.load(CKPT_PATH[0], map_location=\"cpu\")[\"state_dict\"])\n",
    "        results = predict(model, TEST_WAV, cfg)\n",
    "\n",
    "        # Display results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(results.keys(), results.values())\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.title('Instrument Detection Confidence')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Test WAV file not found: {TEST_WAV}\")\n"
   ],
   "id": "42c66617bb1c40a6"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
