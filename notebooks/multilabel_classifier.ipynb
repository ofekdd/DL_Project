{
 "cells": [
  {
   "cell_type": "code",
   "id": "fae51c5c100772c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.171955Z",
     "start_time": "2025-06-07T07:18:27.160441Z"
    }
   },
   "source": [
    "\n",
    "import sys\n",
    "import warnings, tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmWarning)\n",
    "sys.modules['tqdm.notebook'] = tqdm\n",
    "sys.modules['tqdm.autonotebook'] = tqdm\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "\n",
    "    # Always start fresh and clone the specific branch\n",
    "    print(\"üóëÔ∏è Cleaning up any existing project...\")\n",
    "    %cd / content\n",
    "    !rm -rf DL_Project\n",
    "\n",
    "    #TODO: Fix the branch according to the latest changes\n",
    "    print(\"üì• Cloning specific branch 'master'...\")\n",
    "    !git clone -b master https://github.com/ofekdd/DL_Project.git\n",
    "    %cd DL_Project\n",
    "\n",
    "    # Verify we're on the correct branch\n",
    "    print(\"üîç Verifying branch...\")\n",
    "    !git branch\n",
    "    !git log --oneline -n 3\n",
    "\n",
    "    # Install dependencies\n",
    "    print(\"üì¶ Installing dependencies...\")\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    print(\"‚úÖ Setup complete with branch 'master'!\")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.238878Z",
     "start_time": "2025-06-07T07:18:27.232410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the current working directory and ensure it is the project root\n",
    "from pathlib import Path\n",
    "print(\"CWD :\", Path.cwd())                    # where the kernel is running\n",
    "print(\"Exists?\", Path('configs').is_dir())    # should be True if CWD is project root\n"
   ],
   "id": "3d6ed40822d8c61b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD : /home/odahan/Technion/Semester_8/Deep_Learning/Project/notebooks\n",
      "Exists? False\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.301113Z",
     "start_time": "2025-06-07T07:18:27.291167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Define the path to the YAML configuration file\n",
    "workspace = '/home/odahan/Technion/Semester_8/Deep_Learning/Project'\n",
    "yaml_path = 'configs/panns_enhanced.yaml' if IN_COLAB else f'{workspace}/configs/panns_enhanced.yaml'\n",
    "print(yaml_path)\n",
    "# Open and load the YAML file\n",
    "with open(yaml_path, 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "\n",
    "print(\"PANNs-enhanced configuration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "id": "4be9f8e58789698a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/odahan/Technion/Semester_8/Deep_Learning/Project/configs/multi_stft_cnn.yaml\n",
      "9cnn configuration:\n",
      "  model_name: multi_stft_cnn\n",
      "  sample_rate: 22050\n",
      "  n_mels: 64\n",
      "  hop_length: 512\n",
      "  batch_size: 8\n",
      "  num_epochs: 50\n",
      "  learning_rate: 2e-4\n",
      "  num_workers: 4\n",
      "  n_branches: 9\n",
      "  branch_output_dim: 128\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download the IRMAS dataset (training + testing parts) if needed\n",
    "from data.download_irmas import main as download_irmas_main\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Environment-aware cache path\n",
    "if IN_COLAB:\n",
    "    DATA_CACHE = \"/content/drive/MyDrive/datasets/IRMAS\"\n",
    "else:\n",
    "    # Check for dataset in home directory\n",
    "    home_dataset_path = pathlib.Path.home() / \"datasets\" / \"irmas\"\n",
    "    DATA_CACHE = str(home_dataset_path if home_dataset_path.exists() else \"data/raw\")\n",
    "\n",
    "# Create the dataset directory if it doesn't exist\n",
    "os.makedirs(DATA_CACHE, exist_ok=True)\n",
    "\n",
    "# Download the IRMAS datasets if not already downloaded\n",
    "irmas_zips = [\n",
    "    \"IRMAS.TrainingData.zip\",\n",
    "    \"IRMAS-TestingData-Part1.zip\",\n",
    "    \"IRMAS-TestingData-Part2.zip\",\n",
    "    \"IRMAS-TestingData-Part3.zip\"\n",
    "]\n",
    "download_required = any(not (pathlib.Path(DATA_CACHE) / zip_name).exists() for zip_name in irmas_zips)\n",
    "\n",
    "if download_required:\n",
    "    print(f\"üì• Downloading IRMAS datasets into: {DATA_CACHE}\")\n",
    "    download_irmas_main(pathlib.Path(DATA_CACHE))\n",
    "else:\n",
    "    print(\"‚úÖ All IRMAS zip files already present. Skipping download.\")\n",
    "\n",
    "# Confirm dataset availability\n",
    "expected_dirs = [\n",
    "    \"IRMAS-TrainingData\",\n",
    "    \"IRMAS-TestingData-Part1\",\n",
    "    \"IRMAS-TestingData-Part2\",\n",
    "    \"IRMAS-TestingData-Part3\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Verifying dataset extraction:\")\n",
    "missing = []\n",
    "for dir_name in expected_dirs:\n",
    "    expected_path = pathlib.Path(DATA_CACHE) / dir_name\n",
    "    if expected_path.exists():\n",
    "        print(f\"‚úÖ {dir_name} found at {expected_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dir_name} missing at {expected_path}\")\n",
    "        missing.append(dir_name)\n",
    "\n",
    "if missing:\n",
    "    print(\"\\n‚ö†Ô∏è Some dataset parts are missing. You may want to delete corrupted zip files and re-run the cell.\")\n",
    "else:\n",
    "    print(\"üéâ All expected IRMAS dataset parts are ready.\")\n",
    "\n",
    "# Set IRMAS root to base path (used in later cells to access all parts)\n",
    "irmas_root = pathlib.Path(DATA_CACHE)\n",
    "print(f\"\\nüìÇ IRMAS base path set to: {irmas_root}\")"
   ],
   "id": "6e04ee7069ac34ec",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fix NumPy compatibility issue\n",
    "import sys\n",
    "\n",
    "print(\"üîß Fixing NumPy compatibility...\")\n",
    "\n",
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Current NumPy version: {np.__version__}\")\n",
    "\n",
    "# If NumPy 2.0+, we need to downgrade or use a workaround\n",
    "if int(np.__version__.split('.')[0]) >= 2:\n",
    "    print(\"‚ö†Ô∏è  NumPy 2.0+ detected. Installing compatible version...\")\n",
    "    !pip install \"numpy<2.0\" --quiet\n",
    "\n",
    "    # Restart the kernel to load the new NumPy version\n",
    "    print(\"üîÑ Restarting kernel to load compatible NumPy...\")\n",
    "    import os\n",
    "\n",
    "    os.kill(os.getpid(), 9)  # This will restart the kernel in Colab\n",
    "else:\n",
    "    print(\"‚úÖ NumPy version is compatible\")"
   ],
   "id": "d9db882fdcb43259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from data.download_irmas import load_irmas_audio_dataset, load_irmas_testing_dataset\n",
    "import pathlib\n",
    "\n",
    "if irmas_root and irmas_root.exists():\n",
    "\n",
    "    print(\"üìÅ Dataset creation settings from config:\")\n",
    "    print(f\"   max_original_samples: {cfg.get('max_original_samples', 50)}\")\n",
    "    print(f\"   num_mixtures: {cfg.get('num_mixtures', 100)}\")\n",
    "    print(f\"   min_instruments: {cfg.get('min_instruments', 1)}\")\n",
    "    print(f\"   max_instruments: {cfg.get('max_instruments', 2)}\")\n",
    "\n",
    "    # Normalize root\n",
    "    base_root = irmas_root.parent if irmas_root.name == \"IRMAS-TrainingData\" else irmas_root\n",
    "\n",
    "    # Define separate paths\n",
    "    training_path = base_root / \"IRMAS-TrainingData\"\n",
    "    testing_paths = [base_root / f\"IRMAS-TestingData-Part{i}\" for i in range(1, 4)]\n",
    "\n",
    "    print(f\"üìÇ IRMAS paths:\")\n",
    "    print(f\"   ‚îú‚îÄ Training: {training_path}\")\n",
    "    for tp in testing_paths:\n",
    "        print(f\"   ‚îî‚îÄ Test Part: {tp}\")\n",
    "\n",
    "    # Load training data (single-label)\n",
    "    original_dataset = load_irmas_audio_dataset(base_root, cfg,\n",
    "                                            max_samples=cfg.get(\"max_original_samples\"))\n",
    "\n",
    "    # Load test data (multi-label)\n",
    "    test_datasets = []\n",
    "    for _ in testing_paths:        # paths 1-3 are *ignored* here\n",
    "        test_datasets.extend(load_irmas_testing_dataset(base_root, cfg))\n",
    "\n",
    "    # Merge datasets if needed\n",
    "    total_loaded = len(original_dataset) + len(test_datasets)\n",
    "    print(f\"\\nüìä Final dataset summary:\")\n",
    "    print(f\"   ‚úÖ Training samples loaded: {len(original_dataset)}\")\n",
    "    print(f\"   ‚úÖ Testing samples loaded: {len(test_datasets)}\")\n",
    "    print(f\"   ‚úÖ Total samples loaded:   {total_loaded}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå IRMAS root not found or invalid. Please run the download step first.\")"
   ],
   "id": "4277635d9e525b8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading IRMAS dataset to data/raw...\n",
      "Archive already exists, skipping download\n",
      "Verifying checksum ...\n",
      "Extracting ...\n",
      "Done. Data at data/raw\n",
      "IRMAS dataset found at: data/raw/IRMAS-TrainingData\n",
      "\n",
      "To preprocess the data, you can run:\n",
      "python data/preprocess.py --in_dir data/raw/IRMAS-TrainingData --out_dir data/processed\n",
      "\n",
      "Or execute this command in the next cell:\n",
      "!python data/preprocess.py --in_dir data/raw/IRMAS-TrainingData --out_dir data/processed\n"
     ]
    }
   ],
   "execution_count": 29,
   "source": [
    "if irmas_root:\n",
    "    print(f\"IRMAS dataset found at: {irmas_root}\")\n",
    "    PROCESSED_DIR = \"/content/IRMAS_features\" if IN_COLAB else \"data/processed\"\n",
    "\n",
    "    # Use config value for original data percentage\n",
    "    original_data_percentage = cfg.get('original_data_percentage', 0.1)\n",
    "    print(f\"Using {original_data_percentage*100}% of original IRMAS data (from config)\")\n",
    "\n",
    "    from data.preprocess import preprocess_data\n",
    "\n",
    "    preprocess_data(\n",
    "        irmas_root=irmas_root,\n",
    "        out_dir=PROCESSED_DIR,\n",
    "        cfg=cfg,\n",
    "        original_data_percentage=original_data_percentage\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Preprocessing complete with mixed labels. Features saved to {PROCESSED_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not locate IRMAS dataset after download. Check paths and try again.\")"
   ],
   "id": "e96b51ebbb46946c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# üì¶  Configure paths & echo training settings\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"üîß Configuring data paths and training settings...\")\n",
    "\n",
    "# 1) show YAML-driven parameters\n",
    "base_max_samples = cfg.get(\"max_samples\", None)\n",
    "print(\"üìÅ Base configuration from YAML:\")\n",
    "print(f\"   max_samples            : {base_max_samples}\")\n",
    "print(f\"   max_original_samples   : {cfg.get('max_original_samples', 50)}\")\n",
    "print(f\"   num_mixtures           : {cfg.get('num_mixtures', 100)}\")\n",
    "print(f\"   min_instruments        : {cfg.get('min_instruments', 1)}\")\n",
    "print(f\"   max_instruments        : {cfg.get('max_instruments', 2)}\")\n",
    "print(f\"   original_data_percentage : {cfg.get('original_data_percentage', 0.1)}\")\n",
    "\n",
    "# 2) notebook override notice\n",
    "if base_max_samples != cfg.get(\"max_samples\"):\n",
    "    print(f\"‚ö†Ô∏è  Notebook override: max_samples changed to {cfg.get('max_samples')}\")\n",
    "\n",
    "# 3) define processed-data locations\n",
    "PROCESSED_DIR = \"/content/IRMAS_features\" if IN_COLAB else \"data/processed\"\n",
    "cfg.update(\n",
    "    {\n",
    "        \"data_dir\": PROCESSED_DIR,\n",
    "        \"train_dir\": f\"{PROCESSED_DIR}/train\",\n",
    "        \"val_dir\": f\"{PROCESSED_DIR}/val\",\n",
    "        \"test_dir\": f\"{PROCESSED_DIR}/test\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nüìÇ Data directories:\")\n",
    "print(f\"   Processed data : {cfg['data_dir']}\")\n",
    "print(f\"   Training       : {cfg['train_dir']}\")\n",
    "print(f\"   Validation     : {cfg['val_dir']}\")\n",
    "print(f\"   Test           : {cfg['test_dir']}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# üîç  Verify directory existence & detailed sample counts\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import pathlib\n",
    "\n",
    "def count_sample_folders(split_path: pathlib.Path):\n",
    "    counts = dict(original=0, mixed=0, irmasTest=0, other=0)\n",
    "    if not split_path.exists():\n",
    "        return counts  # all zeros\n",
    "    for d in split_path.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        n = d.name\n",
    "        if n.startswith(\"original_\"):\n",
    "            counts[\"original\"] += 1\n",
    "        elif n.startswith(\"mixed_\"):\n",
    "            counts[\"mixed\"] += 1\n",
    "        elif n.startswith(\"irmasTest_\"):\n",
    "            counts[\"irmasTest\"] += 1\n",
    "        else:\n",
    "            counts[\"other\"] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "print(\"\\nüîç Verifying data directories & split composition:\")\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    path = pathlib.Path(cfg[f\"{split}_dir\"])\n",
    "    counts = count_sample_folders(path)\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    status = \"‚úÖ\" if total else \"‚ùå\"\n",
    "    print(f\"\\n{status} {split.upper()} ({path}): {total} total sample folders\")\n",
    "    print(f\"      ‚Ä¢ original_:  {counts['original']}\")\n",
    "    print(f\"      ‚Ä¢ mixed_:     {counts['mixed']}\")\n",
    "    print(f\"      ‚Ä¢ irmasTest_: {counts['irmasTest']}\")\n",
    "    if counts[\"other\"]:\n",
    "        print(f\"      ‚Ä¢ other:      {counts['other']}  ‚Üê check if expected!\")\n",
    "\n",
    "    # sanity warnings\n",
    "    if split in (\"train\", \"val\") and counts[\"irmasTest\"]:\n",
    "        print(\"      ‚ö†Ô∏è  Unexpected irmasTest_ folders in this split!\")\n",
    "    if split == \"test\" and (counts[\"original\"] or counts[\"mixed\"]):\n",
    "        print(\"      ‚ö†Ô∏è  Test split should contain ONLY irmasTest_ folders.\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# üéõÔ∏è  Final training configuration summary\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n‚úÖ Final training configuration:\")\n",
    "print(f\"   Training samples limit : {cfg.get('max_samples', 'unlimited')}\")\n",
    "print(f\"   Batch size             : {cfg.get('batch_size')}\")\n",
    "print(\n",
    "    f\"   Validation limit       : {cfg.get('limit_val_batches', 1.0)} \"\n",
    "    f\"({'percentage' if cfg.get('limit_val_batches', 1.0) <= 1 else 'batches'})\"\n",
    ")\n",
    "print(f\"   Learning rate          : {cfg.get('learning_rate')}\")\n",
    "print(f\"   Epochs                 : {cfg.get('num_epochs')}\")\n"
   ],
   "id": "65e8d58f34a6b23d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import required modules for the model\n",
    "import torch\n",
    "from var import LABELS\n",
    "from models.panns_enhanced import MultiSTFTCNN_WithPANNs\n",
    "from data.download_pnn import download_panns_checkpoint\n",
    "\n",
    "n_classes = len(LABELS)\n",
    "\n",
    "# Download PANNs checkpoint if needed\n",
    "panns_path = download_panns_checkpoint()\n",
    "\n",
    "# Create the enhanced model with PANNs\n",
    "model = MultiSTFTCNN_WithPANNs(\n",
    "    n_classes=n_classes,\n",
    "    pretrained_path=panns_path,\n",
    "    freeze_backbone=False\n",
    ")\n",
    "\n",
    "print(\"PANNs-Enhanced Architecture:\")\n",
    "print(model)\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "\n",
    "    class ModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, x1, x2, x3):\n",
    "            return self.model([x1, x2, x3])\n",
    "\n",
    "    wrapped_model = ModelWrapper(model)\n",
    "\n",
    "    dummy_inputs = [\n",
    "        torch.zeros(1, 1, 128, 100),\n",
    "        torch.zeros(1, 1, 128, 100),\n",
    "        torch.zeros(1, 1, 128, 100)\n",
    "    ]\n",
    "\n",
    "    print(\"\\nModel Summary:\")\n",
    "    summary(wrapped_model, input_data=dummy_inputs, verbose=1)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nInstall torchinfo for detailed model summary: pip install torchinfo\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not generate model summary: {e}\")\n",
    "    print(\"This is normal - the model architecture is still correctly defined.\")\n",
    "\n",
    "print(\"\\nüîß Manual Model Summary:\")\n",
    "print(\"   üìä Input: 3 spectrograms (optimized window sizes for each frequency band)\")\n",
    "print(\"   üß† Architecture: 3 PANNs feature extractors + fusion layer + classifier\")\n",
    "print(f\"   üì§ Output: {n_classes} instrument classes\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"   üìà Total Parameters: {total_params:,}\")\n",
    "print(f\"   üéØ Trainable Parameters: {trainable_params:,}\")\n",
    "print(\"   üöÄ Using PANNs pretrained weights for enhanced feature extraction\")\n",
    "\n",
    "print(\"\\nüß™ Testing PANNs-enhanced model with dummy data...\")\n",
    "try:\n",
    "    dummy_input = [torch.zeros(2, 1, 20, 30) for _ in range(3)]\n",
    "    output = model(dummy_input)\n",
    "    print(\"   ‚úÖ Model test successful!\")\n",
    "    print(f\"   üìä Input: 3 tensors of shape {dummy_input[0].shape}\")\n",
    "    print(f\"   üì§ Output shape: {output.shape}\")\n",
    "    print(f\"   üéØ Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "    print(\"   ‚ÑπÔ∏è The PANNs model already applies sigmoid in its classifier\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Model test failed: {e}\")\n"
   ],
   "id": "bb56e96fc293e802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# üöÄ  TRAINING LAUNCH  ‚Äì  with split summary & extra sanity checks\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "print(\"üöÄ Starting training‚Ä¶\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1) Echo key configuration values\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"üìÅ Configuration:\")\n",
    "print(f\"   max_samples            : {cfg.get('max_samples', 'all')}\")\n",
    "print(f\"   train_dir              : {cfg.get('train_dir', 'not set')}\")\n",
    "print(f\"   val_dir                : {cfg.get('val_dir', 'not set')}\")\n",
    "print(f\"   test_dir               : {cfg.get('test_dir', 'not set')}\")\n",
    "print(f\"   batch_size             : {cfg.get('batch_size', 'not set')}\")\n",
    "print(\n",
    "    f\"   limit_val_batches      : {cfg.get('limit_val_batches', 1.0)} \"\n",
    "    f\"({'percentage' if cfg.get('limit_val_batches', 1.0) <= 1 else 'batches'})\"\n",
    ")\n",
    "print(f\"   num_sanity_val_steps   : {cfg.get('num_sanity_val_steps', 'default')}\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2) Split-directory summary helper\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def summarize_split(path: pathlib.Path):\n",
    "    if not path.exists():\n",
    "        return 0, \"missing dir\"\n",
    "    counts = Counter(\n",
    "        (p.name.split(\"_\")[0] for p in path.iterdir() if p.is_dir())\n",
    "    )  # original / mixed / irmasTest / ‚Ä¶\n",
    "    total = sum(counts.values())\n",
    "    detail = \", \".join(f\"{k}:{v}\" for k, v in counts.items()) if counts else \"empty\"\n",
    "    return total, detail\n",
    "\n",
    "\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    p = pathlib.Path(cfg.get(f\"{split}_dir\", \"\"))\n",
    "    total, detail = summarize_split(p)\n",
    "    status = \"‚úÖ\" if total else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {split.upper():5} ‚Üí {total:4} sample folders ({detail})\")\n",
    "\n",
    "print()  # spacer\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3) Guard against an empty validation set\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "val_total, _ = summarize_split(pathlib.Path(cfg[\"val_dir\"]))\n",
    "if val_total == 0:\n",
    "    raise RuntimeError(\n",
    "        \"Validation split is empty! \"\n",
    "        \"Increase `original_data_percentage` or check preprocessing.\"\n",
    "    )\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4) Training routine with robust import / fallback\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    from training.panns_train import main as train_main  # prefer PANNs variant\n",
    "    print(\"‚úÖ Imported training.panns_train.main\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  training.panns_train not available ‚Äì falling back to training.train\")\n",
    "    try:\n",
    "        from training.train import main as train_main\n",
    "    except ImportError as e:\n",
    "        sys.exit(f\"‚ùå Could not import training module: {e}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Launch training\n",
    "# --------------------------------------------------------------------\n",
    "try:\n",
    "    train_main(cfg)  # your Lightning entry point\n",
    "    print(\"üèÅ Training completed successfully!\")\n",
    "\n",
    "    # OPTIONAL: automatic test-set evaluation\n",
    "    # Comment out if your train_main already runs tests internally\n",
    "    if hasattr(train_main, \"__code__\") and \"run_test\" in train_main.__code__.co_varnames:\n",
    "        print(\"\\nüî¨ Running test-set evaluation‚Ä¶\")\n",
    "        train_main(cfg, run_test=True)\n",
    "        print(\"‚úÖ Test evaluation completed!\")\n",
    "\n",
    "except Exception as err:\n",
    "    print(f\"‚ùå Training error: {err}\")\n",
    "    traceback.print_exc(limit=2)\n",
    "    # Additional debugging for empty dataset issues\n",
    "    if \"num_samples=0\" in str(err).lower():\n",
    "        td = pathlib.Path(cfg[\"train_dir\"])\n",
    "        if td.exists():\n",
    "            items = list(td.iterdir())\n",
    "            print(f\"üîç Train dir contains {len(items)} items. First few:\")\n",
    "            for itm in items[:5]:\n",
    "                print(\"   ‚Ä¢\", itm.name)\n",
    "        else:\n",
    "            print(f\"‚ùå Train dir {td} does not exist!\")\n"
   ],
   "id": "730ba260b71c40b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# üöÄ  Inference on IRMAS test set (quick demo)   ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "from pathlib import Path\n",
    "from utils.model_loader import load_model\n",
    "from inference.predict import predict_with_ground_truth\n",
    "from var import LABELS\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# ‚îÄ‚îÄ 1. pick best checkpoint ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def find_best_checkpoint(log_dir: str = \"lightning_logs\") -> str | None:\n",
    "    \"\"\"\n",
    "    Return the .ckpt that has the highest val_mAP.\n",
    "    Ignores   - last.ckpt\n",
    "              - files without epoch= / val_mAP=\n",
    "    \"\"\"\n",
    "    ckpts = glob.glob(f\"{log_dir}/*/checkpoints/*.ckpt\")\n",
    "    if not ckpts:\n",
    "        print(f\"‚ùå No checkpoints in {log_dir}\")\n",
    "        return None\n",
    "\n",
    "    pat_ep  = re.compile(r\"epoch=(\\d+)\")\n",
    "    pat_map = re.compile(r\"val_mAP=([0-9]+(?:\\.[0-9]*)?)\")\n",
    "\n",
    "    best, best_map, best_ep = None, -1.0, -1\n",
    "    for c in ckpts:\n",
    "        m_ep  = pat_ep.search(c)\n",
    "        m_map = pat_map.search(c)\n",
    "        if not (m_ep and m_map):              # ‚Üê skip e.g. ‚Äúlast.ckpt‚Äù\n",
    "            continue\n",
    "        ep  = int(m_ep.group(1))\n",
    "        mp  = float(m_map.group(1).rstrip(\".\"))  # trims stray dot\n",
    "        if mp > best_map or (mp == best_map and ep > best_ep):\n",
    "            best, best_map, best_ep = c, mp, ep\n",
    "\n",
    "    if best:\n",
    "        print(f\"‚úÖ Using checkpoint {best} (epoch {best_ep}, mAP {best_map:.4f})\")\n",
    "        return best\n",
    "\n",
    "    # Fallback ‚Äì take the *first* ckpt (often last.ckpt) just so the\n",
    "    # rest of the code can still run, but warn the user.\n",
    "    print(\"‚ö†Ô∏è  No metric-tagged checkpoints found; falling back to\", ckpts[0])\n",
    "    return ckpts[0]\n",
    "\n",
    "\n",
    "ckpt = find_best_checkpoint()\n",
    "model = load_model(ckpt, n_classes=len(LABELS)).eval()\n",
    "\n",
    "# ‚îÄ‚îÄ 2. load per-class thresholds (optional) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def load_thr(path):\n",
    "    try:\n",
    "        return yaml.safe_load(open(path))[\"thresholds\"]\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "f1_thr  = load_thr(\"configs/optimal_thresholds_f1.yaml\")\n",
    "\n",
    "# ‚îÄ‚îÄ 3. collect true *test* WAVs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "wav_root = Path(irmas_root)\n",
    "\n",
    "wav_files = []\n",
    "for part_dir in wav_root.glob(\"IRMAS-TestingData-Part*\"):\n",
    "    # pick the inner folder if it exists, else use the part dir itself\n",
    "    inner = part_dir / \"IRMAS-TestingData\"\n",
    "    search_root = inner if inner.exists() else part_dir\n",
    "    wav_files.extend(search_root.rglob(\"*.wav\"))\n",
    "\n",
    "max_n = cfg.get(\"max_test_samples\")\n",
    "if max_n:\n",
    "    wav_files = wav_files[:max_n]\n",
    "\n",
    "print(f\"üóÇÔ∏è  Inference on {len(wav_files)} WAVs\")\n",
    "\n",
    "# ‚îÄ‚îÄ 4. run prediction ------------------------------------------------\n",
    "LOG_EVERY = 5          # print every 20th file\n",
    "for i, wav in enumerate(wav_files, 1):\n",
    "    res = predict_with_ground_truth(\n",
    "        model, str(wav), cfg, thresholds=f1_thr\n",
    "    )\n",
    "\n",
    "    # print only every k-th file (or change condition to whatever you prefer)\n",
    "    if i % LOG_EVERY == 0 or i == 1 or i == len(wav_files):\n",
    "        acts = \", \".join(res['active_instruments']) or \"None\"\n",
    "        print(f\"{i:>4}/{len(wav_files)} {wav.name} ‚Üí {acts}\")\n",
    "\n",
    "print(\"‚úÖ  Inference finished!\")\n"
   ],
   "id": "f7809bccc1e49b2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Threshold Optimization\n",
    "\n",
    "Optimize classification thresholds to improve instrument detection accuracy.\n"
   ],
   "id": "51c99b31c0173b4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# üéØ  Threshold optimisation & quick evaluation  ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "import glob, re, sys, yaml, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from utils.model_loader import load_model\n",
    "from inference.predict import predict_with_ground_truth\n",
    "from visualization.threshold_optimization import (\n",
    "    find_optimal_thresholds, save_thresholds\n",
    ")\n",
    "from data.dataset import create_dataloaders\n",
    "from var import LABELS\n",
    "\n",
    "# ‚îÄ‚îÄ helper to reuse the robust ckpt picker ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def best_ckpt(log_dir=\"lightning_logs\"):\n",
    "    ckpts = glob.glob(f\"{log_dir}/*/checkpoints/*.ckpt\")\n",
    "    if not ckpts:\n",
    "        sys.exit(f\"‚ùå No checkpoints inside {log_dir}\")\n",
    "\n",
    "    re_ep  = re.compile(r\"epoch=(\\d+)\")\n",
    "    re_map = re.compile(r\"val_mAP=([0-9]+(?:\\.[0-9]*)?)\")\n",
    "\n",
    "    best, best_map, best_ep = None, -1.0, -1\n",
    "    for path in ckpts:\n",
    "        m_ep, m_map = re_ep.search(path), re_map.search(path)\n",
    "        if not (m_ep and m_map):            # ‚Üê skip malformed names\n",
    "            continue\n",
    "\n",
    "        ep   = int(m_ep.group(1))\n",
    "        vmap = float(m_map.group(1).rstrip(\".\"))   # remove stray dot\n",
    "\n",
    "        if vmap > best_map or (vmap == best_map and ep > best_ep):\n",
    "            best, best_map, best_ep = path, vmap, ep\n",
    "\n",
    "    if best is None:\n",
    "        sys.exit(\"ü§î Found ckpts but none had both epoch= and val_mAP= in the name\")\n",
    "\n",
    "    print(f\"üèÜ {Path(best).name}  (epoch {best_ep}, mAP {best_map:.3f})\")\n",
    "    return best\n",
    "\n",
    "ckpt = best_ckpt()\n",
    "model = load_model(ckpt, n_classes=len(LABELS))\n",
    "\n",
    "# ‚îÄ‚îÄ 1. optimise thresholds on validation split ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "val_dir = Path(cfg[\"val_dir\"])\n",
    "_, val_loader = create_dataloaders(\n",
    "    train_dir=val_dir, val_dir=val_dir,\n",
    "    batch_size=cfg[\"batch_size\"], num_workers=cfg[\"num_workers\"],\n",
    "    use_multi_stft=True\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Optimising F1 thresholds ‚Ä¶\")\n",
    "f1_thr = find_optimal_thresholds(model, val_loader, metric=\"f1\")\n",
    "save_thresholds(f1_thr, \"configs/optimal_thresholds_f1.yaml\", \"f1\")\n",
    "\n",
    "print(\"üìä Optimising balanced-acc thresholds ‚Ä¶\")\n",
    "bal_thr = find_optimal_thresholds(model, val_loader, metric=\"balanced\")\n",
    "save_thresholds(bal_thr, \"configs/optimal_thresholds_balanced.yaml\", \"balanced\")\n",
    "print(\"‚úÖ Threshold optimisation done!\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ 2. quick demo on 5 random test files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "wav_root = Path(irmas_root)\n",
    "wav_pool = list(wav_root.glob(\"IRMAS-TestingData-Part*/*.wav\"))\n",
    "np.random.seed(42)\n",
    "demo = np.random.choice(wav_pool, 5, replace=False)\n",
    "\n",
    "print(\"üîç Demo predictions (F1 thresholds)\")\n",
    "for w in demo:\n",
    "    res = predict_with_ground_truth(model, str(w), cfg, thresholds=f1_thr)\n",
    "    print(f\"{Path(w).name:35} ‚Üí {res['active_instruments']}\")"
   ],
   "id": "9800c0a94a1197d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
