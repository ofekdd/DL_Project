{
 "cells": [
  {
   "cell_type": "code",
   "id": "fae51c5c100772c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.171955Z",
     "start_time": "2025-06-07T07:18:27.160441Z"
    }
   },
   "source": [
    "import sys\n",
    "import warnings, tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=tqdm.TqdmWarning)\n",
    "sys.modules['tqdm.notebook'] = tqdm\n",
    "sys.modules['tqdm.autonotebook'] = tqdm\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/ofekdd/DL_Project.git\n",
    "    %cd DL_Project\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install -r requirements.txt\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.238878Z",
     "start_time": "2025-06-07T07:18:27.232410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the current working directory and ensure it is the project root\n",
    "from pathlib import Path\n",
    "print(\"CWD :\", Path.cwd())                    # where the kernel is running\n",
    "print(\"Exists?\", Path('configs').is_dir())    # should be True if CWD is project root\n"
   ],
   "id": "3d6ed40822d8c61b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD : /home/odahan/Technion/Semester_8/Deep_Learning/Project/notebooks\n",
      "Exists? False\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:18:27.301113Z",
     "start_time": "2025-06-07T07:18:27.291167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Define the path to the YAML configuration file\n",
    "workspace = '/home/odahan/Technion/Semester_8/Deep_Learning/Project'\n",
    "yaml_path = f'{workspace}/configs/multi_stft_cnn.yaml'\n",
    "print(yaml_path)\n",
    "# Open and load the YAML file\n",
    "with open(yaml_path, 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "\n",
    "print(\"9cnn configuration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "id": "4be9f8e58789698a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/odahan/Technion/Semester_8/Deep_Learning/Project/configs/multi_stft_cnn.yaml\n",
      "9cnn configuration:\n",
      "  model_name: multi_stft_cnn\n",
      "  sample_rate: 22050\n",
      "  n_mels: 64\n",
      "  hop_length: 512\n",
      "  batch_size: 8\n",
      "  num_epochs: 50\n",
      "  learning_rate: 2e-4\n",
      "  num_workers: 4\n",
      "  n_branches: 9\n",
      "  branch_output_dim: 128\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download the IRMAS dataset if needed\n",
    "from data.download_irmas import main as download_irmas_main, find_irmas_root\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Check for existing dataset in user's home directory first\n",
    "home_dataset_path = pathlib.Path.home() / \"datasets\" / \"irmas\" / \"IRMAS.zip\"\n",
    "\n",
    "# Determine the appropriate download location based on environment\n",
    "if IN_COLAB:\n",
    "    # For Colab, use Google Drive to store the dataset (already mounted)\n",
    "    DATA_CACHE = \"/content/drive/MyDrive/datasets/IRMAS\"\n",
    "else:\n",
    "    # For local environment, check if dataset exists in home directory\n",
    "    if home_dataset_path.exists():\n",
    "        print(f\"Found existing dataset at {home_dataset_path}\")\n",
    "        DATA_CACHE = str(home_dataset_path.parent)\n",
    "    else:\n",
    "        # Fall back to project directory\n",
    "        DATA_CACHE = \"data/raw\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(DATA_CACHE, exist_ok=True)\n",
    "\n",
    "# Only download if we don't have the zip file already\n",
    "zip_path = pathlib.Path(DATA_CACHE) / \"IRMAS.zip\"\n",
    "if zip_path.exists():\n",
    "    print(f\"Dataset already exists at {zip_path}, skipping download...\")\n",
    "else:\n",
    "    print(f\"Downloading IRMAS dataset to {DATA_CACHE}...\")\n",
    "    download_irmas_main(pathlib.Path(DATA_CACHE))\n",
    "\n",
    "# Find the IRMAS dataset root\n",
    "irmas_root = find_irmas_root()"
   ],
   "id": "6e04ee7069ac34ec",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Convert the training dataset into multi-label format\n",
    "from data.mix_labels import create_multilabel_dataset\n",
    "\n",
    "if irmas_root:\n",
    "    print(\"Creating multi-label dataset from IRMAS...\")\n",
    "\n",
    "    # Create both original and mixed datasets\n",
    "    original_dataset, mixed_dataset = create_multilabel_dataset(\n",
    "        irmas_root=irmas_root,\n",
    "        cfg=cfg,\n",
    "        max_original_samples=50,  # Limit original samples to avoid memory issues\n",
    "        num_mixtures=100,  # Create 100 synthetic mixtures\n",
    "        min_instruments=1,  # Allow 1-2 instruments per mixture\n",
    "        max_instruments=2\n",
    "    )\n",
    "\n",
    "    # Optional: You can now save these datasets or use them for training\n",
    "    if mixed_dataset:\n",
    "        MIXED_DIR = \"/content/IRMAS_mixed\" if IN_COLAB else \"data/mixed\"\n",
    "        print(f\"\\nTo save mixed samples for later use, you could write them to: {MIXED_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\"IRMAS root not found. Please run the download cell first.\")"
   ],
   "id": "4277635d9e525b8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading IRMAS dataset to data/raw...\n",
      "Archive already exists, skipping download\n",
      "Verifying checksum ...\n",
      "Extracting ...\n",
      "Done. Data at data/raw\n",
      "IRMAS dataset found at: data/raw/IRMAS-TrainingData\n",
      "\n",
      "To preprocess the data, you can run:\n",
      "python data/preprocess.py --in_dir data/raw/IRMAS-TrainingData --out_dir data/processed\n",
      "\n",
      "Or execute this command in the next cell:\n",
      "!python data/preprocess.py --in_dir data/raw/IRMAS-TrainingData --out_dir data/processed\n"
     ]
    }
   ],
   "execution_count": 29,
   "source": [
    "\n",
    "if irmas_root:\n",
    "    print(f\"IRMAS dataset found at: {irmas_root}\")\n",
    "\n",
    "    # Define the processing output directory\n",
    "    PROCESSED_DIR = \"/content/IRMAS_features\" if IN_COLAB else \"data/processed\"\n",
    "\n",
    "    # Check if we have mixed dataset from previous cell\n",
    "    if 'mixed_dataset' in globals() and mixed_dataset:\n",
    "        print(f\"\\nFound {len(mixed_dataset)} mixed samples from previous cell\")\n",
    "\n",
    "        # Save mixed dataset to a temporary directory for preprocessing\n",
    "        MIXED_TEMP_DIR = \"/content/IRMAS_mixed_temp\" if IN_COLAB else \"data/mixed_temp\"\n",
    "\n",
    "        # Use the preprocessing function that handles mixed data\n",
    "        from data.preprocess import preprocess_mixed_data\n",
    "\n",
    "        print(f\"Preprocessing original + mixed data to {PROCESSED_DIR}...\")\n",
    "        preprocess_mixed_data(\n",
    "            irmas_root=irmas_root,\n",
    "            mixed_dataset=mixed_dataset,\n",
    "            out_dir=PROCESSED_DIR,\n",
    "            cfg=cfg\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Preprocessing complete with mixed labels. Features saved to {PROCESSED_DIR}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No mixed dataset found. Running standard preprocessing...\")\n",
    "        print(f\"To preprocess the data, you can run:\")\n",
    "        print(f\"python data/preprocess.py --in_dir {irmas_root} --out_dir {PROCESSED_DIR}\")\n",
    "\n",
    "        # Run standard preprocessing\n",
    "        preprocess_cmd = f\"!python data/preprocess.py --in_dir {irmas_root} --out_dir {PROCESSED_DIR} --config configs/default.yaml\"\n",
    "        print(f\"\\nExecuting: {preprocess_cmd}\")\n",
    "        !python data/preprocess.py --in_dir {irmas_root} --out_dir {PROCESSED_DIR} --config configs/default.yaml\n",
    "\n",
    "else:\n",
    "    print(\"Could not locate IRMAS dataset after download. Check paths and try again.\")"
   ],
   "id": "e96b51ebbb46946c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:23:06.150747Z",
     "start_time": "2025-06-07T07:23:06.114009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import required modules for the model\n",
    "import torch\n",
    "from var import LABELS\n",
    "from models.multi_stft_cnn import MultiSTFTCNN\n",
    "\n",
    "n_classes = len(LABELS)\n",
    "\n",
    "# Create the model\n",
    "model = MultiSTFTCNN(\n",
    "    n_classes=n_classes,  # Number of instrument classes\n",
    "    n_branches=9,  # 3 FFT sizes × 3 frequency bands\n",
    "    branch_output_dim=128  # Default value for feature dimension\n",
    ")\n",
    "\n",
    "print(\"9 CNN Baseline Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Optional: Print model summary if torchinfo is available\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "    # Create dummy input for the model (9 spectrograms with random dimensions)\n",
    "    dummy_input = [torch.zeros(1, 1, 20, 30) for _ in range(9)]\n",
    "    print(\"\\nModel Summary:\")\n",
    "    summary(model, input_data=dummy_input)\n",
    "except ImportError:\n",
    "    print(\"\\nInstall torchinfo for detailed model summary: pip install torchinfo\")"
   ],
   "id": "17f023b62a55eedb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 CNN Baseline Architecture:\n",
      "MultiSTFTCNN(\n",
      "  (branches): ModuleList(\n",
      "    (0-8): 9 x STFTBranch(\n",
      "      (cnn): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU()\n",
      "        (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (10): ReLU()\n",
      "        (11): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "        (12): Flatten(start_dim=1, end_dim=-1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1152, out_features=11, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "Install torchinfo for detailed model summary: pip install torchinfo\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:28:46.395798Z",
     "start_time": "2025-06-07T07:28:46.390145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the number of samples to use for training\n",
    "# Set to None to use all samples, or a number (e.g., 50) to limit the samples\n",
    "max_samples = 1  # Change to a number like 50 to run with limited samples\n",
    "\n",
    "# Add max_samples to the configuration if it's not None\n",
    "if max_samples is not None:\n",
    "    cfg['max_samples'] = max_samples\n",
    "    print(f\"Training with limited samples: {max_samples}\")\n",
    "else:\n",
    "    print(\"Training with all available samples\")\n"
   ],
   "id": "11f0f267a4c9c98c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with limited samples: 1\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-07T07:28:47.933285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    from training.train import main as train_main\n",
    "    train_main(cfg)\n",
    "    print(\"Training completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with direct import: {e}\")\n",
    "    print(\"Falling back to shell command\")\n",
    "    # If using shell command, we need to create a temporary config file with max_samples\n",
    "    if max_samples is not None:\n",
    "        import tempfile\n",
    "        import yaml\n",
    "\n",
    "        # Create a temporary config file with max_samples\n",
    "        temp_cfg_path = tempfile.mktemp(suffix='.yaml')\n",
    "        with open(temp_cfg_path, 'w') as temp_cfg:\n",
    "            yaml.dump(cfg, temp_cfg)\n",
    "\n",
    "        !python -m training.train --config {temp_cfg_path}\n",
    "\n",
    "        # Clean up the temporary file\n",
    "        import os\n",
    "        os.unlink(temp_cfg_path)\n",
    "    else:\n",
    "        !python -m training.train --config {yaml_path}"
   ],
   "id": "9d09f471df9d60b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with direct import: expected str, bytes or os.PathLike object, not dict\n",
      "Falling back to shell command\n",
      "GPU available: False, used: False\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "/home/odahan/Technion/Semester_8/Deep_Learning/Project/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\r\n",
      "  warning_cache.warn(\r\n",
      "\r\n",
      "  | Name    | Type             | Params\r\n",
      "---------------------------------------------\r\n",
      "0 | model   | MultiSTFTCNN     | 850 K \r\n",
      "1 | metrics | MetricCollection | 0     \r\n",
      "---------------------------------------------\r\n",
      "850 K     Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "850 K     Total params\r\n",
      "3.403     Total estimated model params size (MB)\r\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s]/home/odahan/Technion/Semester_8/Deep_Learning/Project/.venv/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\r\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\r\n",
      "/home/odahan/Technion/Semester_8/Deep_Learning/Project/.venv/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\r\n",
      "  rank_zero_warn(\r\n",
      "Epoch 0: 100%|█| 1/1 [00:01<00:00,  1.01s/it, v_num=2, train/loss=0.738, train/m\r\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                       | 0/160 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                          | 0/160 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   1%|                  | 1/160 [00:02<06:30,  2.46s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   1%|▏                 | 2/160 [00:04<06:01,  2.29s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   2%|▎                 | 3/160 [00:06<05:44,  2.20s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   2%|▍                 | 4/160 [00:08<05:34,  2.14s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   3%|▌                 | 5/160 [00:11<05:55,  2.29s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   4%|▋                 | 6/160 [00:13<05:53,  2.30s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   4%|▊                 | 7/160 [00:15<05:48,  2.28s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   5%|▉                 | 8/160 [00:17<05:41,  2.25s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   6%|█                 | 9/160 [00:20<05:35,  2.22s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   6%|█                | 10/160 [00:22<05:31,  2.21s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   7%|█▏               | 11/160 [00:24<05:35,  2.25s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   8%|█▎               | 12/160 [00:27<05:39,  2.29s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   8%|█▍               | 13/160 [00:29<05:38,  2.30s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   9%|█▍               | 14/160 [00:31<05:31,  2.27s/it]\u001B[A\r\n",
      "Validation DataLoader 0:   9%|█▌               | 15/160 [00:33<05:26,  2.25s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  10%|█▋               | 16/160 [00:35<05:21,  2.23s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  11%|█▊               | 17/160 [00:37<05:17,  2.22s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  11%|█▉               | 18/160 [00:39<05:13,  2.21s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  12%|██               | 19/160 [00:41<05:09,  2.20s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  12%|██▏              | 20/160 [00:43<05:05,  2.18s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  13%|██▏              | 21/160 [00:45<05:01,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  14%|██▎              | 22/160 [00:47<04:58,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  14%|██▍              | 23/160 [00:49<04:55,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  15%|██▌              | 24/160 [00:52<04:55,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  16%|██▋              | 25/160 [00:54<04:55,  2.19s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  16%|██▊              | 26/160 [00:56<04:53,  2.19s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  17%|██▊              | 27/160 [00:58<04:50,  2.18s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  18%|██▉              | 28/160 [01:00<04:46,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  18%|███              | 29/160 [01:02<04:43,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  19%|███▏             | 30/160 [01:04<04:40,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  19%|███▎             | 31/160 [01:06<04:37,  2.15s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  20%|███▍             | 32/160 [01:08<04:34,  2.15s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  21%|███▌             | 33/160 [01:10<04:32,  2.14s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  21%|███▌             | 34/160 [01:12<04:29,  2.14s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  22%|███▋             | 35/160 [01:14<04:26,  2.13s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  22%|███▊             | 36/160 [01:16<04:23,  2.13s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  23%|███▉             | 37/160 [01:18<04:21,  2.12s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  24%|████             | 38/160 [01:20<04:18,  2.12s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  24%|████▏            | 39/160 [01:22<04:17,  2.12s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  25%|████▎            | 40/160 [01:26<04:18,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  26%|████▎            | 41/160 [01:29<04:20,  2.19s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  26%|████▍            | 42/160 [01:32<04:20,  2.20s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  27%|████▌            | 43/160 [01:34<04:17,  2.20s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  28%|████▋            | 44/160 [01:36<04:14,  2.19s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  28%|████▊            | 45/160 [01:38<04:11,  2.19s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  29%|████▉            | 46/160 [01:40<04:09,  2.19s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  29%|████▉            | 47/160 [01:42<04:06,  2.18s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  30%|█████            | 48/160 [01:44<04:03,  2.18s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  31%|█████▏           | 49/160 [01:46<04:01,  2.18s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  31%|█████▎           | 50/160 [01:48<03:59,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  32%|█████▍           | 51/160 [01:50<03:56,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  32%|█████▌           | 52/160 [01:52<03:54,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  33%|█████▋           | 53/160 [01:55<03:52,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  34%|█████▋           | 54/160 [01:57<03:49,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  34%|█████▊           | 55/160 [01:59<03:47,  2.17s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  35%|█████▉           | 56/160 [02:01<03:45,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  36%|██████           | 57/160 [02:03<03:42,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  36%|██████▏          | 58/160 [02:05<03:40,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  37%|██████▎          | 59/160 [02:07<03:37,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  38%|██████▍          | 60/160 [02:09<03:35,  2.15s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  38%|██████▍          | 61/160 [02:11<03:33,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  39%|██████▌          | 62/160 [02:13<03:30,  2.15s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  39%|██████▋          | 63/160 [02:15<03:28,  2.15s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  40%|██████▊          | 64/160 [02:17<03:26,  2.15s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  41%|██████▉          | 65/160 [02:19<03:23,  2.15s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  41%|███████          | 66/160 [02:21<03:21,  2.14s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  42%|███████          | 67/160 [02:23<03:19,  2.14s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  42%|███████▏         | 68/160 [02:26<03:17,  2.15s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  43%|███████▎         | 69/160 [02:28<03:16,  2.15s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  44%|███████▍         | 70/160 [02:31<03:14,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  44%|███████▌         | 71/160 [02:33<03:11,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  45%|███████▋         | 72/160 [02:35<03:10,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  46%|███████▊         | 73/160 [02:37<03:08,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  46%|███████▊         | 74/160 [02:39<03:05,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  47%|███████▉         | 75/160 [02:41<03:03,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  48%|████████         | 76/160 [02:44<03:01,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  48%|████████▏        | 77/160 [02:46<02:59,  2.16s/it]\u001B[A\r\n",
      "Validation DataLoader 0:  49%|████████▎        | 78/160 [02:48<02:56,  2.16s/it]\u001B[A"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T08:11:31.847006Z",
     "start_time": "2025-06-07T08:11:31.789528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inference and visualization using the inference module\n",
    "\n",
    "# Define paths for checkpoint, audio file, and config\n",
    "# You should replace these with your actual paths\n",
    "ckpt_path = f\"{workspace}/notebooks/lightning_logs/version_2/checkpoints/epoch=0-val_mAP=0.000.ckpt\"  # Path to your trained model checkpoint\n",
    "test_data_dir = f\"{workspace}/notebooks/data/raw/IRMAS-TestingData\"  # Path to test data directory\n",
    "config_path = yaml_path  # Use the same config we loaded earlier\n",
    "\n",
    "# Get a single test file for inference\n",
    "test_wav_files = list(pathlib.Path(test_data_dir).rglob(\"*.wav\"))\n",
    "if test_wav_files:\n",
    "    wav_path = str(test_wav_files[0])  # Use first test file\n",
    "    print(f\"Using test file: {wav_path}\")\n",
    "else:\n",
    "    print(f\"No WAV files found in {test_data_dir}\")\n",
    "    # Fallback to any available WAV file\n",
    "    if irmas_root:\n",
    "        fallback_files = list(pathlib.Path(irmas_root).rglob(\"*.wav\"))\n",
    "        if fallback_files:\n",
    "            wav_path = str(fallback_files[0])\n",
    "            print(f\"Using fallback file: {wav_path}\")\n",
    "        else:\n",
    "            wav_path = None\n",
    "    else:\n",
    "        wav_path = None\n",
    "\n",
    "if wav_path:\n",
    "    # Run the inference\n",
    "    print(\"Running inference with the following parameters:\")\n",
    "    print(f\"  Checkpoint: {ckpt_path}\")\n",
    "    print(f\"  Audio file: {wav_path}\")\n",
    "    print(f\"  Config: {config_path}\")\n",
    "\n",
    "    # Check if the files exist\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f\"Warning: Checkpoint file {ckpt_path} does not exist. Please update the path.\")\n",
    "    if not os.path.exists(wav_path):\n",
    "        print(f\"Warning: Audio file {wav_path} does not exist. Please update the path.\")\n",
    "\n",
    "    # Import necessary modules\n",
    "    import torch\n",
    "    from inference.predict import predict\n",
    "    from models.multi_stft_cnn import MultiSTFTCNN\n",
    "    from var import LABELS\n",
    "    from visualization.visualization import visualize_audio\n",
    "\n",
    "    # Load config\n",
    "    with open(config_path, 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    # Load model\n",
    "    model = MultiSTFTCNN(n_classes=len(LABELS))\n",
    "    try:\n",
    "        state = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        model.load_state_dict(state)\n",
    "\n",
    "        # Run prediction\n",
    "        scores = predict(model, wav_path, cfg)\n",
    "        print(\"\\nPredicted class-probabilities:\")\n",
    "        for label, score in scores.items():\n",
    "            print(f\"  {label:<15} {score:>.4f}\")\n",
    "\n",
    "        # Visualize the audio\n",
    "        print(\"\\nRendering waveform & spectrograms...\")\n",
    "        visualize_audio(wav_path, cfg)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No audio files available for inference\")"
   ],
   "id": "d7b9c7f1da8a02ae",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'workspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Inference and visualization using the inference module\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Define paths for checkpoint, audio file, and config\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# You should replace these with your actual paths\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworkspace\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/notebooks/lightning_logs/version_2/checkpoints/epoch=0-val_mAP=0.000.ckpt\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Path to your trained model checkpoint\u001B[39;00m\n\u001B[1;32m      6\u001B[0m wav_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworkspace\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/notesbooks/data/raw/IRMAS-TestingData\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Path to an audio file for inference\u001B[39;00m\n\u001B[1;32m      7\u001B[0m config_path \u001B[38;5;241m=\u001B[39m yaml_path  \u001B[38;5;66;03m# Use the same config we loaded earlier\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'workspace' is not defined"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
